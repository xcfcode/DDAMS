[{"edus": [{"text": "hello , hello , hello .", "speaker": "C"}, {"text": "wh - what causes the crash ?", "speaker": "F"}, {"text": "did you fix something ?", "speaker": "A"}, {"text": "five , five .", "speaker": "E"}, {"text": "hello , hello .", "speaker": "C"}, {"text": "it 's the turning off and turning on of the mike ,", "speaker": "F"}, {"text": "you think that 's you ?", "speaker": "B"}, {"text": "aaa - aaa .", "speaker": "C"}, {"text": ", mine 's working .", "speaker": "F"}, {"text": "that 's me .", "speaker": "C"}, {"text": "so , we are gonna do the digits at the end .", "speaker": "B"}, {"text": "channel channel three ,", "speaker": "D"}, {"text": "mmm , channel five ?", "speaker": "E"}, {"text": "that 's the mike number there , , mike number five , and channel four .", "speaker": "B"}, {"text": "is it written on her sheet , believe .", "speaker": "A"}, {"text": "era el cuatro .", "speaker": "E"}, {"text": "that 's me .", "speaker": "F"}, {"text": "this is you .", "speaker": "B"}, {"text": "and 'm channel two ,", "speaker": "B"}, {"text": "'m channel two .", "speaker": "C"}, {"text": "'m channel must be channel one .", "speaker": "B"}, {"text": "channel decided to talk about that .", "speaker": "E"}, {"text": "so also copied the results that we all got in the mail from from ogi", "speaker": "B"}, {"text": "and we 'll go through them also .", "speaker": "B"}, {"text": "so where are we on our runs ?", "speaker": "B"}, {"text": "so . we so as was already said , we mainly focused on four features .", "speaker": "D"}, {"text": "the plp , the plp with jrasta , the msg , and the mfcc from the baseline aurora .", "speaker": "D"}, {"text": "and we focused for the test part on the english and the italian .", "speaker": "D"}, {"text": "we 've trained several neural networks on", "speaker": "D"}, {"text": "so on the ti - digits english and on the italian data", "speaker": "D"}, {"text": "and also on the broad english french and spanish databases .", "speaker": "D"}, {"text": "mmm , so there 's our result tables here , for the tandem approach ,", "speaker": "D"}, {"text": "and , actually what we @ @ observed is that if the network is trained on the task data it works pretty .", "speaker": "D"}, {"text": "our our there 's we 're pausing for photo", "speaker": "B"}, {"text": "chicken on the grill .", "speaker": "C"}, {"text": "try that corner .", "speaker": "C"}, {"text": "how about over from the front of the room ?", "speaker": "A"}, {"text": "it 's longer .", "speaker": "C"}, {"text": "we 're pausing for photo opportunity here .", "speaker": "B"}, {"text": "get out of the", "speaker": "C"}, {"text": "let me give you black screen .", "speaker": "F"}, {"text": "he 's facing this way .", "speaker": "B"}, {"text": "this would be good section for our silence detection .", "speaker": "B"}, {"text": "musical chairs everybody !", "speaker": "F"}, {"text": "so , you were saying about the training data", "speaker": "B"}, {"text": "so if the network is trained on the task data tandem works pretty .", "speaker": "D"}, {"text": "and actually we have ,", "speaker": "D"}, {"text": "results are similar only on ,", "speaker": "D"}, {"text": "do you mean if it 's trained only on on data from just that task ,", "speaker": "A"}, {"text": "just that task .", "speaker": "D"}, {"text": "but actually we didn't train network on both types of data", "speaker": "D"}, {"text": "phonetically balanced data and task data .", "speaker": "D"}, {"text": "we only did either task data or broad data .", "speaker": "D"}, {"text": "clearly it 's gonna be good then", "speaker": "B"}, {"text": "but the question is how much worse is it if you have broad data ?", "speaker": "B"}, {"text": "from what saw from the earlier results , last week , was that , if you trained on one language and tested on another , say , that the results were relatively poor .", "speaker": "B"}, {"text": "but but the question is if you train on one language but you have broad coverage and then test in another , does that is that improve things in comparison ?", "speaker": "B"}, {"text": "if we use the same language ?", "speaker": "D"}, {"text": "no , no .", "speaker": "B"}, {"text": "so if you train on ti - digits and test on italian digits , you do poorly , let 's say .", "speaker": "B"}, {"text": "don't have the numbers in front of me ,", "speaker": "B"}, {"text": "but but did not do that .", "speaker": "D"}, {"text": "so 'm just imagining .", "speaker": "B"}, {"text": "so , you didn't train on timit and test on italian digits , say ?", "speaker": "B"}, {"text": "we no , we did four testing , actually .", "speaker": "D"}, {"text": "the first testing is with task data", "speaker": "D"}, {"text": "so , with nets trained on task data .", "speaker": "D"}, {"text": "so for italian on the italian speech @ @ .", "speaker": "D"}, {"text": "the second test is trained on single language with broad database ,", "speaker": "D"}, {"text": "but the same language as the task data .", "speaker": "D"}, {"text": "but for italian we choose spanish which we assume is close to italian .", "speaker": "D"}, {"text": "the third test is by using , the three language database", "speaker": "D"}, {"text": "and the fourth is", "speaker": "D"}, {"text": "it has three languages .", "speaker": "B"}, {"text": "that 's including the the", "speaker": "B"}, {"text": "the one that it 's", "speaker": "B"}, {"text": "but not digits .", "speaker": "D"}, {"text": "the three languages is not digits ,", "speaker": "A"}, {"text": "it 's the broad data .", "speaker": "A"}, {"text": "and the fourth test is excluding from these three languages the language that is the task language .", "speaker": "D"}, {"text": "so , that is what wanted to know .", "speaker": "B"}, {"text": "wasn't saying it very , .", "speaker": "B"}, {"text": "so for ti - digits for ins", "speaker": "D"}, {"text": "example when we go from ti - digits training to timit training we lose around ten percent ,", "speaker": "D"}, {"text": "the error rate increase of of ten percent , relative .", "speaker": "D"}, {"text": "so this is not so bad .", "speaker": "D"}, {"text": "and then when we jump to the multilingual data it 's it become worse", "speaker": "D"}, {"text": "and , around , let 's say , twenty perc twenty percent further .", "speaker": "D"}, {"text": "ab - about how much ?", "speaker": "B"}, {"text": "twenty percent further ?", "speaker": "B"}, {"text": "twenty to thirty percent further .", "speaker": "D"}, {"text": "and so , remind me ,", "speaker": "A"}, {"text": "the multilingual is just the broad data .", "speaker": "A"}, {"text": "it 's not the digits .", "speaker": "A"}, {"text": "so it 's the combination of two things there .", "speaker": "A"}, {"text": "it 's removing the task specific training and it 's adding other languages .", "speaker": "A"}, {"text": "but the first step is al already removing the task specific from", "speaker": "D"}, {"text": "so they were building here ?", "speaker": "A"}, {"text": "so , when it 's trained on the multilingual broad data or number", "speaker": "D"}, {"text": "our error rates with the baseline error rate is around one point one .", "speaker": "D"}, {"text": "yes . and it 's something like one point three of the", "speaker": "B"}, {"text": "if you compare everything to the first case at the baseline ,", "speaker": "B"}, {"text": "you get something like one point one for the for the using the same language but different task ,", "speaker": "B"}, {"text": "and something like one point three for three languages broad .", "speaker": "B"}, {"text": "same language we are at for at english at point eight .", "speaker": "D"}, {"text": "so it improves , compared to the baseline .", "speaker": "D"}, {"text": "le - let me .", "speaker": "D"}, {"text": "tas - task data", "speaker": "D"}, {"text": "something different by baseline", "speaker": "B"}, {"text": "so let me let me", "speaker": "B"}, {"text": "let 's let 's use the conventional meaning of baseline .", "speaker": "B"}, {"text": "by baseline here using the task specific data .", "speaker": "B"}, {"text": "but , because that 's what you were just doing with this ten percent .", "speaker": "B"}, {"text": "so was just trying to understand that .", "speaker": "B"}, {"text": "so if we call factor of just one , just normalized to one , the word error rate that you have for using ti - digits as training and ti - digits as test ,", "speaker": "B"}, {"text": "different words , 'm ,", "speaker": "B"}, {"text": "but , the same task and so on .", "speaker": "B"}, {"text": "if we call that \" one \" , then what you 're saying is that the word error rate for the same language but using different training data than you 're testing on , say timit and , it 's one point one .", "speaker": "B"}, {"text": "it 's around one point one .", "speaker": "D"}, {"text": "and if it 's", "speaker": "B"}, {"text": "you do go to three languages including the english , it 's something like one point three .", "speaker": "B"}, {"text": "that 's what you were just saying , .", "speaker": "B"}, {"text": "one point four ?", "speaker": "A"}, {"text": "so , it 's an additional thirty percent .", "speaker": "A"}, {"text": "what would you say ?", "speaker": "D"}, {"text": "around one point four", "speaker": "D"}, {"text": "and if you exclude english , from this combination , what 's that ?", "speaker": "B"}, {"text": "if we exclude english , there is not much difference with the data", "speaker": "D"}, {"text": "that 's interesting . that 's interesting .", "speaker": "B"}, {"text": "do you see ? because ,", "speaker": "B"}, {"text": "so no , that 's important .", "speaker": "B"}, {"text": "so what it 's saying here is just that", "speaker": "B"}, {"text": "there is reduction in performance , when you don't have the when you don't have", "speaker": "B"}, {"text": "no , actually it 's interesting .", "speaker": "B"}, {"text": "so it 's so when you go to different task , there 's actually not so different .", "speaker": "B"}, {"text": "it 's when you went to these", "speaker": "B"}, {"text": "so what 's the difference between two and three ?", "speaker": "B"}, {"text": "between the one point one case and the one point four case ?", "speaker": "B"}, {"text": "it 's multilingual .", "speaker": "A"}, {"text": "the only difference it 's is that it 's multilingual", "speaker": "D"}, {"text": "cuz in both in both of those cases , you don't have the same task .", "speaker": "B"}, {"text": "so is the training data for the for this one point four case", "speaker": "B"}, {"text": "does it include the training data for the one point one case ?", "speaker": "B"}, {"text": "fraction of it .", "speaker": "F"}, {"text": "part of it ,", "speaker": "D"}, {"text": "how how much bigger is it ?", "speaker": "B"}, {"text": "it 's two times ,", "speaker": "D"}, {"text": "the english data no , the multilingual databases are two times the broad english data .", "speaker": "D"}, {"text": "we just wanted to keep this , , not too huge .", "speaker": "D"}, {"text": "so it 's two times ,", "speaker": "B"}, {"text": "but it includes the but it includes the broad english data .", "speaker": "B"}, {"text": "and the broad english data is what you got this one point one with .", "speaker": "B"}, {"text": "so that 's timit", "speaker": "B"}, {"text": "so it 's band - limited timit .", "speaker": "B"}, {"text": "this is all eight kilohertz sampling .", "speaker": "B"}, {"text": "so you have band - limited timit , gave you almost as good as result as using ti - digits on ti - digits test .", "speaker": "B"}, {"text": "but , when you add in more training data but keep the neural net the same size , it performs worse on the ti - digits .", "speaker": "B"}, {"text": "now all of this is this is noisy ti - digits , assume ?", "speaker": "B"}, {"text": "both training and test ?", "speaker": "B"}, {"text": ". we we may just need to", "speaker": "B"}, {"text": "so it 's interesting that going to different task didn't seem to hurt us that much ,", "speaker": "B"}, {"text": "and going to different language", "speaker": "B"}, {"text": "it doesn't seem to matter", "speaker": "B"}, {"text": "the difference between three and four is not particularly ,", "speaker": "B"}, {"text": "so that means that whether you have the language in or not is not such big deal .", "speaker": "B"}, {"text": "it sounds like we may need to have more of things that are similar to target language", "speaker": "B"}, {"text": "or . you have the same number of parameters in the neural net ,", "speaker": "B"}, {"text": "you haven't increased the size of the neural net ,", "speaker": "B"}, {"text": "and there 's just not enough complexity to it to represent the variab increased variability in the in the training set .", "speaker": "B"}, {"text": "that that could be .", "speaker": "B"}, {"text": "so , what about", "speaker": "B"}, {"text": "so these are results with that you 're describing now , that they are pretty similar for the different features or", "speaker": "B"}, {"text": "let me check .", "speaker": "D"}, {"text": "so . this was for the plp ,", "speaker": "D"}, {"text": "for the plp with jrasta the we", "speaker": "D"}, {"text": "this is quite the same tendency , with slight increase of the error rate , if we go to timit .", "speaker": "D"}, {"text": "and then it 's it gets worse with the multilingual .", "speaker": "D"}, {"text": "there there is difference actually with between plp and jrasta", "speaker": "D"}, {"text": "is that jrasta seems to perform better with the highly mismatched condition but slightly worse for the matched condition .", "speaker": "D"}, {"text": "have suggestion , actually ,", "speaker": "B"}, {"text": "even though it 'll delay us slightly ,", "speaker": "B"}, {"text": "would you mind running into the other room and making copies of this ?", "speaker": "B"}, {"text": "cuz we 're all", "speaker": "B"}, {"text": "if we if we could look at it , while we 're talking , it 'd be", "speaker": "B"}, {"text": "'ll 'll sing song or dance while you do it , too .", "speaker": "B"}, {"text": "while you 're gone 'll ask some of my questions .", "speaker": "A"}, {"text": "this way and just slightly to the left ,", "speaker": "B"}, {"text": "the what was was this number forty", "speaker": "A"}, {"text": "or it was roughly the same as this one , he said ?", "speaker": "A"}, {"text": "when you had the two language versus the three language ?", "speaker": "A"}, {"text": "that 's what he was saying .", "speaker": "B"}, {"text": "that 's where he removed english ,", "speaker": "A"}, {"text": "it sometimes , actually , depends on what features you 're using .", "speaker": "F"}, {"text": "but but it sounds like", "speaker": "B"}, {"text": "but he - .", "speaker": "F"}, {"text": "because it seems like what it 's saying is not so much that you got hurt because you didn't have so much representation of english ,", "speaker": "B"}, {"text": "because in the other case you don't get hurt any more ,", "speaker": "B"}, {"text": "at least when it seemed like it might simply be case that you have something that is just much more diverse ,", "speaker": "B"}, {"text": "but you have the same number of parameters representing it .", "speaker": "B"}, {"text": "were all three of these nets using the same output ?", "speaker": "A"}, {"text": "this multi - language labelling ?", "speaker": "A"}, {"text": "he was using sixty - four phonemes from sampa .", "speaker": "F"}, {"text": "so this would from this you would say , \" , it doesn't really matter if we put finnish into the training of the neural net , if there 's gonna be , , finnish in the test data . \"", "speaker": "A"}, {"text": "it 's it sounds , we have to be careful ,", "speaker": "B"}, {"text": "cuz we haven't gotten good result yet .", "speaker": "B"}, {"text": "and comparing different bad results can be tricky .", "speaker": "B"}, {"text": "but it does suggest that it 's not so much cross language as cross type of speech .", "speaker": "B"}, {"text": "it 's it 's", "speaker": "B"}, {"text": "the other thing was asking him , though , is that that in the case", "speaker": "B"}, {"text": "you do have to be careful", "speaker": "B"}, {"text": "because of com compounded results .", "speaker": "B"}, {"text": "we got some earlier results in which you trained on one language and tested on another", "speaker": "B"}, {"text": "and you didn't have three , but you just had one language .", "speaker": "B"}, {"text": "so you trained on one type of digits and tested on another .", "speaker": "B"}, {"text": "didn - wasn't there something of that ?", "speaker": "B"}, {"text": "where you , say , trained on spanish and tested on ti - digits ,", "speaker": "B"}, {"text": "or the other way around ?", "speaker": "B"}, {"text": "something like that ?", "speaker": "B"}, {"text": "there was something like that , that he showed me last week .", "speaker": "B"}, {"text": "we 'll have to till", "speaker": "B"}, {"text": "that would be interesting .", "speaker": "A"}, {"text": "this may have been what was asking before , stephane ,", "speaker": "B"}, {"text": "but , , wasn't there something that you did , where you trained on one language and tested on another ?", "speaker": "B"}, {"text": "no mixture but just", "speaker": "B"}, {"text": "'ll get it for you .", "speaker": "F"}, {"text": "we 've never just trained on one lang", "speaker": "B"}, {"text": "training on single language , you mean ,", "speaker": "D"}, {"text": "and testing on the other one ?", "speaker": "D"}, {"text": "so the only task that 's similar to this is the training on two languages ,", "speaker": "D"}, {"text": "but we 've done bunch of things where we just trained on one language .", "speaker": "B"}, {"text": "you haven't you haven't done all your tests on multiple languages .", "speaker": "B"}, {"text": "either thi this is test with the same language but from the broad data ,", "speaker": "D"}, {"text": "or it 's test with different languages", "speaker": "D"}, {"text": "also from the broad data ,", "speaker": "D"}, {"text": "so , it 's it 's three or three and four .", "speaker": "D"}, {"text": "the early experiment that", "speaker": "E"}, {"text": "did you do different languages from digits ?", "speaker": "A"}, {"text": "you mean training digits on one language and using the net to recognize on the other ?", "speaker": "D"}, {"text": "digits on another language ?", "speaker": "A"}, {"text": "see , you showed me something like that last week .", "speaker": "B"}, {"text": "you had you had little", "speaker": "B"}, {"text": "no , don't .", "speaker": "D"}, {"text": "these numbers are ratio to baseline ?", "speaker": "C"}, {"text": "so , wha what 's the", "speaker": "B"}, {"text": "this this chart this table that we 're looking at is , show is all testing for ti - digits ,", "speaker": "B"}, {"text": "bigger is worse .", "speaker": "F"}, {"text": "so you have two parts .", "speaker": "D"}, {"text": "this is error rate , .", "speaker": "F"}, {"text": "no . no .", "speaker": "F"}, {"text": "the upper part is for ti - digits", "speaker": "D"}, {"text": "and it 's divided in three rows of four rows each .", "speaker": "D"}, {"text": "and the first four rows is - matched ,", "speaker": "D"}, {"text": "then the the second group of four rows is mismatched ,", "speaker": "D"}, {"text": "and finally highly mismatched .", "speaker": "D"}, {"text": "and then the lower part is for italian", "speaker": "D"}, {"text": "and it 's the same the same thing .", "speaker": "D"}, {"text": "so , so the upper part is training ti - digits ?", "speaker": "A"}, {"text": "so . it 's it 's the htk results , .", "speaker": "D"}, {"text": "so it 's htk training testings with different features", "speaker": "D"}, {"text": "and what appears in the left column is the networks that are used for doing this .", "speaker": "D"}, {"text": "what was is that what was it that you had done last week when you showed", "speaker": "B"}, {"text": "do you remember ?", "speaker": "B"}, {"text": "wh - when you showed me the your table last week ?", "speaker": "B"}, {"text": "it - it was part of these results .", "speaker": "D"}, {"text": "so where is the baseline for the ti - digits located in here ?", "speaker": "A"}, {"text": "you mean the htk aurora baseline ?", "speaker": "D"}, {"text": "it 's the one hundred number .", "speaker": "D"}, {"text": "it 's , , all these numbers are the ratio with respect to the baseline .", "speaker": "D"}, {"text": "so this is word error rate ,", "speaker": "B"}, {"text": "so high number is bad .", "speaker": "B"}, {"text": "this is word error rate ratio .", "speaker": "D"}, {"text": "so , seventy point two means that we reduced the error rate by thirty percent .", "speaker": "D"}, {"text": "so if we take", "speaker": "B"}, {"text": "plp with on - line normalization and delta - del", "speaker": "B"}, {"text": "so that 's this thing you have circled here in the second column ,", "speaker": "B"}, {"text": "and \" multi - english \" refers to what ?", "speaker": "B"}, {"text": "then you have mf , ms and me", "speaker": "D"}, {"text": "which are for french , spanish and english .", "speaker": "D"}, {"text": "actually forgot to say that the multilingual net are trained on features without the derivatives", "speaker": "D"}, {"text": "but with increased frame numbers .", "speaker": "D"}, {"text": "and we can we can see on the first line of the table that it it 's slightly worse when we don't use", "speaker": "D"}, {"text": "delta but it 's not that much .", "speaker": "D"}, {"text": "so so , 'm . missed that .", "speaker": "B"}, {"text": "what 's mf , ms and me ?", "speaker": "B"}, {"text": "multi - french ,", "speaker": "A"}, {"text": "so . multi - french , multi - spanish , and multi - english .", "speaker": "D"}, {"text": "so , it 's broader vocabulary .", "speaker": "B"}, {"text": "so what 'm what saw in your smaller chart that was thinking of was there were some numbers saw , , that included these multiple languages", "speaker": "B"}, {"text": "and it and was seeing that it got worse .", "speaker": "B"}, {"text": "that was all it was .", "speaker": "B"}, {"text": "you had some very limited results that at that point", "speaker": "B"}, {"text": "which showed having in these other languages .", "speaker": "B"}, {"text": "it might have been just this last category , having two languages broad that were where english was removed .", "speaker": "B"}, {"text": "so that was cross language", "speaker": "B"}, {"text": "and the and the result was quite poor .", "speaker": "B"}, {"text": "what we hadn't seen yet was that if you added in the english , it 's still poor .", "speaker": "B"}, {"text": "now , what 's the noise condition of the training data", "speaker": "B"}, {"text": "this is what you were explaining .", "speaker": "B"}, {"text": "the noise condition is the same", "speaker": "B"}, {"text": "it 's the same", "speaker": "B"}, {"text": "aurora noises , in all these cases for the training .", "speaker": "B"}, {"text": "so there 's not statistical sta strong st statistically different noise characteristic between the training and test", "speaker": "B"}, {"text": "no these are the same noises ,", "speaker": "D"}, {"text": "and yet we 're seeing some effect", "speaker": "B"}, {"text": "at least at least for the first for the - matched ,", "speaker": "D"}, {"text": "so there 's some an effect from having these this broader coverage", "speaker": "B"}, {"text": "now what we should try doing with this is try testing these on this same thing", "speaker": "B"}, {"text": "you probably must have this lined up to do .", "speaker": "B"}, {"text": "to try the same with the exact same training , do testing on the other languages .", "speaker": "B"}, {"text": "you have this here , for the italian .", "speaker": "B"}, {"text": "so , so .", "speaker": "B"}, {"text": "so for the italian the results are stranger", "speaker": "D"}, {"text": "so what appears is that perhaps spanish is not very close to italian", "speaker": "D"}, {"text": "because , , when using the network trained only on spanish it 's the error rate is almost twice the baseline error rate .", "speaker": "D"}, {"text": ", let 's see .", "speaker": "B"}, {"text": "is there any difference in", "speaker": "B"}, {"text": "so it 's in the", "speaker": "B"}, {"text": "so you 're saying that when you train on english and and test on", "speaker": "B"}, {"text": "no , you don't have training on english testing", "speaker": "B"}, {"text": "there there is another difference , is that the noise the noises are different .", "speaker": "D"}, {"text": "in in what ?", "speaker": "B"}, {"text": "for for the italian part", "speaker": "D"}, {"text": "the the networks are trained with noise from aurora ti - digits ,", "speaker": "D"}, {"text": "aurora - two .", "speaker": "E"}, {"text": "and the noise is different in", "speaker": "B"}, {"text": "and perhaps the noise are quite different from the noises in the speech that italian .", "speaker": "D"}, {"text": "do we have any test sets in any other language that have the same noise as in the aurora ?", "speaker": "B"}, {"text": "mmm , no .", "speaker": "E"}, {"text": "can ask something real quick ?", "speaker": "A"}, {"text": "in in the upper part in the english , it looks like the very best number is sixty point nine ?", "speaker": "A"}, {"text": "and that 's in the the third section in the upper part under plp jrasta ,", "speaker": "A"}, {"text": "the middle column ?", "speaker": "A"}, {"text": "is that noisy condition ?", "speaker": "A"}, {"text": "so that 's matched training ?", "speaker": "A"}, {"text": "is that what that is ?", "speaker": "A"}, {"text": "the third part ,", "speaker": "D"}, {"text": "so it 's highly mismatched .", "speaker": "D"}, {"text": "so . training and test noise are different .", "speaker": "D"}, {"text": "so why do you get your best number in", "speaker": "A"}, {"text": "wouldn't you get your best number in the clean case ?", "speaker": "A"}, {"text": "it 's relative to the baseline mismatching", "speaker": "C"}, {"text": "so these are not", "speaker": "A"}, {"text": "alright , see .", "speaker": "A"}, {"text": "and then so , in the in the in the non - mismatched clean case , your best one was under mfcc ?", "speaker": "A"}, {"text": "that sixty - one point four ?", "speaker": "A"}, {"text": "but it 's not clean case .", "speaker": "D"}, {"text": "it 's noisy case", "speaker": "D"}, {"text": "but training and test noises are the same .", "speaker": "D"}, {"text": "! so this upper third ?", "speaker": "A"}, {"text": "that 's still noisy ?", "speaker": "A"}, {"text": "so it 's always noisy ,", "speaker": "D"}, {"text": "and , , the", "speaker": "D"}, {"text": "this will take some looking at , thinking about .", "speaker": "B"}, {"text": "but , what is what is currently running , that 's , that just filling in the holes here", "speaker": "B"}, {"text": "no we don't plan to fill the holes", "speaker": "D"}, {"text": "but actually there is something important ,", "speaker": "D"}, {"text": "is that we made lot of assumption concerning the on - line normalization", "speaker": "D"}, {"text": "and we just noticed recently that the approach that we were using was not leading to very good results when we used the straight features to htk .", "speaker": "D"}, {"text": "so if you look at the at the left of the table , the first row , with eighty - six , one hundred , and forty - three and seventy - five ,", "speaker": "D"}, {"text": "these are the results we obtained for italian with straight mmm , plp features using on - line normalization .", "speaker": "D"}, {"text": "mmm . and the , mmm what 's in the table , just at the left of the plp twelve on - line normalization column ,", "speaker": "D"}, {"text": "so , the numbers seventy - nine , fifty - four and forty - two are the results obtained by pratibha with his on - line normalization her on - line normalization approach .", "speaker": "D"}, {"text": "where is that ?", "speaker": "A"}, {"text": "seventy - nine , fifty", "speaker": "A"}, {"text": "it 's just sitting on the the column line .", "speaker": "B"}, {"text": "fifty - one ?", "speaker": "E"}, {"text": "so these are the results of ogi with on - line normalization", "speaker": "D"}, {"text": "and straight features to htk .", "speaker": "D"}, {"text": "and the previous result , eighty - six and so on , are with our features straight to htk .", "speaker": "D"}, {"text": "so what we see that is there is that the way we were doing this was not correct ,", "speaker": "D"}, {"text": "but still the networks are very good .", "speaker": "D"}, {"text": "when we use the networks our number are better that pratibha results .", "speaker": "D"}, {"text": "so , do what was wrong with the on - line normalization , or ?", "speaker": "B"}, {"text": "there were diff there were different things", "speaker": "D"}, {"text": "and , the first thing is the mmm , alpha value .", "speaker": "D"}, {"text": "so , the recursion part .", "speaker": "D"}, {"text": "used point five percent , which was the default value in the in the programs here .", "speaker": "D"}, {"text": "and pratibha used five percent .", "speaker": "D"}, {"text": "so it adapts more quickly", "speaker": "D"}, {"text": "assume that this was not important because previous results from dan and show that the both values give the same results .", "speaker": "D"}, {"text": "it was true on ti - digits but it 's not true on italian .", "speaker": "D"}, {"text": "second thing is the initialization of the .", "speaker": "D"}, {"text": "actually , what we were doing is to start the recursion from the beginning of the utterance .", "speaker": "D"}, {"text": "and using initial values that are the global mean and variances measured across the whole database .", "speaker": "D"}, {"text": "and pratibha did something different is", "speaker": "D"}, {"text": "that he she initialed the values of the mean and variance by computing this on the twenty - five first frames of each utterance .", "speaker": "D"}, {"text": "mmm . there were other minor differences ,", "speaker": "D"}, {"text": "the fact that she used fifteen dissities instead instead of thirteen ,", "speaker": "D"}, {"text": "and that she used - zero instead of log energy .", "speaker": "D"}, {"text": "but the main differences concerns the recursion .", "speaker": "D"}, {"text": "so . , changed the code", "speaker": "D"}, {"text": "and now we have baseline that 's similar to the ogi baseline .", "speaker": "D"}, {"text": "we it it 's slightly different", "speaker": "D"}, {"text": "because don't exactly initialize the same way she does .", "speaker": "D"}, {"text": "actually start , mmm , don't to fifteen twenty - five frames before computing mean and the variance to to start the recursion .", "speaker": "D"}, {"text": "use the on - line scheme", "speaker": "D"}, {"text": "and only start the re recursion after the twenty - five twenty - fifth frame .", "speaker": "D"}, {"text": "but , it 's similar .", "speaker": "D"}, {"text": "so retrained the networks with these", "speaker": "D"}, {"text": "the the networks are retaining with these new features .", "speaker": "D"}, {"text": "so what expect is that these numbers will little bit go down", "speaker": "D"}, {"text": "but perhaps not so much", "speaker": "D"}, {"text": "because the neural networks learn perhaps to", "speaker": "D"}, {"text": "even if the features are not normalized . it it will learn how to normalize", "speaker": "D"}, {"text": "but that given the pressure of time we probably want to draw because of that especially , we wanna draw some conclusions from this ,", "speaker": "B"}, {"text": "do some reductions in what we 're looking at ,", "speaker": "B"}, {"text": "and make some strong decisions for what we 're gonna do testing on before next week .", "speaker": "B"}, {"text": "so do you are you did you have something going on , on the side , with multi - band or on this ,", "speaker": "B"}, {"text": "we plan to start this", "speaker": "D"}, {"text": "so , act actually we have discussed @ @ , these", "speaker": "D"}, {"text": "what we could do more as as research", "speaker": "D"}, {"text": "and we were thinking perhaps that the way we use the tandem is not", "speaker": "D"}, {"text": ", there is perhaps flaw in the in the", "speaker": "D"}, {"text": "because we trained the networks", "speaker": "D"}, {"text": "if we trained the networks on the on language and or specific task ,", "speaker": "D"}, {"text": "what we ask is to the network is to put the bound the decision boundaries somewhere in the space .", "speaker": "D"}, {"text": "and mmm and ask the network to put one , at one side of the for particular phoneme at one side of the boundary decision boundary", "speaker": "D"}, {"text": "and one for another phoneme at the other side .", "speaker": "D"}, {"text": "and so there is reduction of the information there that 's not correct", "speaker": "D"}, {"text": "because if we change task and if the phonemes are not in the same context in the new task , the decision boundaries are not should not be at the same place .", "speaker": "D"}, {"text": "but the way the feature gives the the way the network gives the features is that it reduce completely the it removes completely the information lot of information from the features by placing the decision boundaries at optimal places for one data", "speaker": "D"}, {"text": "but this is not the case for another data .", "speaker": "D"}, {"text": "it 's trade - off ,", "speaker": "B"}, {"text": "any - anyway go ahead .", "speaker": "B"}, {"text": "so what we were thinking about is perhaps one way to solve this problem is increase the number of outputs of the neural networks .", "speaker": "D"}, {"text": "doing something like , phonemes within context and ,", "speaker": "D"}, {"text": "context dependent phonemes .", "speaker": "D"}, {"text": "you could make the same argument ,", "speaker": "B"}, {"text": "it 'd be just as legitimate , for hybrid systems as .", "speaker": "B"}, {"text": "but , we know that", "speaker": "D"}, {"text": "and , things get better with context dependent versions .", "speaker": "B"}, {"text": "ye - but here it 's something different .", "speaker": "D"}, {"text": "we want to have features", "speaker": "D"}, {"text": "but it 's still true that what you 're doing is", "speaker": "B"}, {"text": "you 're you 're coming up with something to represent , whether it 's distribution , probability distribution or features ,", "speaker": "B"}, {"text": "you 're coming up with set of variables that are representing , things that vary over context .", "speaker": "B"}, {"text": "and you 're putting it all together ,", "speaker": "B"}, {"text": "ignoring the differences in context .", "speaker": "B"}, {"text": "that that 's true for the hybrid system ,", "speaker": "B"}, {"text": "it 's true for tandem system .", "speaker": "B"}, {"text": "so , for that reason , when you in hybrid system , when you incorporate context one way or another , you do get better scores .", "speaker": "B"}, {"text": "but it 's big deal to get that .", "speaker": "B"}, {"text": "and once you the other thing is that once you represent start representing more and more context it is much more specific to particular task in language .", "speaker": "B"}, {"text": "so , the acoustics associated with particular context ,", "speaker": "B"}, {"text": "you may have some kinds of contexts that will never occur in one language and will occur frequently in the other ,", "speaker": "B"}, {"text": "the issue of getting enough training for particular context becomes harder .", "speaker": "B"}, {"text": "we already actually don't have huge amount of training data", "speaker": "B"}, {"text": "but mmm , , the", "speaker": "D"}, {"text": "the way we do it now is that we have neural network", "speaker": "D"}, {"text": "and the net network is trained almost to give binary decisions .", "speaker": "D"}, {"text": "and binary decisions about phonemes .", "speaker": "D"}, {"text": "but it it does give distribution .", "speaker": "B"}, {"text": "it 's and it is true that if there 's two phones that are very similar , that the it may prefer one but it will give reasonably high value to the other , too .", "speaker": "B"}, {"text": "so it 's almost binary decisions", "speaker": "D"}, {"text": "and the idea of using more classes is to get something that 's less binary decisions .", "speaker": "D"}, {"text": "but it would still be even more of binary decision .", "speaker": "B"}, {"text": "it it 'd be even more of one .", "speaker": "B"}, {"text": "because then you would say that in that this phone in this context is one , but the same phone in slightly different context is zero .", "speaker": "B"}, {"text": "that would be even more distinct of binary decision .", "speaker": "B"}, {"text": "actually would have thought you 'd wanna go the other way and have fewer classes .", "speaker": "B"}, {"text": ", the thing was arguing for before , but again which don't think we have time to try , is something in which you would modify the code so you could train to have several outputs on and use articulatory features", "speaker": "B"}, {"text": "cuz then that would that would go that would be much broader and cover many different situations .", "speaker": "B"}, {"text": "but if you go to very fine categories , it 's very binary .", "speaker": "B"}, {"text": "perhaps you 're ,", "speaker": "D"}, {"text": "but you have more classes", "speaker": "D"}, {"text": "so you have more information in your features .", "speaker": "D"}, {"text": "so , you have more information in the", "speaker": "D"}, {"text": "but still the information is relevant", "speaker": "D"}, {"text": "because it 's it 's information that helps to discriminate ,", "speaker": "D"}, {"text": "if it 's possible to be able to discriminate among the phonemes in context .", "speaker": "D"}, {"text": "it 's it 's an interesting thought .", "speaker": "B"}, {"text": "we could disagree about it at length", "speaker": "B"}, {"text": "but the real thing is if you 're interested in it you 'll probably try it", "speaker": "B"}, {"text": "and we 'll see .", "speaker": "B"}, {"text": "but but what 'm more concerned with now , as an operational level , is , ,", "speaker": "B"}, {"text": "what do we do in four or five days ?", "speaker": "B"}, {"text": "and so we have to be concerned with", "speaker": "B"}, {"text": "are we gonna look at any combinations of things ,", "speaker": "B"}, {"text": "once the nets get retrained so you have this problem out of it .", "speaker": "B"}, {"text": "are we going to look at multi - band ?", "speaker": "B"}, {"text": "are we gonna look at combinations of things ?", "speaker": "B"}, {"text": "what questions are we gonna ask ,", "speaker": "B"}, {"text": "we should probably turn shortly to this note .", "speaker": "B"}, {"text": "how are we going to combine with what they 've been focusing on ?", "speaker": "B"}, {"text": "we haven't been doing any of the rasta thing .", "speaker": "B"}, {"text": "and they , although they don't talk about it in this note ,", "speaker": "B"}, {"text": "there 's , the issue of the mu law business versus the logarithm ,", "speaker": "B"}, {"text": "so what what is going on now ?", "speaker": "B"}, {"text": "you 've got nets retraining ,", "speaker": "B"}, {"text": "are there is there are there any trainings testings going on ?", "speaker": "B"}, {"text": "'m trying the htk with , plp twelve on - line delta - delta and msg filter together .", "speaker": "E"}, {"text": "the combination , .", "speaker": "E"}, {"text": "but haven't result at this moment .", "speaker": "E"}, {"text": "msg and plp .", "speaker": "B"}, {"text": "and is this with the revised on - line normalization ?", "speaker": "B"}, {"text": "ye - , with the old older ,", "speaker": "E"}, {"text": "so it 's using all the nets for that", "speaker": "B"}, {"text": "but again we have the hope that it we have the hope that it it 's not making too much difference ,", "speaker": "B"}, {"text": "so there is this combination ,", "speaker": "D"}, {"text": "working on combination .", "speaker": "D"}, {"text": "will start work on multi - band .", "speaker": "D"}, {"text": "and we plan to work also on the idea of using both features and net outputs .", "speaker": "D"}, {"text": "and we think that with this approach perhaps we could reduce the number of outputs of the neural network .", "speaker": "D"}, {"text": "so , get simpler networks ,", "speaker": "D"}, {"text": "because we still have the features .", "speaker": "D"}, {"text": "so we have come up with different broad phonetic categories .", "speaker": "D"}, {"text": "and we have we have three types of broad phonetic classes .", "speaker": "D"}, {"text": "something using place of articulation", "speaker": "D"}, {"text": "which leads to nine , , broad classes .", "speaker": "D"}, {"text": "another which is based on manner ,", "speaker": "D"}, {"text": "which is also something like nine classes .", "speaker": "D"}, {"text": "and then , something that combine both ,", "speaker": "D"}, {"text": "and we have twenty twenty - five ?", "speaker": "D"}, {"text": "twenty - seven .", "speaker": "F"}, {"text": "twenty - seven broad classes .", "speaker": "D"}, {"text": "so like , ,", "speaker": "D"}, {"text": "like back vowels , front vowels .", "speaker": "D"}, {"text": "so what you do", "speaker": "B"}, {"text": "for the moments we do not don't have nets ,", "speaker": "D"}, {"text": "so you have two net or three nets ?", "speaker": "B"}, {"text": "how many how many nets do you have ?", "speaker": "B"}, {"text": "it 's just were we just changing the labels to retrain nets with fewer out outputs .", "speaker": "D"}, {"text": "begin to work in this .", "speaker": "E"}, {"text": "we are @ @ .", "speaker": "E"}, {"text": "but but didn't understand", "speaker": "B"}, {"text": "the software currently just has allows for , the one hot output .", "speaker": "B"}, {"text": "so you 're having multiple nets and combining them ,", "speaker": "B"}, {"text": "how are you how are you coming up with", "speaker": "B"}, {"text": "if you say if you have place characteristic and manner characteristic , how do you", "speaker": "B"}, {"text": "it - it 's the single net ,", "speaker": "D"}, {"text": "they have one output .", "speaker": "A"}, {"text": "it 's just one net .", "speaker": "B"}, {"text": "it 's one net with twenty - seven outputs", "speaker": "D"}, {"text": "if we have twenty - seven classes ,", "speaker": "D"}, {"text": "so it 's , it 's standard net with fewer classes .", "speaker": "D"}, {"text": "so you 're going the other way of what you were saying bit ago", "speaker": "B"}, {"text": "including the features , .", "speaker": "D"}, {"text": "but including the features .", "speaker": "F"}, {"text": "don't think this will work alone .", "speaker": "D"}, {"text": "it will get worse", "speaker": "D"}, {"text": "because , believe the effect that of too reducing too much the information is what happens", "speaker": "D"}, {"text": "but you you include that plus the other features ,", "speaker": "B"}, {"text": "because there is perhaps one important thing that the net brings ,", "speaker": "D"}, {"text": "and ogi show showed that , is the distinction between sp speech and silence", "speaker": "D"}, {"text": "because these nets are trained on - controlled condition .", "speaker": "D"}, {"text": "the labels are obtained on clean speech , and we add noise after .", "speaker": "D"}, {"text": "so this is one thing", "speaker": "D"}, {"text": "but perhaps , something intermediary using also some broad classes could bring so much more information .", "speaker": "D"}, {"text": "so so again then we have these broad classes", "speaker": "B"}, {"text": "and , somewhat broad .", "speaker": "B"}, {"text": "it 's twenty - seven instead of sixty - four , .", "speaker": "B"}, {"text": "and you have the original features .", "speaker": "B"}, {"text": "which are plp , .", "speaker": "B"}, {"text": "and then , just to remind me , all of that goes into , of that is transformed by , - kl , or ?", "speaker": "B"}, {"text": "there will probably be ,", "speaker": "D"}, {"text": "one single kl to transform everything", "speaker": "D"}, {"text": "and only transform the other", "speaker": "E"}, {"text": "this is still something that", "speaker": "D"}, {"text": "so there 's question of whether you would", "speaker": "B"}, {"text": "two @ @ it 's one .", "speaker": "E"}, {"text": "whether you would transform together or just one .", "speaker": "B"}, {"text": "might wanna try it both ways .", "speaker": "B"}, {"text": "but that 's interesting .", "speaker": "B"}, {"text": "so that 's something that you 're you haven't trained yet but are preparing to train , and", "speaker": "B"}, {"text": "so hynek will be here monday .", "speaker": "B"}, {"text": "monday or tuesday .", "speaker": "B"}, {"text": "so , , we need to choose the experiments carefully ,", "speaker": "B"}, {"text": "so we can get key questions answered before then", "speaker": "B"}, {"text": "and leave other ones aside even if it leaves incomplete tables someplace ,", "speaker": "B"}, {"text": "it 's it 's really time to time to choose .", "speaker": "B"}, {"text": "let me pass this out , .", "speaker": "B"}, {"text": "did did interrupt you ?", "speaker": "B"}, {"text": "were there other things that you wanted to", "speaker": "B"}, {"text": "no . don't .", "speaker": "D"}, {"text": "! . , we have lots of them .", "speaker": "B"}, {"text": "we have one .", "speaker": "E"}, {"text": "so , something asked", "speaker": "B"}, {"text": "so they 're they 're doing the vad", "speaker": "B"}, {"text": "they mean voice activity detection so again , it 's the silence", "speaker": "B"}, {"text": "so they 've just trained up net which has two outputs , believe .", "speaker": "B"}, {"text": "haven't talked to sunil", "speaker": "B"}, {"text": "asked hynek whether they compared that to just taking the nets we already had and summing up the probabilities .", "speaker": "B"}, {"text": "to get the speech voice activity detection , or else just using the silence , if there 's only one silence output .", "speaker": "B"}, {"text": "and , he didn't think they had ,", "speaker": "B"}, {"text": "but on the other hand , they can get by with smaller net", "speaker": "B"}, {"text": "and sometimes you don't run the other ,", "speaker": "B"}, {"text": "there 's computational advantage to having separate net , anyway .", "speaker": "B"}, {"text": "so their the results look pretty good .", "speaker": "B"}, {"text": ", not uniformly .", "speaker": "B"}, {"text": "there 's an example or two that you can find , where it made it slightly worse , but in all but couple examples .", "speaker": "B"}, {"text": "but they have question of the result .", "speaker": "E"}, {"text": "how are trained the lda filter ?", "speaker": "E"}, {"text": "how obtained the lda filter ?", "speaker": "E"}, {"text": "don't understand your question .", "speaker": "B"}, {"text": "the lda filter needs some training set to obtain the filter .", "speaker": "E"}, {"text": "exactly how they are obtained .", "speaker": "E"}, {"text": "it 's on training .", "speaker": "B"}, {"text": "training , with the training test of each", "speaker": "E"}, {"text": "you understand me ?", "speaker": "E"}, {"text": "lda filter need set of set of training to obtain the filter .", "speaker": "E"}, {"text": "and for the italian , for the td te on for finnish , these filter are obtained with their own training set .", "speaker": "E"}, {"text": "that 's that 's so that 's that 's very good question ,", "speaker": "B"}, {"text": "then now that it understand it . it 's", "speaker": "B"}, {"text": "where does the lda come from ? \"", "speaker": "B"}, {"text": "in the in earlier experiments , they had taken lda from completely different database ,", "speaker": "B"}, {"text": "because it the same situation that the neural network training with their own", "speaker": "E"}, {"text": "so that 's good question .", "speaker": "B"}, {"text": "where does it come from ?", "speaker": "B"}, {"text": "but to tell you the truth , wasn't actually looking at the lda so much when was looking at it", "speaker": "B"}, {"text": "was mostly thinking about the vad .", "speaker": "B"}, {"text": "and , it ap it ap", "speaker": "B"}, {"text": "what does what does asp ?", "speaker": "B"}, {"text": "it says \" baseline asp \" .", "speaker": "B"}, {"text": "is what is the difference between asp and baseline over ?", "speaker": "E"}, {"text": "there it is .", "speaker": "C"}, {"text": "cuz there 's \" baseline aurora \" above it .", "speaker": "B"}, {"text": "and it 's this is mostly better than baseline ,", "speaker": "B"}, {"text": "although in some cases it 's little worse , in couple cases .", "speaker": "B"}, {"text": "it says baseline asp is twenty - three mill minus thirteen .", "speaker": "C"}, {"text": "it says what it is .", "speaker": "B"}, {"text": "but don't how that 's different from", "speaker": "B"}, {"text": "from the baseline . .", "speaker": "C"}, {"text": "this was this is the same point we were at when we were up in oregon .", "speaker": "B"}, {"text": "it 's the - zero using - zero instead of log energy .", "speaker": "D"}, {"text": "it 's this .", "speaker": "D"}, {"text": "it should be that ,", "speaker": "D"}, {"text": "they they say in here that the vad is not used as an additional feature .", "speaker": "A"}, {"text": "does does anybody know how they 're using it ?", "speaker": "A"}, {"text": "so so what they 're doing here is ,", "speaker": "B"}, {"text": "if you look down at the block diagram , , they estimate they get they get an estimate of whether it 's speech or silence ,", "speaker": "B"}, {"text": "and then they have median filter of it .", "speaker": "B"}, {"text": "and so , they 're trying to find stretches .", "speaker": "B"}, {"text": "the median filter is enforcing it having some continuity .", "speaker": "B"}, {"text": "you find stretches where the combination of the frame wise vad and the median filter say that there 's stretch of silence .", "speaker": "B"}, {"text": "and then it 's going through and just throwing the data away .", "speaker": "B"}, {"text": "so it 's it 's", "speaker": "A"}, {"text": "you mean it 's throwing out frames ?", "speaker": "A"}, {"text": "it 's throwing out chunks of frames , .", "speaker": "B"}, {"text": "there 's the median filter is enforcing that it 's not gonna be single cases of frames , or isolated frames .", "speaker": "B"}, {"text": "so it 's throwing out frames", "speaker": "B"}, {"text": "and , what don't understand is how they 're doing this with", "speaker": "B"}, {"text": "that 's what was just gonna ask .", "speaker": "A"}, {"text": "how can you just throw out frames ?", "speaker": "A"}, {"text": "it stretches again .", "speaker": "B"}, {"text": "for single frames it would be pretty hard .", "speaker": "B"}, {"text": "but if you say speech starts here , speech ends there .", "speaker": "B"}, {"text": "you can remove the frames from the feature files .", "speaker": "D"}, {"text": "so in the in the in the decoding , you 're saying that we 're gonna decode from here to here .", "speaker": "B"}, {"text": "they 're they 're treating it , , like", "speaker": "B"}, {"text": "it 's not isolated word ,", "speaker": "B"}, {"text": "in the text they say that this is tentative block diagram of possible configuration we could think of .", "speaker": "A"}, {"text": "so that sounds like they 're not doing that yet .", "speaker": "A"}, {"text": "no they have numbers though ,", "speaker": "B"}, {"text": "so they 're they 're doing something like that .", "speaker": "B"}, {"text": "that they 're they 're", "speaker": "B"}, {"text": "what by tha that is they 're trying to come up with block diagram that 's plausible for the standard .", "speaker": "B"}, {"text": "in other words , it 's", "speaker": "B"}, {"text": "from the point of view of reducing the number of bits you have to transmit it 's not bad idea to detect silence anyway .", "speaker": "B"}, {"text": "'m just wondering what exactly did they do up in this table if it wasn't this .", "speaker": "A"}, {"text": "but it 's it 's that that 's", "speaker": "B"}, {"text": "certainly it would be tricky about it intrans in transmitting voice , for listening to , is that these kinds of things cut speech off lot .", "speaker": "B"}, {"text": "plus it 's gonna introduce delays .", "speaker": "A"}, {"text": "it does introduce delays", "speaker": "B"}, {"text": "but they 're claiming that it 's it 's within the boundaries of it .", "speaker": "B"}, {"text": "and the lda introduces delays ,", "speaker": "B"}, {"text": "and what he 's suggesting this here is parallel path", "speaker": "B"}, {"text": "so that it doesn't introduce , any more delay .", "speaker": "B"}, {"text": "it introduces two hundred milliseconds of delay", "speaker": "B"}, {"text": "but at the same time the lda down here", "speaker": "B"}, {"text": "what 's the difference between tlda and slda ?", "speaker": "B"}, {"text": "temporal and spectral .", "speaker": "C"}, {"text": "you would know that .", "speaker": "B"}, {"text": "so . the temporal lda does include the same", "speaker": "B"}, {"text": "so that he , by saying this is tentative block di diagram means if you construct it this way , this delay would work in that way", "speaker": "B"}, {"text": "and then it 'd be .", "speaker": "B"}, {"text": "they they clearly did actually remove silent sections", "speaker": "B"}, {"text": "in order because they got these word error rate results .", "speaker": "B"}, {"text": "so that it 's it 's to do that in this", "speaker": "B"}, {"text": "because , it 's gonna give better word error result", "speaker": "B"}, {"text": "and therefore will help within an evaluation .", "speaker": "B"}, {"text": "whereas to whether this would actually be in final standard , .", "speaker": "B"}, {"text": ", as , part of the problem with evaluation now is that the word models are pretty bad", "speaker": "B"}, {"text": "and nobody wants has approached improving them .", "speaker": "B"}, {"text": "so it 's possible that lot of the problems with so many insertions and would go away if they were better word models to begin with .", "speaker": "B"}, {"text": "so this might just be temporary thing .", "speaker": "B"}, {"text": "but , on the other hand , and it 's decent idea .", "speaker": "B"}, {"text": "so the question we 're gonna wanna go through next week when hynek shows up is given that we 've been", "speaker": "B"}, {"text": "if you look at what we 've been trying ,", "speaker": "B"}, {"text": "we 're looking at ,", "speaker": "B"}, {"text": "by then , combinations of features and multi - band", "speaker": "B"}, {"text": "and we 've been looking at cross - language , cross task issues .", "speaker": "B"}, {"text": "and they 've been not so much looking at the cross task multiple language issues .", "speaker": "B"}, {"text": "but they 've been looking at at these issues .", "speaker": "B"}, {"text": "at the on - line normalization and the voice activity detection .", "speaker": "B"}, {"text": "and when he comes here we 're gonna have to start deciding about what do we choose from what we 've looked at to blend with some group of things in what they 've looked at", "speaker": "B"}, {"text": "and once we choose that , how do we split up the effort ?", "speaker": "B"}, {"text": "because we still have even once we choose , we 've still got another month or so ,", "speaker": "B"}, {"text": "there 's holidays in the way , but the evaluation data comes january thirty - first", "speaker": "B"}, {"text": "so there 's still fair amount of time to do things together", "speaker": "B"}, {"text": "it 's just that they probably should be somewhat more coherent between the two sites in that amount of time .", "speaker": "B"}, {"text": "when they removed the silence frames , did they insert some marker so that the recognizer knows it 's knows when it 's time to back trace ?", "speaker": "A"}, {"text": "the specifics of how they 're doing it .", "speaker": "B"}, {"text": "they 're they 're getting around the way the recognizer works because they 're not allowed to , change the scripts for the recognizer , believe .", "speaker": "B"}, {"text": "they 're just inserting some nummy frames ?", "speaker": "A"}, {"text": ", that 's what had thought .", "speaker": "B"}, {"text": "but don't don't think they are .", "speaker": "B"}, {"text": "that 's what the way had imagined would happen is that on the other side ,", "speaker": "B"}, {"text": "you put some low level noise .", "speaker": "B"}, {"text": "probably don't want all zeros .", "speaker": "B"}, {"text": "most recognizers don't like zeros", "speaker": "B"}, {"text": "but , put some epsilon in or some rand", "speaker": "B"}, {"text": "epsilon random variable in .", "speaker": "B"}, {"text": "some constant vector .", "speaker": "A"}, {"text": "not constant but it doesn't , don't like to divide by the variance of that ,", "speaker": "B"}, {"text": "what is something that is very distinguishable from speech .", "speaker": "A"}, {"text": "so that the silence model in htk will always pick it up .", "speaker": "A"}, {"text": "so that 's what they would do .", "speaker": "B"}, {"text": "or else , there is some indicator to tell it to start and stop ,", "speaker": "B"}, {"text": "but whatever they did , they have to play within the rules of this specific evaluation .", "speaker": "B"}, {"text": "we we can find out .", "speaker": "B"}, {"text": "cuz you gotta do something . otherwise , if it 's just bunch of speech , stuck together", "speaker": "A"}, {"text": "it would do badly", "speaker": "B"}, {"text": "and it didn't so badly ,", "speaker": "B"}, {"text": "so they did something .", "speaker": "B"}, {"text": "so this brings me up to date bit .", "speaker": "B"}, {"text": "it hopefully brings other people up to date bit .", "speaker": "B"}, {"text": "and , wanna look at these numbers off - line little bit", "speaker": "B"}, {"text": "and think about it and talk with everybody , outside of this meeting .", "speaker": "B"}, {"text": "no it sounds like there are the usual number of little problems and bugs and", "speaker": "B"}, {"text": "but it sounds like they 're getting ironed out .", "speaker": "B"}, {"text": "and now we 're seem to be in position to actually , look at and and compare things .", "speaker": "B"}, {"text": "so that 's that 's pretty good .", "speaker": "B"}, {"text": "one of the things wonder about , coming back to the first results you talked about , is how much , things could be helped by more parameters .", "speaker": "B"}, {"text": "and and how many more parameters we can afford to have , in terms of the computational limits .", "speaker": "B"}, {"text": "because anyway when we go to twice as much data and have the same number of parameters ,", "speaker": "B"}, {"text": "particularly when it 's twice as much data and it 's quite diverse ,", "speaker": "B"}, {"text": "wonder if having twice as many parameters would help .", "speaker": "B"}, {"text": "just have bigger hidden layer .", "speaker": "B"}, {"text": "doubt it would help by forty per cent .", "speaker": "B"}, {"text": "how are we doing on the resources ?", "speaker": "B"}, {"text": "we 're alright ,", "speaker": "D"}, {"text": "not much problems with that .", "speaker": "D"}, {"text": "this table took more than five days to get back .", "speaker": "D"}, {"text": "are were you folks using gin ?", "speaker": "B"}, {"text": "that 's that just died ,", "speaker": "B"}, {"text": "mmm , no .", "speaker": "D"}, {"text": "you were using gin perhaps ,", "speaker": "D"}, {"text": "that 's good .", "speaker": "B"}, {"text": "it just died .", "speaker": "F"}, {"text": "we 're gonna get replacement server that 'll be faster server , actually .", "speaker": "B"}, {"text": "that 'll be it 's seven hundred fifty megahertz sun", "speaker": "B"}, {"text": "but it won't be installed for little while .", "speaker": "B"}, {"text": "do we do we have that big new ibm machine", "speaker": "G"}, {"text": "we have the little tiny ibm machine that might someday grow up to be big ibm machine .", "speaker": "B"}, {"text": "it 's got slots for eight ,", "speaker": "B"}, {"text": "ibm was donating five ,", "speaker": "B"}, {"text": "we only got two so far ,", "speaker": "B"}, {"text": "we had originally hoped we were getting eight hundred megahertz processors .", "speaker": "B"}, {"text": "they ended up being five fifty .", "speaker": "B"}, {"text": "so instead of having eight processors that were eight hundred megahertz , we ended up with two that are five hundred and fifty megahertz .", "speaker": "B"}, {"text": "and more are supposed to come soon", "speaker": "B"}, {"text": "and there 's only moderate amount of dat of memory .", "speaker": "B"}, {"text": "so don't think anybody has been sufficiently excited by it to spend much time with it ,", "speaker": "B"}, {"text": "but hopefully , they 'll get us some more parts , soon", "speaker": "B"}, {"text": ", that 'll be", "speaker": "B"}, {"text": "once we get it populated , that 'll be machine .", "speaker": "B"}, {"text": "we will ultimately get eight processors in there .", "speaker": "B"}, {"text": "and and amount of memory .", "speaker": "B"}, {"text": "so it 'll be pr pretty fast linux machine .", "speaker": "B"}, {"text": "and if we can do things on linux , some of the machines we have going already , like swede ?", "speaker": "G"}, {"text": "it seems pretty fast .", "speaker": "G"}, {"text": "but fudge is pretty fast too .", "speaker": "G"}, {"text": "you can check with dave johnson .", "speaker": "B"}, {"text": "it 's the machine is just sitting there .", "speaker": "B"}, {"text": "and it does have two processors ,", "speaker": "B"}, {"text": "and somebody could do , , check out the multi - threading libraries .", "speaker": "B"}, {"text": "and it 's possible that the", "speaker": "B"}, {"text": "the prudent thing to do would be for somebody to do the work on getting our code running on that machine with two processors even though there aren't five or eight .", "speaker": "B"}, {"text": "there 's there 's gonna be debugging hassles", "speaker": "B"}, {"text": "and then we 'd be set for when we did have five or eight , to have it really be useful .", "speaker": "B"}, {"text": "but . notice how said somebody", "speaker": "B"}, {"text": "and turned my head your direction .", "speaker": "B"}, {"text": "that 's one thing you don't get in these recordings .", "speaker": "B"}, {"text": "you don't get the don't get the visuals", "speaker": "B"}, {"text": "is it mostly the neural network trainings that are slowing us down or the htk runs that are slowing us down ?", "speaker": "G"}, {"text": "you 're you 're held up by both ,", "speaker": "B"}, {"text": "if the if the neural net trainings were hundred times faster you still wouldn't be anything", "speaker": "B"}, {"text": "running through these hundred times faster", "speaker": "B"}, {"text": "because you 'd be stuck by the htk trainings ,", "speaker": "B"}, {"text": "but if the htk", "speaker": "B"}, {"text": "they 're both it sounded like they were roughly equal ?", "speaker": "B"}, {"text": "is that about ?", "speaker": "B"}, {"text": "because , that 'll be running linux ,", "speaker": "G"}, {"text": "and sw - swede and fudge are already running linux", "speaker": "G"}, {"text": "so , could try to get the train the neural network trainings or the htk running under linux , and to start with 'm wondering which one should pick first .", "speaker": "G"}, {"text": "probably the neural net", "speaker": "B"}, {"text": "cuz it 's probably it 's it 's", "speaker": "B"}, {"text": "htk we use for this aurora", "speaker": "B"}, {"text": "it 's not clear yet what we 're gonna use for trainings", "speaker": "B"}, {"text": "there 's the trainings", "speaker": "B"}, {"text": "is it the training that takes the time , or the decoding ?", "speaker": "B"}, {"text": "is it about equal between the two ?", "speaker": "B"}, {"text": "for for aurora ?", "speaker": "B"}, {"text": "for the aurora ?", "speaker": "B"}, {"text": "training is longer .", "speaker": "D"}, {"text": "do we have htk source ?", "speaker": "B"}, {"text": "you would think that would fairly trivially", "speaker": "B"}, {"text": "the training would , anyway ,", "speaker": "B"}, {"text": "the testing don't think would parallelize all that .", "speaker": "B"}, {"text": "but that you could certainly do , distributed , ,", "speaker": "B"}, {"text": "no , it 's the each individual sentence", "speaker": "B"}, {"text": "is pretty tricky to parallelize .", "speaker": "B"}, {"text": "but you could split up the sentences in test set .", "speaker": "B"}, {"text": "they have they have thing for doing that", "speaker": "A"}, {"text": "and they have for awhile , in", "speaker": "A"}, {"text": "and you can parallelize the training .", "speaker": "A"}, {"text": "and run it on several machines", "speaker": "A"}, {"text": "and it just keeps counts .", "speaker": "A"}, {"text": "and there 's something final thing that you run", "speaker": "A"}, {"text": "and it accumulates all the counts together .", "speaker": "A"}, {"text": "don't what their scripts are set up to do for the aurora ,", "speaker": "A"}, {"text": "something that we haven't really settled on yet is other than this aurora , what do we do , large vocabulary training slash testing for tandem systems .", "speaker": "B"}, {"text": "cuz we hadn't really done much with tandem systems for larger .", "speaker": "B"}, {"text": "cuz we had this one collaboration with cmu and we used sphinx .", "speaker": "B"}, {"text": "we 're also gonna be collaborating with sri", "speaker": "B"}, {"text": "and we have their have theirs .", "speaker": "B"}, {"text": "so the advantage of going with the neural net thing is that we 're gonna use the neural net trainings , no matter what ,", "speaker": "B"}, {"text": "for lot of the things we 're doing ,", "speaker": "B"}, {"text": "whereas , exactly which gaussian - mixture - based thing we use is gonna depend", "speaker": "B"}, {"text": "so with that , we should go to our {nonvocalsound} digit recitation task .", "speaker": "B"}, {"text": "and , it 's about eleven fifty .", "speaker": "B"}, {"text": "start over here .", "speaker": "B"}, {"text": "could you give adam call .", "speaker": "B"}, {"text": "he 's at two nine seven .", "speaker": "B"}, {"text": "we can @ @ herve 's coming tomorrow ,", "speaker": "B"}, {"text": "herve will be giving talk , , talk at eleven .", "speaker": "B"}, {"text": "did , did everybody sign these consent er everybody has everyone signed consent form before , on previous meetings ?", "speaker": "B"}, {"text": "you don't have to do it again each time", "speaker": "B"}], "id": "xxxxx", "relations": [{"y": 1, "x": 0, "type": "Explanation"}]}]