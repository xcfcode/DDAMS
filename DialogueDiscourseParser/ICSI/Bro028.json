[{"edus": [{"text": "we should be going .", "speaker": "A"}, {"text": "so ne next week we 'll have , , both birger and , , mike michael", "speaker": "B"}, {"text": "and birger kollmeier will join us .", "speaker": "B"}, {"text": "and you 're you 're probably gonna go up in couple three weeks or so ?", "speaker": "B"}, {"text": "when when are you thinking of going up to , , ogi ?", "speaker": "B"}, {"text": "like , , not next week", "speaker": "D"}, {"text": "but the week after .", "speaker": "D"}, {"text": "good . so at least we 'll have one meeting with yo with you still around ,", "speaker": "B"}, {"text": "that 's good .", "speaker": "B"}, {"text": ", we can start with this .", "speaker": "D"}, {"text": "all today , ?", "speaker": "B"}, {"text": "so there was this conference call this morning ,", "speaker": "D"}, {"text": "and the only topic on the agenda was just to discuss", "speaker": "D"}, {"text": "and to come at , to get decision about this latency problem .", "speaker": "D"}, {"text": "no , this 'm , this is conference call between different aurora people", "speaker": "B"}, {"text": ". it 's the conference call between the aurora , , group .", "speaker": "D"}, {"text": "it 's the main conference call .", "speaker": "B"}, {"text": ". there were like two hours of discussions ,", "speaker": "D"}, {"text": "and then suddenly , , people were tired , ,", "speaker": "D"}, {"text": "and they decided on {nonvocalsound} number ,", "speaker": "D"}, {"text": "two hundred and twenty ,", "speaker": "D"}, {"text": "included including everything .", "speaker": "D"}, {"text": "it means that it 's like eighty milliseconds less than before .", "speaker": "D"}, {"text": "and what are we sitting at currently ?", "speaker": "B"}, {"text": "so , currently , we have system that has two hundred and thirty .", "speaker": "D"}, {"text": "so , that 's fine .", "speaker": "D"}, {"text": "so that 's the system that 's described on the second point of this document .", "speaker": "D"}, {"text": "we have to reduce it by ten milliseconds somehow .", "speaker": "B"}, {"text": "that 's not problem , .", "speaker": "D"}, {"text": "it 's it 's primary primarily determined by the vad at this point ,", "speaker": "B"}, {"text": "so we can make the vad little shorter .", "speaker": "B"}, {"text": "at this point ,", "speaker": "D"}, {"text": "we probably should do that pretty soon so that we don't get used to it being certain way .", "speaker": "B"}, {"text": "was hari on the on the phone ?", "speaker": "B"}, {"text": "it was mainly discussion between hari and david ,", "speaker": "D"}, {"text": ". so , the second thing is the system that we have currently .", "speaker": "D"}, {"text": "yes . we have , like , system that gives sixty - two percent improvement ,", "speaker": "D"}, {"text": "but if you want to stick to the this latency", "speaker": "D"}, {"text": "it has latency of two thirty ,", "speaker": "D"}, {"text": "but if you want also to stick to the number of features that limit it to sixty , then we go little bit down", "speaker": "D"}, {"text": "but it 's still sixty - one percent .", "speaker": "D"}, {"text": "and if we drop the tandem network , then we have fifty - seven percent .", "speaker": "D"}, {"text": "but the two two thirty includes the tandem network ?", "speaker": "B"}, {"text": "and is the tandem network , , small enough that it will fit on the terminal size", "speaker": "B"}, {"text": "in terms of ?", "speaker": "B"}, {"text": "no , don't .", "speaker": "D"}, {"text": "it 's still in terms of computation , if we use , like , their way of computing the maps the mips , it fits ,", "speaker": "D"}, {"text": "but it 's , , mainly problem of memory .", "speaker": "D"}, {"text": "and how much this can be discussed or not ,", "speaker": "D"}, {"text": "because it 's it could be in rom ,", "speaker": "D"}, {"text": "so it 's not that expensive .", "speaker": "D"}, {"text": "ho - how much memory ? how many ?", "speaker": "B"}, {"text": "don't kn remember exactly ,", "speaker": "D"}, {"text": "have to check that .", "speaker": "D"}, {"text": "'d like to see that ,", "speaker": "B"}, {"text": "cuz could think little bit about it ,", "speaker": "B"}, {"text": "cuz we we could make it little smaller", "speaker": "B"}, {"text": "or , it 'd be it 'd be neat if we could fit it all .", "speaker": "B"}, {"text": "'d like to see how far off we are .", "speaker": "B"}, {"text": "but it 's still within their rules to have it on the , , , server side .", "speaker": "B"}, {"text": "and this is still ?", "speaker": "B"}, {"text": ", you 're saying here .", "speaker": "B"}, {"text": "should just let you go on .", "speaker": "B"}, {"text": "there were small tricks to make this tandem network work .", "speaker": "D"}, {"text": "and one of the trick was to , , use some hierarchical structure", "speaker": "D"}, {"text": "where the silence probability is not computed by the final tandem network but by the vad network .", "speaker": "D"}, {"text": "so it looks better when , , we use the silence probability from the vad network", "speaker": "D"}, {"text": "and we re - scale the other probabilities by one minus the silence probability .", "speaker": "D"}, {"text": "so it 's some hierarchical thing , , that sunil also tried , , on spine", "speaker": "D"}, {"text": "and it helps little bit also .", "speaker": "D"}, {"text": "the reason why we did that with the silence probability was that ,", "speaker": "D"}, {"text": "could ? , ,", "speaker": "B"}, {"text": "'m 'm really .", "speaker": "B"}, {"text": "can you repeat what you were saying about the silence probability ?", "speaker": "B"}, {"text": "my mind was some", "speaker": "B"}, {"text": "so there is the tandem network that estimates the phone probabilities", "speaker": "D"}, {"text": "and the silence probabilities also .", "speaker": "D"}, {"text": "and things get better when , instead of using the silence probability computed by the tandem network , we use the silence probability , , given by the vad network ,", "speaker": "D"}, {"text": "the vad network is ?", "speaker": "B"}, {"text": "which is smaller ,", "speaker": "D"}, {"text": "so we have network for the vad which has one hundred hidden units ,", "speaker": "D"}, {"text": "and the tandem network has five hundred .", "speaker": "D"}, {"text": "so it 's smaller", "speaker": "D"}, {"text": "but the silence probability from this network seems , , better .", "speaker": "D"}, {"text": "it looks strange ,", "speaker": "D"}, {"text": "it 's has something to do to the fact that we don't have infinite training data", "speaker": "D"}, {"text": "so , things are not optimal", "speaker": "D"}, {"text": "are you were going to say why what made you wh what led you to do that .", "speaker": "E"}, {"text": ", there was problem that we observed , , that there was there were , like , many insertions in the in the system .", "speaker": "D"}, {"text": "actually plugging in the tandem network was increasing , , the number of insertions .", "speaker": "D"}, {"text": "so it looked strange", "speaker": "D"}, {"text": "and then just using the other silence probability helps .", "speaker": "D"}, {"text": "the next thing we will do is train this tandem on more data .", "speaker": "D"}, {"text": "so , , in way what it might it 's it 's little bit like combining knowledge sources .", "speaker": "B"}, {"text": "because the fact that you have these two nets that are different sizes means they behave little differently ,", "speaker": "B"}, {"text": "they find different things .", "speaker": "B"}, {"text": "if you have , the distribution that you have from , , speech sounds is one source of knowledge .", "speaker": "B"}, {"text": "and rather than just taking one minus that to get the other ,", "speaker": "B"}, {"text": "which is essentially what 's happening ,", "speaker": "B"}, {"text": "you have this other source of knowledge that you 're putting in there .", "speaker": "B"}, {"text": "so you make use of both of them in what you 're ending up with .", "speaker": "B"}, {"text": "it 's better .", "speaker": "B"}, {"text": "anyway , you can probably justify anything if what 's use", "speaker": "B"}, {"text": "and and the features are different also .", "speaker": "D"}, {"text": "the vad doesn't use the same features there are .", "speaker": "D"}, {"text": "that might be the key , actually .", "speaker": "B"}, {"text": "cuz you were really thinking about speech versus nonspeech for that .", "speaker": "B"}, {"text": "that 's good point .", "speaker": "B"}, {"text": ", there are other things that we should do", "speaker": "D"}, {"text": "but , , it requires time", "speaker": "D"}, {"text": "and we have ideas ,", "speaker": "D"}, {"text": "like so , these things are like hav having better vad .", "speaker": "D"}, {"text": "we have some ideas about that .", "speaker": "D"}, {"text": "it would probably implies working little bit on features that are more suited to voice activity detection .", "speaker": "D"}, {"text": "working on the second stream .", "speaker": "D"}, {"text": "we have ideas on this also ,", "speaker": "D"}, {"text": "but we need to try different things", "speaker": "D"}, {"text": "but their noise estimation ,", "speaker": "D"}, {"text": "back on the second stream ,", "speaker": "B"}, {"text": "that 's something we 've talked about for while .", "speaker": "B"}, {"text": "{nonvocalsound} that 's certainly high hope .", "speaker": "B"}, {"text": "so we have this default idea about just using some purely spectral thing ?", "speaker": "B"}, {"text": "for second stream ?", "speaker": "B"}, {"text": "but , , we did first try with this ,", "speaker": "D"}, {"text": "and it clearly hurts .", "speaker": "D"}, {"text": "but , , how was the stream combined ?", "speaker": "B"}, {"text": "it was it was just combined , , by the acoustic model .", "speaker": "D"}, {"text": "so there was , no neural network for the moment .", "speaker": "D"}, {"text": "so , , if you just had second stream that was just spectral and had another neural net and combined there , that , , might be good .", "speaker": "B"}, {"text": "and the other thing , that noise estimation", "speaker": "D"}, {"text": "and , try to train , the training data for the", "speaker": "D"}, {"text": "tandem network , now , is like is using the noises from the aurora task", "speaker": "D"}, {"text": "and that people might , , try to argue about that", "speaker": "D"}, {"text": "because then in some cases we have the same noises in for training the network than the noises that are used for testing ,", "speaker": "D"}, {"text": "so we have , to try to get rid of these this problem .", "speaker": "D"}, {"text": "you just put in some other noise ,", "speaker": "B"}, {"text": "something that 's different .", "speaker": "B"}, {"text": "it 's probably helpful to have little noise there .", "speaker": "B"}, {"text": "but it may be something else", "speaker": "B"}, {"text": "at least you could say it was .", "speaker": "B"}, {"text": "and then if it doesn't hurt too much , though .", "speaker": "B"}, {"text": "that 's good idea .", "speaker": "B"}, {"text": "the last thing is that we are getting close to human performance .", "speaker": "D"}, {"text": "that 's something would like to investigate further ,", "speaker": "D"}, {"text": "did , like , did , , listen to the most noisy utterances of the speechdat - car italian", "speaker": "D"}, {"text": "and tried to transcribe them .", "speaker": "D"}, {"text": "so this is particular human .", "speaker": "B"}, {"text": "this is this this is stephane .", "speaker": "B"}, {"text": "so that 's that 's", "speaker": "D"}, {"text": "st - stephane .", "speaker": "E"}, {"text": "that 's the flaw of the experiment .", "speaker": "D"}, {"text": "this is just it 's just one subject ,", "speaker": "D"}, {"text": "but still , , what happens is that , , the digit error rate on this is around one percent ,", "speaker": "D"}, {"text": "while our system is currently at seven percent .", "speaker": "D"}, {"text": "but what happens also is that if listen to the , {nonvocalsound} re - synthesized version of the speech", "speaker": "D"}, {"text": "and re - synthesized this using white noise that 's filtered by lpc , , filter", "speaker": "D"}, {"text": "you can argue , that , that this is not speech ,", "speaker": "D"}, {"text": "so the ear is not trained to recognize this .", "speaker": "D"}, {"text": "but actually it sound like whispering ,", "speaker": "D"}, {"text": "there 's two problems there .", "speaker": "B"}, {"text": "so the first is that by doing lpc - twelve with synthesized speech like you 're saying , , it 's you 're you 're adding other degradation .", "speaker": "B"}, {"text": "so it 's not just the noise", "speaker": "B"}, {"text": "but you 're adding some degradation", "speaker": "B"}, {"text": "because it 's only an approximation .", "speaker": "B"}, {"text": "and the second thing is which is more interesting is that , , if you do it with whispered speech , you get this number .", "speaker": "B"}, {"text": "what if you had done analysis re - synthesis and taken the pitch as ?", "speaker": "B"}, {"text": "so now you put the pitch in .", "speaker": "B"}, {"text": "what would the percentage be then ?", "speaker": "B"}, {"text": "see , that 's the question .", "speaker": "B"}, {"text": "so , you see , if it 's if it 's , let 's say it 's back down to one percent again .", "speaker": "B"}, {"text": "that would say at least for people , having the pitch is really , really important ,", "speaker": "B"}, {"text": "which would be interesting in itself .", "speaker": "B"}, {"text": "if on the other hand , if it stayed up near five percent , then 'd say \" boy , lpc twelve is pretty crummy \" .", "speaker": "B"}, {"text": "so 'm not 'm not how we can conclude from this anything about that our system is close to the human performance .", "speaker": "B"}, {"text": ", that ey that , , what what listened to when re - synthesized the lp - the lpc - twelve spectrum is in way what the system , , is hearing ,", "speaker": "D"}, {"text": "cuz @ @ all the all the , , excitation all the", "speaker": "D"}, {"text": "the excitation is not taken into account .", "speaker": "D"}, {"text": "that 's what we do with our system .", "speaker": "D"}, {"text": "you 're not doing the lpc", "speaker": "B"}, {"text": "so what if you did", "speaker": "B"}, {"text": "it 's not lpc , ,", "speaker": "D"}, {"text": "what if you did lpc - twenty ?", "speaker": "B"}, {"text": "lpc is not really representation of speech .", "speaker": "B"}, {"text": "so , all 'm saying is that you have in addition to the the , , removal of pitch , you also are doing , , particular parameterization ,", "speaker": "B"}, {"text": "so , let 's see ,", "speaker": "B"}, {"text": "how would you do ? so , fo", "speaker": "B"}, {"text": "but that 's that 's what we do with our systems .", "speaker": "D"}, {"text": "no . actually , we we don't ,", "speaker": "B"}, {"text": "because we do we do , , , mel filter bank , .", "speaker": "B"}, {"text": "but is it that is it that different , ?", "speaker": "D"}, {"text": "what mel , , based synthesis would sound like ,", "speaker": "B"}, {"text": "but certainly the spectra are quite different .", "speaker": "B"}, {"text": "couldn't you couldn't you , , test the human performance on just the original audio ?", "speaker": "A"}, {"text": "this is the one percent number .", "speaker": "D"}, {"text": "it 's one percent .", "speaker": "B"}, {"text": "he 's trying to remove the pitch information", "speaker": "B"}, {"text": "and make it closer to what to what we 're seeing as the feature vectors .", "speaker": "B"}, {"text": "so , , your performance was one percent ,", "speaker": "A"}, {"text": "and then when you re - synthesize with lpc - twelve it went to five .", "speaker": "A"}, {"text": "we were we were it it 's little bit still apples and oranges", "speaker": "B"}, {"text": "because we are choosing these features in order to be the best for recognition .", "speaker": "B"}, {"text": "if you listen to them they still might not be very even if you made something closer to what we 're gonna it might not sound very good .", "speaker": "B"}, {"text": "and the degradation from that might actually make it even harder , , to understand than the lpc - twelve .", "speaker": "B"}, {"text": "so all 'm saying is that the lpc - twelve puts in synthesis", "speaker": "B"}, {"text": "puts in some degradation", "speaker": "B"}, {"text": "that 's not what we 're used to hearing ,", "speaker": "B"}, {"text": "it 's not it 's not just question of how much information is there , as if you will always take maximum advantage of any information that 's presented to you .", "speaker": "B"}, {"text": "you hear some things better than others .", "speaker": "B"}, {"text": "and so it isn't", "speaker": "B"}, {"text": "but , agree that it says that , , the information that we 're feeding it is probably , , little bit , , minimal .", "speaker": "B"}, {"text": "there 's definitely some things that we 've thrown away .", "speaker": "B"}, {"text": "and that 's why was saying it might be interesting if you an interesting test of this would be if you if you actually put the pitch back in .", "speaker": "B"}, {"text": "so , you just extract it from the actual speech and put it back in ,", "speaker": "B"}, {"text": "and see does that is that does that make the difference ?", "speaker": "B"}, {"text": "if that if that takes it down to one percent again , then you 'd say \" , it 's it 's having , , not just the spectral envelope but also the also the pitch that , , @ @ has the information that people can use , anyway . \"", "speaker": "B"}, {"text": "but from this it 's pretty safe to say that the system is with either two to seven percent away from the performance of human .", "speaker": "A"}, {"text": "so it 's somewhere in that range .", "speaker": "A"}, {"text": "or it 's it 's", "speaker": "B"}, {"text": "two two to six percent .", "speaker": "A"}, {"text": "it 's it 's one point four times , , to , , seven times the error ,", "speaker": "B"}, {"text": "to seven times , .", "speaker": "D"}, {"text": "do don't wanna take you away from other things .", "speaker": "B"}, {"text": "but that 's that 's what that 's the first thing that would be curious about , is , , when you we", "speaker": "B"}, {"text": "but the signal itself is like mix of , of periodic sound and , @ @ , unvoiced sound , and the noise", "speaker": "D"}, {"text": "which is mostly , , noise .", "speaker": "D"}, {"text": "so , what do you mean exactly by putting back the pitch in ?", "speaker": "D"}, {"text": "in the lpc synthesis ?", "speaker": "A"}, {"text": "you did lpc re - synthesis", "speaker": "B"}, {"text": "pc re - synthesis .", "speaker": "B"}, {"text": "so , and you did it with noise source ,", "speaker": "B"}, {"text": "rather than with periodic source .", "speaker": "B"}, {"text": "so if you actually did real re - synthesis like you do in an lpc synthesizer , where it 's unvoiced you use noise ,", "speaker": "B"}, {"text": "where it 's voiced you use , , periodic pulses .", "speaker": "B"}, {"text": "but it 's neither purely voiced or purely unvoiced .", "speaker": "D"}, {"text": "esp - especially because there is noise .", "speaker": "D"}, {"text": "it might be hard to do it", "speaker": "B"}, {"text": "but it but that if you , if you detect that there 's periodic strong periodic components , then you can use voiced voice thing .", "speaker": "B"}, {"text": ", it 's probably not worth your time .", "speaker": "B"}, {"text": "it 's it 's side thing", "speaker": "B"}, {"text": "and and there 's lot to do .", "speaker": "B"}, {"text": "but 'm 'm just saying , at least as thought experiment , that 's what would wanna test .", "speaker": "B"}, {"text": "wan would wanna drive it with two - source system rather than than one - source system .", "speaker": "B"}, {"text": "and then that would tell you whether it 's", "speaker": "B"}, {"text": "cuz we 've talked about , like , this harmonic tunneling or other things that people have done based on pitch ,", "speaker": "B"}, {"text": "that 's really key element .", "speaker": "B"}, {"text": ", , without that , it 's it 's not possible to do whole lot better than we 're doing .", "speaker": "B"}, {"text": "that that could be .", "speaker": "B"}, {"text": "that 's what was thinking by doing this es experiment ,", "speaker": "D"}, {"text": "but , , other than that , don't 's", "speaker": "B"}, {"text": "other than the pitch de information , it 's hard to imagine that there 's whole lot more in the signal that , that we 're throwing away that 's important .", "speaker": "B"}, {"text": "- . , .", "speaker": "D"}, {"text": "? , we 're using fair number of filters in the filter bank", "speaker": "B"}, {"text": "that 's it .", "speaker": "D"}, {"text": "that 's that 's , one percent is what would would figure .", "speaker": "B"}, {"text": "if somebody was paying really close attention , you might get", "speaker": "B"}, {"text": "would actually think that if , you looked at people on various times of the day and different amounts of attention , you might actually get up to three or four percent error on digits .", "speaker": "B"}, {"text": "we 're not we 're not incredibly far off .", "speaker": "B"}, {"text": "on the other hand , with any of these numbers except the one percent , it 's st it 's not actually usable in commercial system with full telephone number .", "speaker": "B"}, {"text": "at these noise levels .", "speaker": "D"}, {"text": ". these numbers , .", "speaker": "D"}, {"text": "while we 're still on aurora you can talk little about the status with the , , wall street journal things for it .", "speaker": "B"}, {"text": "so 've , , downloaded , , couple of things from mississippi state .", "speaker": "A"}, {"text": "one is their software", "speaker": "A"}, {"text": "their , , lvcsr system .", "speaker": "A"}, {"text": "downloaded the latest version of that .", "speaker": "A"}, {"text": "got it compiled and everything .", "speaker": "A"}, {"text": "downloaded the scripts .", "speaker": "A"}, {"text": "they wrote some scripts that make it easy to run the system on the wall street journal , , data .", "speaker": "A"}, {"text": "so haven't run the scripts yet .", "speaker": "A"}, {"text": "'m waiting there was one problem with part of it", "speaker": "A"}, {"text": "and wrote note to joe asking him about it .", "speaker": "A"}, {"text": "so 'm waiting to hear from him .", "speaker": "A"}, {"text": "but , , did print something out just to give you an idea about where the system is .", "speaker": "A"}, {"text": "they on their web site they , , did this little table of where their system performs relative to other systems that have done this task .", "speaker": "A"}, {"text": "and , , the mississippi state system using bigram grammar , , is at about eight point two percent .", "speaker": "A"}, {"text": "other comparable systems from , were getting from , , like six point nine , six point eight percent .", "speaker": "A"}, {"text": "this is on clean test set ?", "speaker": "B"}, {"text": "this is on clean on clean . .", "speaker": "A"}, {"text": "they they 've started table where they 're showing their results on various different noise conditions", "speaker": "A"}, {"text": "but they don't have whole lot of it filled in", "speaker": "A"}, {"text": "and didn't notice until after 'd printed it out that , , they don't say here what these different testing conditions are .", "speaker": "A"}, {"text": "you actually have to click on it on the web site to see them .", "speaker": "A"}, {"text": "so what those numbers really mean .", "speaker": "A"}, {"text": "what numbers are they getting on these on the test conditions ?", "speaker": "B"}, {"text": "see , was little confused", "speaker": "A"}, {"text": "because on this table , 'm the they 're showing word error rate .", "speaker": "A"}, {"text": "but on this one ,", "speaker": "A"}, {"text": "if these are word error rates", "speaker": "A"}, {"text": "because they 're really big .", "speaker": "A"}, {"text": "so , under condition one here it 's ten percent .", "speaker": "A"}, {"text": "then under three it goes to sixty - four point six percent .", "speaker": "A"}, {"text": "that 's probably aurora .", "speaker": "B"}, {"text": "so they 're error rates", "speaker": "A"}, {"text": "but they 're , they 're really high .", "speaker": "A"}, {"text": "don't find that surpri", "speaker": "B"}, {"text": "what 's what 's some of the lower error rates on , some of the higher error rates on , , some of these , highly mismatched difficult conditions ?", "speaker": "B"}, {"text": ", it 's around fifteen to twenty percent .", "speaker": "D"}, {"text": "and the baseline ,", "speaker": "D"}, {"text": "twenty percent error rate ,", "speaker": "D"}, {"text": "so twenty percent error rate on digits .", "speaker": "B"}, {"text": ", on digits .", "speaker": "A"}, {"text": "so if you 're doing so if you 're doing ,", "speaker": "B"}, {"text": "and this is so still the baseline .", "speaker": "D"}, {"text": "and if you 're saying sixty - thousand word recognition , getting sixty percent error on some of these noise condition not surprising .", "speaker": "B"}, {"text": "the baseline is sixty percent also on digits ,", "speaker": "D"}, {"text": "on the more mismatched conditions .", "speaker": "D"}, {"text": "so , , that 's probably what it is then .", "speaker": "A"}, {"text": "so they have lot of different conditions that they 're gonna be filling out .", "speaker": "A"}, {"text": "it 's bad sign when you looking at the numbers , you can't tell whether it 's accuracy or error rate .", "speaker": "B"}, {"text": "it 's it 's gonna be hard .", "speaker": "A"}, {"text": "they 're 'm still waiting for them to release the , , multi - cpu version of their scripts ,", "speaker": "A"}, {"text": "cuz now their script only handles processing on single cpu ,", "speaker": "A"}, {"text": "which will take really long time to run .", "speaker": "A"}, {"text": "this is for the training ?", "speaker": "B"}, {"text": "yes , for the training also .", "speaker": "A"}, {"text": "and , , they 're supposed to be coming out with it any time ,", "speaker": "A"}, {"text": "the multi - cpu one .", "speaker": "A"}, {"text": "so , as soon as they get that , then 'll 'll grab those too", "speaker": "A"}, {"text": "cuz we have to get started ,", "speaker": "B"}, {"text": "cuz it 's cuz , ,", "speaker": "B"}, {"text": "'ll go ahead and try to run it though with just the single cpu one ,", "speaker": "A"}, {"text": "and they , , released like smaller data set that you can use that only takes like sixteen hours to train and .", "speaker": "A"}, {"text": "so run it on that just to make that the thing works and everything .", "speaker": "A"}, {"text": "the actual evaluation will be in six weeks .", "speaker": "B"}, {"text": "is that about you think ?", "speaker": "B"}, {"text": "we yet , .", "speaker": "D"}, {"text": "really , we ?", "speaker": "B"}, {"text": "it wasn't on the conference call this morning ?", "speaker": "A"}, {"text": "did they say anything on the conference call about , , how the wall street journal part of the test was going to be run ?", "speaker": "A"}, {"text": "because remembered hearing that some sites were saying that they didn't have the compute to be able to run the wall street journal at their place ,", "speaker": "A"}, {"text": "so there was some talk about having mississippi state run the systems for them .", "speaker": "A"}, {"text": "did did that come up ?", "speaker": "A"}, {"text": "this first , this was not the point of this the meeting today", "speaker": "D"}, {"text": "because didn't read also the most recent mails about the large - vocabulary task .", "speaker": "D"}, {"text": "but , , did you do you still , , get the mails ?", "speaker": "D"}, {"text": "you 're not on the mailing list or what ?", "speaker": "D"}, {"text": "the only , , mail get is from mississippi state", "speaker": "A"}, {"text": ". so we should have look at this .", "speaker": "D"}, {"text": "about their system .", "speaker": "A"}, {"text": "don't get any mail about", "speaker": "A"}, {"text": "have to say , there 's something funny - sounding about saying that one of these big companies doesn't have enough cup compute power do that ,", "speaker": "B"}, {"text": "so they 're having to have it done by mississippi state .", "speaker": "B"}, {"text": "it just sounds funny .", "speaker": "B"}, {"text": "'m 'm wondering about that", "speaker": "A"}, {"text": "because there 's this whole issue about , , simple tuning parameters , like word insertion penalties .", "speaker": "A"}, {"text": "and whether or not those are going to be tuned or not ,", "speaker": "A"}, {"text": "it makes big difference .", "speaker": "A"}, {"text": "if you change your front - end , , the scale is completely can be completely different ,", "speaker": "A"}, {"text": "it seems reasonable that at least should be tweaked to match the front - end .", "speaker": "A"}, {"text": "you didn't get any answer from joe ?", "speaker": "D"}, {"text": "but joe said , , \" what you 're saying makes sense", "speaker": "A"}, {"text": "so he doesn't the answer is .", "speaker": "A"}, {"text": "that 's we had this back and forth little bit about , , are sites gonna are you gonna run this data for different sites ?", "speaker": "A"}, {"text": "and , , if mississippi state runs it , then they 'll do little optimization on that parameter ,", "speaker": "A"}, {"text": "but then he wasn't asked to run it for anybody .", "speaker": "A"}, {"text": "so it 's it 's just not clear yet what 's gonna happen .", "speaker": "A"}, {"text": "he 's been putting this out on their web site and for people to grab", "speaker": "A"}, {"text": "but haven't heard too much about what 's happening .", "speaker": "A"}, {"text": "so it could be , chuck and had actually talked about this couple times , and over some lunches , , that , , one thing that we might wanna do", "speaker": "B"}, {"text": "the - there 's this question about , , what do you wanna scale ?", "speaker": "B"}, {"text": "suppose you can't adjust these word insertion penalties and ,", "speaker": "B"}, {"text": "so you have to do everything at the level of the features .", "speaker": "B"}, {"text": "what could you do ?", "speaker": "B"}, {"text": "and , , one thing had suggested at an earlier time was some scaling ,", "speaker": "B"}, {"text": "some root of the , , , features .", "speaker": "B"}, {"text": "but the problem with that is that isn't quite the same ,", "speaker": "B"}, {"text": "it occurred to me later ,", "speaker": "B"}, {"text": "because what you really want to do is scale the , , @ @ the range of the likelihoods rather than", "speaker": "B"}, {"text": "nnn , the dist", "speaker": "D"}, {"text": "but , what might get at something similar , it just occurred to me , is an intermediate thing", "speaker": "B"}, {"text": "is because we do this strange thing that we do with the tandem system , at least in that system what you could do is take the , , , values that come out of the net ,", "speaker": "B"}, {"text": "which are something like log probabilities ,", "speaker": "B"}, {"text": "and scale those .", "speaker": "B"}, {"text": "and then , , then at least those things would have the values", "speaker": "B"}, {"text": "or the the range .", "speaker": "B"}, {"text": "and then that goes into the rest of it and then that 's used as observations .", "speaker": "B"}, {"text": "so it 's it 's , , another way to do it .", "speaker": "B"}, {"text": "but , these values are not directly used as probabilities anyway .", "speaker": "D"}, {"text": "know they 're not .", "speaker": "B"}, {"text": "so there are there is", "speaker": "D"}, {"text": "know they 're not .", "speaker": "B"}, {"text": "so because what we 're doing is pretty strange and complicated , we don't really the effect is at the other end .", "speaker": "B"}, {"text": "so , , my thought was", "speaker": "B"}, {"text": "they 're not used as probabilities ,", "speaker": "B"}, {"text": "but the log probabilities", "speaker": "B"}, {"text": "we 're taking advantage of the fact that something like log probabilities has more of gaussian shape than gaus - than probabilities ,", "speaker": "B"}, {"text": "and so we can model them better .", "speaker": "B"}, {"text": "so , in way we 're taking advantage of the fact that they 're probabilities ,", "speaker": "B"}, {"text": "because they 're this quantity that looks gaussian when you take it 's log .", "speaker": "B"}, {"text": "so , , it would have reasonable effect to do that .", "speaker": "B"}, {"text": "but , , we still haven't had ruling back on this .", "speaker": "B"}, {"text": "and we may end up being in situation where we just really can't change the word insertion penalty .", "speaker": "B"}, {"text": "but the other thing we could do is also we could", "speaker": "B"}, {"text": "this may not help us , , in the evaluation", "speaker": "B"}, {"text": "but it might help us in our understanding at least .", "speaker": "B"}, {"text": "we might , just run it with different insper insertion penalties ,", "speaker": "B"}, {"text": "and show that , , \" , , not changing it , playing the rules the way you wanted , we did this . but if we did that , it made big difference . \"", "speaker": "B"}, {"text": "wonder if it might be possible to , , simulate the back - end with some other system .", "speaker": "A"}, {"text": "so we get our front - end features ,", "speaker": "A"}, {"text": "as part of the process of figuring out the scaling of these features , , if we 're gonna take it to root or to power , we have some back - end that we attach onto our features that simulates what would be happening .", "speaker": "A"}, {"text": "and just adjust it until it 's the best number ?", "speaker": "B"}, {"text": "and just adjust it until that our version of the back - end , , decides that", "speaker": "A"}, {"text": "we can probably use the real thing ,", "speaker": "B"}, {"text": "and then jus just , , use it on reduced test set .", "speaker": "B"}, {"text": "that 's true .", "speaker": "A"}, {"text": "and then we just use that to determine some scaling factor that we use .", "speaker": "A"}, {"text": "so , that 's reasonable thing to do", "speaker": "B"}, {"text": "and the only question is what 's the actual knob that we use ?", "speaker": "B"}, {"text": "and the knob that we use should", "speaker": "B"}, {"text": "unfortunately , like say , the analytic solution to this", "speaker": "B"}, {"text": "cuz what we really want to do is change the scale of the likelihoods ,", "speaker": "B"}, {"text": "not the cha not the scale of the observations .", "speaker": "B"}, {"text": "out of curiosity , what recognizer is the one from mississippi state ?", "speaker": "E"}, {"text": "what do you mean when you say \" what kind \" ?", "speaker": "A"}, {"text": "is it like gaussian mixture model ?", "speaker": "E"}, {"text": "gaussian mixture model .", "speaker": "A"}, {"text": "it 's the same system that they use when they participate in the hub - five evals .", "speaker": "A"}, {"text": "it 's , came out of , , looking lot like htk .", "speaker": "A"}, {"text": "they started off with , when they were building their system they were always comparing to htk to make they were getting similar results .", "speaker": "A"}, {"text": "and so , it 's gaussian mixture system ,", "speaker": "A"}, {"text": "do they have the same mix - down procedure , where they start off with small number of some things", "speaker": "B"}, {"text": "and then divide the mixtures in half .", "speaker": "A"}, {"text": "if they do that .", "speaker": "A"}, {"text": "'m not really .", "speaker": "A"}, {"text": "do what tying they use ?", "speaker": "B"}, {"text": "are they some bunch of gaussians that they share across everything ?", "speaker": "B"}, {"text": "or or if it 's ?", "speaker": "B"}, {"text": "have don't have it up here", "speaker": "A"}, {"text": "but have the whole system description , that describes exactly what their system is", "speaker": "A"}, {"text": "and 'm not .", "speaker": "A"}, {"text": "it 's some mixture of gaussians and , , clustering", "speaker": "A"}, {"text": "they 're they 're trying to put in all of the standard features that people use nowadays .", "speaker": "A"}, {"text": "so the other , , aurora thing is", "speaker": "B"}, {"text": "if any of this is gonna come in time to be relevant ,", "speaker": "B"}, {"text": "but , , we had talked about , , guenter playing around , , over in germany", "speaker": "B"}, {"text": "and , @ @ , possibly coming up with something that would , , , fit in later .", "speaker": "B"}, {"text": "saw that other mail where he said that he , it wasn't going to work for him to do cvs .", "speaker": "B"}, {"text": "so now he has version of the software .", "speaker": "D"}, {"text": "so he just has it all sitting there .", "speaker": "B"}, {"text": "so if he 'll", "speaker": "B"}, {"text": "he might work on improving the noise estimate", "speaker": "B"}, {"text": "or on some histogram things ,", "speaker": "B"}, {"text": "we we didn't talk about it at our meeting", "speaker": "B"}, {"text": "but saw the just read the paper .", "speaker": "B"}, {"text": "someone , forget the name , and ney , , about histogram equalization ?", "speaker": "B"}, {"text": "did you see that one ?", "speaker": "B"}, {"text": "it was poster .", "speaker": "D"}, {"text": ", read the paper .", "speaker": "B"}, {"text": "didn't see the poster .", "speaker": "B"}, {"text": "it was something similar to on - line normalization finally", "speaker": "D"}, {"text": "in the idea of normalizing", "speaker": "D"}, {"text": "but it 's little more it 's little finer ,", "speaker": "B"}, {"text": "so they had like ten quantiles", "speaker": "B"}, {"text": "and they adjust the distribution .", "speaker": "B"}, {"text": "so you have the distributions from the training set ,", "speaker": "B"}, {"text": "so this is just histogram of the amplitudes , .", "speaker": "B"}, {"text": "and then , people do this in image processing some .", "speaker": "B"}, {"text": "you have this histogram of levels of brightness or whatever .", "speaker": "B"}, {"text": "and and then , when you get new thing that you want to adjust to be better in some way , you adjust it so that the histogram of the new data looks like the old data .", "speaker": "B"}, {"text": "you do this piece - wise linear or , , some piece - wise approximation .", "speaker": "B"}, {"text": "they did one version that was piece - wise linear and another that had power law thing between them between the points .", "speaker": "B"}, {"text": "they said they they see it in way as for the speech case as being generalization of spectral subtraction in way ,", "speaker": "B"}, {"text": "because , , in spectral subtraction you 're trying to get rid of this excess energy .", "speaker": "B"}, {"text": ", it 's not supposed to be there .", "speaker": "B"}, {"text": "this is adjusting it for lot of different levels .", "speaker": "B"}, {"text": "and then they have they have some , , floor ,", "speaker": "B"}, {"text": "so if it gets too low you don't don't do it .", "speaker": "B"}, {"text": "and they claimed very results ,", "speaker": "B"}, {"text": "so is this histogram across different frequency bins ?", "speaker": "A"}, {"text": "don't remember that .", "speaker": "B"}, {"text": "do you remember ?", "speaker": "B"}, {"text": "they have , , different histograms .", "speaker": "D"}, {"text": "something like one per frequency band ,", "speaker": "D"}, {"text": "so , one histogram per frequency bin .", "speaker": "A"}, {"text": "but should read the paper .", "speaker": "D"}, {"text": "went through the poster quickly ,", "speaker": "D"}, {"text": "and don't remember whether it was filter bank things", "speaker": "B"}, {"text": "or whether it was fft bins", "speaker": "B"}, {"text": "and and that , , histogram represents the different energy levels that have been seen at that frequency ?", "speaker": "A"}, {"text": "don't remember that .", "speaker": "B"}, {"text": "and how often they you 've seen them . .", "speaker": "B"}, {"text": "and they do they said that they could do it for the test", "speaker": "B"}, {"text": "so you don't have to change the training .", "speaker": "B"}, {"text": "you just do measurement over the training .", "speaker": "B"}, {"text": "and then , , for testing , , you can do it for one per utterance .", "speaker": "B"}, {"text": "even relatively short utterances .", "speaker": "B"}, {"text": "and they claim it works pretty .", "speaker": "B"}, {"text": "is the idea that you run test utterance through some histogram generation thing", "speaker": "A"}, {"text": "and then you compare the histograms and that tells you what to do to the utterance to make it more like ?", "speaker": "A"}, {"text": "didn't read carefully how they actually implemented it ,", "speaker": "B"}, {"text": "whether it was some , , on - line thing , or whether it was second pass , or what .", "speaker": "B"}, {"text": "but but they that that was the idea .", "speaker": "B"}, {"text": "so that seemed , , different .", "speaker": "B"}, {"text": "we 're curious about , , what are some things that are , , @ @ conceptually quite different from what we 've done .", "speaker": "B"}, {"text": "cuz we , one thing that that ,", "speaker": "B"}, {"text": "stephane and sunil seemed to find , , was , , they could actually make unified piece of software that handled range of different things that people were talking about ,", "speaker": "B"}, {"text": "and it was really just setting of different constants .", "speaker": "B"}, {"text": "and it would turn , , one thing into another .", "speaker": "B"}, {"text": "it 'd turn wiener filtering into spectral subtraction , or whatever .", "speaker": "B"}, {"text": "but there 's other things that we 're not doing .", "speaker": "B"}, {"text": "so , we 're not making any use of pitch ,", "speaker": "B"}, {"text": "which again , might be important ,", "speaker": "B"}, {"text": "because the between the harmonics is probably schmutz .", "speaker": "B"}, {"text": "and and the , , transcribers will have fun with that .", "speaker": "B"}, {"text": "the , , at the harmonics isn't so much .", "speaker": "B"}, {"text": "and we there 's this overall idea of really matching the hi distributions somehow .", "speaker": "B"}, {"text": "not just subtracting off your estimate of the noise .", "speaker": "B"}, {"text": "so , , guenter 's gonna play around with some of these things now over this next period ,", "speaker": "B"}, {"text": "don't have feedback from him ,", "speaker": "D"}, {"text": "he 's gonna ,", "speaker": "D"}, {"text": "he 's got it anyway ,", "speaker": "B"}, {"text": "so he can .", "speaker": "B"}, {"text": "so potentially if he came up with something that was useful , like diff better noise estimation module , he could ship it to you up there", "speaker": "B"}, {"text": "we could put it in .", "speaker": "B"}, {"text": "- . - .", "speaker": "D"}, {"text": "that 's good .", "speaker": "B"}, {"text": "so , why don't we just , ,", "speaker": "B"}, {"text": "starting couple weeks from now , especially if you 're not gonna be around for while , we 'll we 'll be shifting more over to some other territory .", "speaker": "B"}, {"text": "but , , ,", "speaker": "B"}, {"text": "not so much in this meeting about aurora ,", "speaker": "B"}, {"text": "just , , quickly today about you could just say little bit about what you 've been talking about with michael .", "speaker": "B"}, {"text": "and then barry can say something about what we 're talking about .", "speaker": "B"}, {"text": "so michael kleinschmidt , who 's phd student from germany , showed up this week .", "speaker": "C"}, {"text": "he 'll be here for about six months .", "speaker": "C"}, {"text": "and he 's done some work using an auditory model of , , human hearing ,", "speaker": "C"}, {"text": "and using that , to generate speech recognition features .", "speaker": "C"}, {"text": "and he did work back in germany with , , toy recognition system using , , isolated digit recognition as the task .", "speaker": "C"}, {"text": "it was actually just single - layer neural network that classified words", "speaker": "C"}, {"text": "classified digits , .", "speaker": "C"}, {"text": "and he tried that on on some aurora data and got results that he thought seemed respectable .", "speaker": "C"}, {"text": "he he 's coming here to use it on , real speech recognition system .", "speaker": "C"}, {"text": "so 'll be working with him on that .", "speaker": "C"}, {"text": "should say little more about these features ,", "speaker": "C"}, {"text": "although don't understand them that .", "speaker": "C"}, {"text": "the it 's two - stage idea .", "speaker": "C"}, {"text": "and , , the first stage of these features correspond to what 's called the peripheral auditory system .", "speaker": "C"}, {"text": "and that is like filter bank with compressive nonlinearity .", "speaker": "C"}, {"text": "and 'm - 'm not what we have @ @ in there that isn't already modeled in something like , , plp .", "speaker": "C"}, {"text": "should learn more about that .", "speaker": "C"}, {"text": "and then the second stage is , , the most different thing , , from what we usually do .", "speaker": "C"}, {"text": "it 's , it computes features which are , , based on like based on diffe different , wavelet basis functions used to analyze the input .", "speaker": "C"}, {"text": "so he uses analysis functions called gabor functions ,", "speaker": "C"}, {"text": "which have certain extent , , in time and in frequency .", "speaker": "C"}, {"text": "and the idea is these are used to sample , , the signal in represented as time - frequency representation .", "speaker": "C"}, {"text": "so you 're sampling some piece of this time - frequency plane .", "speaker": "C"}, {"text": "and , , that , , is interesting ,", "speaker": "C"}, {"text": "cuz , @ @ for one thing , you could use it , , in multi - scale way .", "speaker": "C"}, {"text": "you could have these", "speaker": "C"}, {"text": "instead of having everything like we use twenty - five millisecond or so analysis window , typically ,", "speaker": "C"}, {"text": "and that 's our time scale for features ,", "speaker": "C"}, {"text": "but you could using this , , basis function idea , you could have some basis functions", "speaker": "C"}, {"text": "which have lot longer time scale", "speaker": "C"}, {"text": "and , , some which have lot shorter ,", "speaker": "C"}, {"text": "and so it would be like set of multi - scale features .", "speaker": "C"}, {"text": "so he 's interested in ,", "speaker": "C"}, {"text": "- this is because it 's , there are these different parameters for the shape of these basis functions , there are lot of different possible basis functions .", "speaker": "C"}, {"text": "and so he actually does an optimization procedure to choose an optimal set of basis functions out of all the possible ones .", "speaker": "C"}, {"text": "what does he do to choose those ?", "speaker": "A"}, {"text": "the method he uses is funny", "speaker": "C"}, {"text": "is , , he starts with he has set of of them .", "speaker": "C"}, {"text": "he and then he uses that to classify", "speaker": "C"}, {"text": "he he tries , , using just minus one of them .", "speaker": "C"}, {"text": "so there are possible subsets of this length - vector .", "speaker": "C"}, {"text": "he tries classifying , using each of the possible sub - vectors .", "speaker": "C"}, {"text": "whichever sub - vector , , works the best , , he says the fe feature that didn't use was the most useless feature ,", "speaker": "C"}, {"text": "gets thrown out .", "speaker": "B"}, {"text": "so we 'll throw it out", "speaker": "C"}, {"text": "and we 're gonna randomly select another feature from the set of possible basis functions .", "speaker": "C"}, {"text": "so so it 's actuall", "speaker": "B"}, {"text": "it 's little bit like genetic algorithm in way .", "speaker": "A"}, {"text": "it 's it 's much simpler .", "speaker": "B"}, {"text": "it 's like greedy", "speaker": "E"}, {"text": "but it 's but it 's , it 's there 's lot number of things like about it , let me just say .", "speaker": "B"}, {"text": "so , first thing , , you 're .", "speaker": "B"}, {"text": "{nonvocalsound} in truth , both pieces of this are have their analogies in we already do .", "speaker": "B"}, {"text": "but it 's different take at how to approach it", "speaker": "B"}, {"text": "and potentially one that 's bit more systematic than what we 've done ,", "speaker": "B"}, {"text": "and bit more inspiration from auditory things .", "speaker": "B"}, {"text": "so it 's so it 's neat thing to try .", "speaker": "B"}, {"text": "the primary features , , are", "speaker": "B"}, {"text": "essentially , it 's it 's , , , plp or mel cepstrum , like that .", "speaker": "B"}, {"text": "you 've you 've got some , , compression .", "speaker": "B"}, {"text": "we always have some compression .", "speaker": "B"}, {"text": "we always have some , the the filter bank with quasi - log scaling .", "speaker": "B"}, {"text": "if you put in if you also include the rasta in it", "speaker": "B"}, {"text": "rasta the filtering being done in the log domain has an agc - like , , characteristic , which , , people typi typically put in these , , , , auditory front - ends .", "speaker": "B"}, {"text": "so it 's very , very similar ,", "speaker": "B"}, {"text": "but it 's not exactly the same .", "speaker": "B"}, {"text": "would agree that the second one is somewhat more different", "speaker": "B"}, {"text": "but , , it 's mainly different in that the things that we have been doing like that have been , had different motivation and have ended up with different kinds of constraints .", "speaker": "B"}, {"text": "so , , if you look at the lda rasta , , what they do is they look at the different eigenvectors out of the lda and they form filters out of it . ?", "speaker": "B"}, {"text": "and those filters have different , , kinds of temporal extents and temporal characteristics .", "speaker": "B"}, {"text": "and so they 're multi - scale .", "speaker": "B"}, {"text": "but , they 're not systematically multi - scale , like \" let 's start here and go to there , and go to there \" , and .", "speaker": "B"}, {"text": "it 's more like , you run it on this , you do discriminant analysis , and you find out what 's helpful .", "speaker": "B"}, {"text": "it 's multi - scale because you use several of these in parallel ,", "speaker": "C"}, {"text": "they use several of them .", "speaker": "B"}, {"text": ", you don't have to", "speaker": "B"}, {"text": "but but , , hynek has .", "speaker": "B"}, {"text": "but it 's also ,", "speaker": "B"}, {"text": "hyn - when hynek 's had people do this lda analysis , they 've done it on frequency direction", "speaker": "B"}, {"text": "and they 've done it on the time direction .", "speaker": "B"}, {"text": "he may have had people sometimes doing it on both simultaneously", "speaker": "B"}, {"text": "and that would be the closest to these gabor function things .", "speaker": "B"}, {"text": "but don't think they 've done that much of that .", "speaker": "B"}, {"text": "and , , the other thing that 's interesting the , the feature selection thing ,", "speaker": "B"}, {"text": "it 's simple method ,", "speaker": "B"}, {"text": "but kinda like it .", "speaker": "B"}, {"text": "there 's old , old method for feature selection .", "speaker": "B"}, {"text": ", , remember people referring to it as old when was playing with it twenty years ago ,", "speaker": "B"}, {"text": "so 's pretty old ,", "speaker": "B"}, {"text": "called stepwise linear discriminant analysis", "speaker": "B"}, {"text": "in which you which", "speaker": "B"}, {"text": "it 's used in social sciences lot .", "speaker": "B"}, {"text": "so , you you pick the best feature .", "speaker": "B"}, {"text": "and then you take you find the next feature that 's the best in combination with it .", "speaker": "B"}, {"text": "and then so on and so on .", "speaker": "B"}, {"text": "and what michael 's describing seems to me much , much better ,", "speaker": "B"}, {"text": "because the problem with the stepwise discriminant analysis is that you that , if you 've picked the set of features .", "speaker": "B"}, {"text": "just because something 's good feature doesn't mean that you should be adding it .", "speaker": "B"}, {"text": "so , , ,", "speaker": "B"}, {"text": "here at least you 're starting off with all of them ,", "speaker": "B"}, {"text": "and you 're throwing out useless features .", "speaker": "B"}, {"text": "that 's that seems , that seems like lot better idea .", "speaker": "B"}, {"text": "you 're always looking at things in combination with other features .", "speaker": "B"}, {"text": "so the only thing is , , there 's this artificial question of , , exactly how you how you how you assess it", "speaker": "B"}, {"text": "and if your order had been different in throwing them out .", "speaker": "B"}, {"text": "it still isn't necessarily really optimal ,", "speaker": "B"}, {"text": "but it seems like pretty good heuristic .", "speaker": "B"}, {"text": "so it 's it 's kinda neat .", "speaker": "B"}, {"text": "and and , ,", "speaker": "B"}, {"text": "the thing that wanted to add to it also was to have us use this in multi - stream way .", "speaker": "B"}, {"text": "so that , , when you come up with these different things , and these different functions , you don't necessarily just put them all into one huge vector ,", "speaker": "B"}, {"text": "but perhaps you have some of them in one stream and some of them in another stream , and .", "speaker": "B"}, {"text": "and we 've also talked little bit about , , , shihab shamma 's ,", "speaker": "B"}, {"text": "in which you the way you look at it is that there 's these different mappings", "speaker": "B"}, {"text": "and some of them emphasize , , upward moving , , energy and fre and frequency .", "speaker": "B"}, {"text": "and some are emphasizing downward", "speaker": "B"}, {"text": "and fast things and slow things and .", "speaker": "B"}, {"text": "so there 's bunch of to look at .", "speaker": "B"}, {"text": "but , , we 're sorta gonna start off with what he , , came here with", "speaker": "B"}, {"text": "and branch out branch out from there .", "speaker": "B"}, {"text": "and his advisor is here , too , at the same time .", "speaker": "B"}, {"text": "he 'll be another interesting source of wisdom .", "speaker": "B"}, {"text": "as as we were talking about this was thinking , , whether there 's relationship between , between michael 's approach to , , some optimal brain damage or optimal brain surgeon on the neural nets .", "speaker": "E"}, {"text": "so , like , if we have ,", "speaker": "E"}, {"text": "we have our we have our rasta features and", "speaker": "E"}, {"text": "and presumably the neural nets are learning some nonlinear mapping , , from the the features to this probability posterior space .", "speaker": "E"}, {"text": "and , and each of the hidden units is learning some some pattern .", "speaker": "E"}, {"text": "and it could be , like these , these auditory patterns that michael is looking at .", "speaker": "E"}, {"text": "and then when you 're looking at the , , , the best features , , you can take out you can do the do this , , brain surgery by taking out , , hidden units that don't really help .", "speaker": "E"}, {"text": "or the or features .", "speaker": "B"}, {"text": "and this is sorta like", "speaker": "E"}, {"text": "actually , you make me think very important point here is that , , if we again try to look at how is this different from what we 're already doing , , there 's , nasty argument that could be made that it 's it 's not different ,", "speaker": "B"}, {"text": "because , if you ignore the selection part", "speaker": "B"}, {"text": "because we are going into very powerful , , nonlinearity", "speaker": "B"}, {"text": "that , , is combining over time and frequency ,", "speaker": "B"}, {"text": "and is coming up with its own , better than gabor functions", "speaker": "B"}, {"text": "its , , neural net functions ,", "speaker": "B"}, {"text": "its whatever it finds to be best .", "speaker": "B"}, {"text": "so you could argue that it", "speaker": "B"}, {"text": "but don't actually believe that argument", "speaker": "B"}, {"text": "because know that , , you can ,", "speaker": "B"}, {"text": "computing features is useful ,", "speaker": "B"}, {"text": "even though in principle you haven't added anything", "speaker": "B"}, {"text": "you subtracted something , from the original waveform", "speaker": "B"}, {"text": ", if you 've you 've processed it in some way you 've typically lost something some information .", "speaker": "B"}, {"text": "and so , you 've lost information and yet it does better with features than it does with the waveform .", "speaker": "B"}, {"text": "know that sometimes it 's useful to constrain things .", "speaker": "B"}, {"text": "so that 's why it really seems like the constraint in all this it 's the constraints that are actually what matters .", "speaker": "B"}, {"text": "because if it wasn't the constraints that mattered , then we would 've completely solved this problem long ago ,", "speaker": "B"}, {"text": "because long ago we already knew how to put waveforms into powerful statistical mechanisms .", "speaker": "B"}, {"text": ", if we had infinite processing power and data , , using the waveform could", "speaker": "D"}, {"text": "then it would work .", "speaker": "B"}, {"text": "there 's the problem .", "speaker": "B"}, {"text": "so , that 's", "speaker": "D"}, {"text": "then it would work .", "speaker": "B"}, {"text": "but but , , it 's with finite of those things", "speaker": "B"}, {"text": ", we have done experiments where we literally have put waveforms in", "speaker": "B"}, {"text": "and and , ,", "speaker": "B"}, {"text": "we kept the number of parameters the same and ,", "speaker": "B"}, {"text": "and it used lot of training data .", "speaker": "B"}, {"text": "and it and it ,", "speaker": "B"}, {"text": "but lot , and then compared to the number parameters", "speaker": "B"}, {"text": "and it , it just doesn't do nearly as .", "speaker": "B"}, {"text": "so , anyway that you want to suppress", "speaker": "B"}, {"text": "it 's not just having the maximum information ,", "speaker": "B"}, {"text": "you want to suppress , , the aspects of the input signal that are not helpful for the discrimination you 're trying to make .", "speaker": "B"}, {"text": "so just briefly ,", "speaker": "B"}, {"text": "that segues into what 'm doing .", "speaker": "E"}, {"text": "so , , the big picture is , come up with set of , , intermediate categories ,", "speaker": "E"}, {"text": "then build intermediate category classifiers , then do recognition ,", "speaker": "E"}, {"text": "and , , improve speech recognition in that way .", "speaker": "E"}, {"text": "so now 'm in the phase where 'm looking at , , deciding on initial set of intermediate categories .", "speaker": "E"}, {"text": "and 'm looking for data - driven methods that can help me find , , set of intermediate categories of speech that , , will help me to discriminate later down the line .", "speaker": "E"}, {"text": "and one of the ideas , , that was to take take neural net", "speaker": "E"}, {"text": "train an ordinary neural net to , to learn the posterior probabilities of phones .", "speaker": "E"}, {"text": "at the end of the day you have this neural net", "speaker": "E"}, {"text": "and it has hidden units .", "speaker": "E"}, {"text": "and each of these hidden units is , is learning some pattern .", "speaker": "E"}, {"text": "and so , , what are these patterns ?", "speaker": "E"}, {"text": "and 'm gonna to try to look at those patterns to see , , from those patterns", "speaker": "E"}, {"text": "presumably those are important patterns for discriminating between phone classes .", "speaker": "E"}, {"text": "and some , , intermediate categories can come from just looking at the patterns of , that the neural net learns .", "speaker": "E"}, {"text": "be - before you get on the next part let me just point out that there 's there 's pretty relationship between what you 're talking about doing and what you 're talking about doing there . ?", "speaker": "B"}, {"text": "so , it seems to me that , , if you take away the the difference of this primary features , and , say , you use as we had talked about doing you use - rasta - plp for the primary features , , then this feature discovery , , thing is just what he 's talking about doing , too ,", "speaker": "B"}, {"text": "except that he 's talking about doing them in order to discover intermediate categories that correspond to these , what these sub - features are are showing you .", "speaker": "B"}, {"text": "and , , the other difference is that , , he 's doing this in in multi - band setting ,", "speaker": "B"}, {"text": "which means that he 's constraining himself to look across time in some relatively limited , , spectral extent . ?", "speaker": "B"}, {"text": "and whereas in this case you 're saying \" let 's just do it unconstrained \" .", "speaker": "B"}, {"text": "so they 're they 're really pretty related", "speaker": "B"}, {"text": "and they 'll be at some point where we 'll see the connections little better", "speaker": "B"}, {"text": "and connect them .", "speaker": "B"}, {"text": "so that 's the that 's the first part , one of the ideas to get at some patterns of intermediate categories .", "speaker": "E"}, {"text": "the other one was , , to , , come up with model , graphical model , that treats the intermediate categories as hidden variables , latent variables , that we anything about ,", "speaker": "E"}, {"text": "but that through , , statistical training and the algorithm , , at the end of the day , we have , we have learned something about these latent , latent variables", "speaker": "E"}, {"text": "which happen to correspond to intermediate categories .", "speaker": "E"}, {"text": "{nonvocalsound} , and so those are the two directions that 'm 'm looking into now .", "speaker": "E"}, {"text": "and , , .", "speaker": "E"}, {"text": "that 's that 's it .", "speaker": "E"}, {"text": "should we do our digits", "speaker": "B"}, {"text": "and get ou get our treats ?", "speaker": "B"}, {"text": "it 's like , , the little rats with the little thing dropping down to them .", "speaker": "B"}, {"text": "we do the digits and then we get our treats .", "speaker": "B"}], "id": "xxxxx", "relations": [{"y": 1, "x": 0, "type": "Explanation"}]}]