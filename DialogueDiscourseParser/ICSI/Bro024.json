[{"edus": [{"text": "and we 're on .", "speaker": "F"}, {"text": "might wanna close the door so that , stephane will", "speaker": "D"}, {"text": "'ll get it .", "speaker": "F"}, {"text": "could you go ahead and turn on , , stephane 's", "speaker": "F"}, {"text": "so that 's the virtual stephane over there .", "speaker": "D"}, {"text": "do you use pc for recording ?", "speaker": "G"}, {"text": "it 's got , , like sixteen channels going into it .", "speaker": "F"}, {"text": "the quality is quite good ?", "speaker": "G"}, {"text": "so far , it 's been pretty good .", "speaker": "F"}, {"text": "the suggestion was to have these start to", "speaker": "D"}, {"text": "why don't you go ahead , dave ?", "speaker": "F"}, {"text": "so , , the this past week 've been main mainly occupied with , , getting some results , from the sri system trained on this short hub - five training set for the mean subtraction method .", "speaker": "C"}, {"text": "ran some tests last night .", "speaker": "C"}, {"text": "the results are suspicious .", "speaker": "C"}, {"text": "it 's , , cuz they 're the baseline results are worse than , , andreas than results andreas got previously .", "speaker": "C"}, {"text": "and it could have something to do with ,", "speaker": "C"}, {"text": "that 's on digits ?", "speaker": "F"}, {"text": "that 's on digits .", "speaker": "C"}, {"text": "it it could it could have something to do with , , downsampling .", "speaker": "C"}, {"text": "that 's that 's worth looking into .", "speaker": "C"}, {"text": "ap apart from that , the main thing have ta have to talk is , , where 'm planning to go over the next week .", "speaker": "C"}, {"text": "so 've been working on integrating this mean subtraction approach into the smartkom system .", "speaker": "C"}, {"text": "and there 's this question of , , so , , in my tests before with htk found it worked it worked the best with about twelve seconds of data used to estimate the mean ,", "speaker": "C"}, {"text": "but , we 'll often have less in the smartkom system .", "speaker": "C"}, {"text": "so we 'll use as much data as we have at particular time ,", "speaker": "C"}, {"text": "and we 'll we 'll concatenate utterances together , , to get as much data as we possibly can from the user .", "speaker": "C"}, {"text": "but , , there 's question of how to set up the models . so , we could train the models .", "speaker": "C"}, {"text": "if we think twelve seconds is ideal we could train the models using twelve seconds to calculate the mean , to mean subtract the training data .", "speaker": "C"}, {"text": "or we could , , use some other amount .", "speaker": "C"}, {"text": "so like did an experiment where , , was using six seconds in test ,", "speaker": "C"}, {"text": "but , for tried twelve seconds in train .", "speaker": "C"}, {"text": "and tried , , the same in train", "speaker": "C"}, {"text": "'m tried six seconds in train .", "speaker": "C"}, {"text": "and six seconds in train was about point three percent better .", "speaker": "C"}, {"text": "and , it 's not clear to me yet whether that 's something significant .", "speaker": "C"}, {"text": "so wanna do some tests and , , actually make some plots of , for particular amount of data and test what happens if you vary the amount of data in train .", "speaker": "C"}, {"text": "guenter , if you followed this but this is , , , long - term long - term window", "speaker": "D"}, {"text": "he you talked about it .", "speaker": "D"}, {"text": "we spoke about it already ,", "speaker": "G"}, {"text": "so what he 's doing .", "speaker": "D"}, {"text": "so was actually ran the experiments mostly", "speaker": "C"}, {"text": "and was was hoping to have the plots with me today .", "speaker": "C"}, {"text": "didn't get to it .", "speaker": "C"}, {"text": "wou would be curious about people 's feedback on this", "speaker": "C"}, {"text": "cuz 'm @ @ there are some it 's it 's like bit of tricky engineering problem .", "speaker": "C"}, {"text": "'m trying to figure out what 's the optimal way to set this up .", "speaker": "C"}, {"text": "so , , 'll try to make the plots and then put some postscript up on my on my web page .", "speaker": "C"}, {"text": "and 'll mention it in my status report if people wanna take look .", "speaker": "C"}, {"text": "you could clarify something for me .", "speaker": "D"}, {"text": "you 're saying point three percent ,", "speaker": "D"}, {"text": "you take point three percent hit , when the training and testing links are don't match ?", "speaker": "D"}, {"text": "is that what it is ?", "speaker": "D"}, {"text": "don't 's just for any mismatch you take hit .", "speaker": "C"}, {"text": "in some cases it might be better to have mismatch .", "speaker": "C"}, {"text": "like saw something like if you only have two seconds in test , or , , it was something like four seconds , you actually do little better if you , , train on six seconds than if you train on four seconds .", "speaker": "C"}, {"text": "but the case , with the point three percent hit was using six seconds in test , , comparing train on twelve seconds versus train on six seconds .", "speaker": "C"}, {"text": "and which was worse ?", "speaker": "D"}, {"text": "the train on twelve seconds .", "speaker": "C"}, {"text": "but point three percent , , from what to what ?", "speaker": "D"}, {"text": "that 's point three percent", "speaker": "D"}, {"text": "on the the accuracies went from it was something vaguely like ninety - five point six accuracy , , improved to ninety - five point nine wh when", "speaker": "C"}, {"text": "so four point four to four point one .", "speaker": "D"}, {"text": "so about about an eight percent , , seven or eight percent relative ?", "speaker": "D"}, {"text": "in , if you were going for an evaluation system you 'd care .", "speaker": "D"}, {"text": "but if you were doing live system that people were actually using nobody would notice .", "speaker": "D"}, {"text": "it 's , to get something that 's practical , that you could really use .", "speaker": "D"}, {"text": "that 's that 's interesting .", "speaker": "C"}, {"text": "alright , the , see your point .", "speaker": "C"}, {"text": "was thinking of it as , , an interesting research problem .", "speaker": "C"}, {"text": "the how to was thinking that for the asru paper we could have section saying , \" for smartkom , we in we tried this approach in , , interactive system \" , which don't think has been done before .", "speaker": "C"}, {"text": "and and then there was two research questions from that .", "speaker": "C"}, {"text": "and one is the does it still work if you just use the past history ?", "speaker": "C"}, {"text": "and the other was this question of , what was just talking about now .", "speaker": "C"}, {"text": "so that 's why it was interesting .", "speaker": "C"}, {"text": "short - time fft short - time cepstrum calculation , , mean mean calculation work that people have in commercial systems , they do this all the time .", "speaker": "D"}, {"text": "they the they calculate it from previous utterances and then use it , .", "speaker": "D"}, {"text": "but but , ,", "speaker": "D"}, {"text": "as you say , there hasn't been that much with this long - time , , spectra work .", "speaker": "D"}, {"text": "so that 's that 's standard .", "speaker": "C"}, {"text": "no , it is interesting .", "speaker": "D"}, {"text": "and the other thing is , , there 's two sides to these really small , , gradations in performance .", "speaker": "D"}, {"text": ", on the one hand in practical system if something is , , four point four percent error , four point one percent error , people won't really tell be able to tell the difference .", "speaker": "D"}, {"text": "on the other hand , when you 're doing , , research , you may , you might find that the way that you build up change from ninety - five percent accurate system to ninety - eight percent accurate system is through ten or twelve little things that you do that each are point three percent .", "speaker": "D"}, {"text": "so so the they it 's don't mean to say that they 're they 're irrelevant .", "speaker": "D"}, {"text": "they are relevant .", "speaker": "D"}, {"text": "but , , for demo , you won't see it .", "speaker": "D"}, {"text": "let 's let 's see .", "speaker": "C"}, {"text": "and then there 's , another thing wanna start looking at , , wi is , , the choice of the analysis window length .", "speaker": "C"}, {"text": "so 've just been using two seconds", "speaker": "C"}, {"text": "just because that 's what carlos did before .", "speaker": "C"}, {"text": "wrote to him asking about he chose the two seconds .", "speaker": "C"}, {"text": "and it seemed like he chose it bit informally .", "speaker": "C"}, {"text": "with the with the htk set - up should be able to do some experiments , on just varying that length ,", "speaker": "C"}, {"text": "say between one and three seconds , in few different reverberation conditions ,", "speaker": "C"}, {"text": "say this room and also few of the artificial impulse responses we have for reverberation ,", "speaker": "C"}, {"text": "just , , making some plots and seeing how they look .", "speaker": "C"}, {"text": "with the sampling rate was using , one second or two seconds or four seconds is at power of two , number of samples", "speaker": "C"}, {"text": "and , , 'll 'll jus for the ones in between 'll just zero - pad .", "speaker": "C"}, {"text": "one thing that might also be an issue , , cuz part of what you 're doing is you 're getting spectrum over bunch of different kinds of speech sounds .", "speaker": "D"}, {"text": "and so it might matter how fast someone was talking .", "speaker": "D"}, {"text": "if you if there 's lot of phones in one second you 'll get really good sampling of all these different things ,", "speaker": "D"}, {"text": "and , , on the other hand if someone 's talking slowly you 'd need more .", "speaker": "D"}, {"text": "if you have some samples of faster or slower speech", "speaker": "D"}, {"text": "but it might make difference .", "speaker": "D"}, {"text": "don't don't think the ti - digits data that have , , is would be appropriate for that .", "speaker": "C"}, {"text": "but what do you what about if fed it through some , , speech processing algorithm that changed the speech rate ?", "speaker": "C"}, {"text": "but then you 'll have the degradation of , , whatever you do , added onto that .", "speaker": "D"}, {"text": "if you get something that sounds that 's does pretty job at that .", "speaker": "D"}, {"text": ", just if you 's worth looking into .", "speaker": "C"}, {"text": "you could imagine that .", "speaker": "D"}, {"text": "it is getting little away from reverberation .", "speaker": "C"}, {"text": "it 's just that you 're making choice", "speaker": "D"}, {"text": "was thinking more from the system aspect , if you 're making choice for smartkom , that that it might be that it 's it the optimal number could be different , depending on", "speaker": "D"}, {"text": "and and the third thing , , , is , , barry explained lda filtering to me yesterday .", "speaker": "C"}, {"text": "and so , , mike shire in his thesis , did series of experiments , , training lda filters in on different conditions .", "speaker": "C"}, {"text": "and you were interested in having me repeat this for", "speaker": "C"}, {"text": "for this mean subtraction approach ?", "speaker": "C"}, {"text": "is is that ?", "speaker": "C"}, {"text": "or for these long analysis windows , , is the way to put it .", "speaker": "C"}, {"text": "the the issue was the general issue was bringing up was that if you 're have moving window , , wa set of weights times things that , , move along , shift along in time , that you have linear time invariant filter .", "speaker": "D"}, {"text": "and you just happened to have picked particular one by setting all the weights to be equal .", "speaker": "D"}, {"text": "and so the issue is what are some other filters that you could use , , in that sense of \" filter \" ?", "speaker": "D"}, {"text": "as was saying , the simplest thing to do is not to train anything , but just to do some , , hamming or hanning , , window , thing ,", "speaker": "D"}, {"text": "just to de - emphasize the jarring .", "speaker": "D"}, {"text": "so that would be the first thing to do .", "speaker": "D"}, {"text": "but then , , the lda , is interesting because it would say , suppose you actually trained this up to do the best you could by some criterion ,", "speaker": "D"}, {"text": "what would the filter look like then ?", "speaker": "D"}, {"text": "that 's what we 're doing in this aur - aurora .", "speaker": "D"}, {"text": "it 's still not clear to me in the long run whether the best thing to do would be to do that or to have some stylized version of the filter that looks like these things you 've trained up , because you always have the problem that it 's trained up for one condition and it isn't quite for another .", "speaker": "D"}, {"text": "that 's why that 's why rasta filter has actually ended up lasting long time ,", "speaker": "D"}, {"text": "people still using it quite bit , because you don't change it .", "speaker": "D"}, {"text": "doesn't get any worse .", "speaker": "D"}, {"text": "actually was just thinking about what was asking about earlier , wi which is about having less than say twelve seconds in the smartkom system to do the mean subtraction .", "speaker": "C"}, {"text": "you said in systems where you use cepstral mean subtraction , they concatenate utterances", "speaker": "C"}, {"text": "and , do how they address this issue of , , testing versus training ?", "speaker": "C"}, {"text": "what they do is they do it always on - line ,", "speaker": "G"}, {"text": "that you just take what you have from the past ,", "speaker": "G"}, {"text": "that you calculate the mean of this and subtract the mean .", "speaker": "G"}, {"text": "and then you can , you can increase your window whi while you get while you are getting more samples .", "speaker": "G"}, {"text": "and , , so in tha in that case , wh what do they do when they 're , performing the cepstral mean subtraction on the training data ?", "speaker": "C"}, {"text": "so because you 'd have hours and hours of training data .", "speaker": "C"}, {"text": "so do they cut it off and start over ?", "speaker": "C"}, {"text": "so do you have , you mean you have files which are hours of hours long ?", "speaker": "G"}, {"text": "usually you have in the training set you have similar conditions ,", "speaker": "G"}, {"text": "file lengths are , the same order or in the same size as for test data , or", "speaker": "G"}, {"text": "so if someone 's interacting with the system , though , , morgan , morgan said that you would tend to , , chain utterances together", "speaker": "C"}, {"text": "what was what was saying was that , , at any given point you are gonna start off with what you had from before .", "speaker": "D"}, {"text": "and so if you 're splitting things up into utterances", "speaker": "D"}, {"text": "so , , in dialogue system , where you 're gonna be asking , , , for some information , there 's some initial something .", "speaker": "D"}, {"text": "and , , the first time out you might have some general average .", "speaker": "D"}, {"text": "but you you don't have very much information yet .", "speaker": "D"}, {"text": "but at after they 've given one utterance you 've got something .", "speaker": "D"}, {"text": "you can compute your mean cepstra from that ,", "speaker": "D"}, {"text": "and then can use it for the next thing that they say ,", "speaker": "D"}, {"text": "so that , , the performance should be better that second time .", "speaker": "D"}, {"text": "and the heuristics of exactly how people handle that and how they handle their training 'm vary from place to place .", "speaker": "D"}, {"text": "but the ideally , it seems to me anyway , that you would wanna do the same thing in training as you do in test .", "speaker": "D"}, {"text": "but that 's that 's just , , prejudice .", "speaker": "D"}, {"text": "and anybody working on this with some particular task would experiment .", "speaker": "D"}, {"text": "the question had was , , amount of data was the amount of data that you 'd give it to , update this estimate .", "speaker": "C"}, {"text": "because say you if you have say five thousand utterances in your training set , , and you keep the mean from the last utterance ,", "speaker": "C"}, {"text": "by the time it gets to the five thousandth utterance", "speaker": "C"}, {"text": "no , but those are all different people with different , in", "speaker": "D"}, {"text": "so , in the in telephone task , these are different phone calls .", "speaker": "D"}, {"text": "so you don't wanna @ @ chain it together from from different phone call .", "speaker": "D"}, {"text": "so so they would", "speaker": "C"}, {"text": "so it 's within speaker ,", "speaker": "D"}, {"text": "within phone call ,", "speaker": "D"}, {"text": "if it 's dialogue system , it 's within whatever this characteristic you 're trying to get rid of is expected to be consistent over ,", "speaker": "D"}, {"text": "so you 'd you and so in training you would start over at every new phone call or at every new speaker .", "speaker": "C"}, {"text": "now , , you 'd use something from the others", "speaker": "D"}, {"text": "just because at the beginning of call you anything ,", "speaker": "D"}, {"text": "and so you might have some general thing that 's your best to start with .", "speaker": "D"}, {"text": "lot of these things are proprietary", "speaker": "D"}, {"text": "so we 're doing little bit of guesswork here .", "speaker": "D"}, {"text": "what do comp what do people do who really face these problems in the field ?", "speaker": "D"}, {"text": "and they don't tell other people exactly what they do .", "speaker": "D"}, {"text": "but but , when you the hints that you get from what they when they talk about it are that they do they all do something like this .", "speaker": "D"}, {"text": "bec - because so this smartkom task first off , it 's this tv and movie information system .", "speaker": "C"}, {"text": "but you might have somebody who 's using it", "speaker": "D"}, {"text": "and then later you might have somebody else who 's using it .", "speaker": "D"}, {"text": "and so you 'd wanna set some", "speaker": "D"}, {"text": "was was about to say . so if you ask it \" what what movies are on tv tonight ? \" ,", "speaker": "C"}, {"text": "if look at my wristwatch when say that it 's about two seconds .", "speaker": "C"}, {"text": "the way currently have the mean subtraction , , set up , the analysis window is two seconds .", "speaker": "C"}, {"text": "so what you just said , about what do you start with , raises question of what do start with then ?", "speaker": "C"}, {"text": "so in that situation , though , what 's little different there , is you 're talking about there 's only one", "speaker": "D"}, {"text": "it it also depends", "speaker": "D"}, {"text": "we 're getting little off track here .", "speaker": "D"}, {"text": "there 's been some discussion about whether the work we 're doing in that project is gonna be for the kiosk or for the mobile or for both .", "speaker": "D"}, {"text": "and for this discussion it matters .", "speaker": "D"}, {"text": "if it 's in the kiosk , then the physical situation is the same .", "speaker": "D"}, {"text": "it 's gonna , the exact interaction of the microphone 's gonna differ depending on the person and .", "speaker": "D"}, {"text": "but at least the basic acoustics are gonna be the same .", "speaker": "D"}, {"text": "so if it 's really in one kiosk , then that you could just chain together and , as much as much speech as possible to", "speaker": "D"}, {"text": "because what you 're really trying to get at is the is the reverberation characteristic .", "speaker": "D"}, {"text": "but in the case of the mobile , , presumably the acoustic 's changing all over the place .", "speaker": "D"}, {"text": "and in that case you probably don't wanna have it be endless because you wanna have some it 's not question of how long do you 's you can get an approximation to stationary something , given that it 's not really stationary .", "speaker": "D"}, {"text": "and just started thinking of another question ,", "speaker": "C"}, {"text": "which is , for the very first frame , what do do", "speaker": "C"}, {"text": "if 'm if take if use that frame to calculate the mean , then 'm just gonna get nothing .", "speaker": "C"}, {"text": "so should probably have some default mean for the first couple of frames ?", "speaker": "C"}, {"text": "or subtract nothing .", "speaker": "D"}, {"text": "or subtract nothing .", "speaker": "C"}, {"text": "and and that 's that 's something that 's people have figured out how to deal with in cepstral mean subtraction as ?", "speaker": "C"}, {"text": "people do something .", "speaker": "D"}, {"text": "they they , , they have some , ,", "speaker": "D"}, {"text": "in cepstral mean subtraction , for short - term window analysis windows , as is usually done , you 're trying to get rid of some very general characteristic .", "speaker": "D"}, {"text": "and so , , if you have any other information about what general characteristic would be , then you can do it there .", "speaker": "D"}, {"text": "you can also reflect the data .", "speaker": "F"}, {"text": "so you take ,", "speaker": "F"}, {"text": "'m not how many frames you need .", "speaker": "F"}, {"text": "but you take that many from the front and flip it around to as the negative value .", "speaker": "F"}, {"text": "so you can always", "speaker": "F"}, {"text": "the other thing is that and remember doing this , is that if you have multi - pass system , , if the first pass ta it takes most of the computation , the second and the third pass could be very , very quick ,", "speaker": "D"}, {"text": "just looking at relatively small small , , space of hypotheses .", "speaker": "D"}, {"text": "then you can do your first pass without any subtraction .", "speaker": "D"}, {"text": "and then your second pass , , eliminates those most of those hypotheses by , by having an improved version of the analysis .", "speaker": "D"}, {"text": "so that was all had , for now .", "speaker": "C"}, {"text": "do you wanna go , barry ?", "speaker": "F"}, {"text": "so for the past , , week an or two , 've been just writing my , , formal thesis proposal .", "speaker": "A"}, {"text": "so 'm taking this qualifier exam that 's coming up in two weeks .", "speaker": "A"}, {"text": "and finish writing proposal and submit it to the committee .", "speaker": "A"}, {"text": "and , should should explain , , more about what 'm proposing to do , and and ?", "speaker": "A"}, {"text": "yes , briefly .", "speaker": "D"}, {"text": "so briefly , 'm proposing to do new approach to speech recognition using , combination of , , multi - band ideas and ideas , , about the , acoustic phonec phonetic approach to speech recognition .", "speaker": "A"}, {"text": "so will be using these graphical models that , that implement the multi - band approach to recognize set of intermediate categories that might involve , , things like phonetic features or other feature things that are more closely related to the acoustic signal itself .", "speaker": "A"}, {"text": "and the hope in all of this is that by going multi - band and by going into these , intermediate classifications , that we can get system that 's more robust to unseen noises , and situations like that .", "speaker": "A"}, {"text": "some of the research issues involved in this are , , one , what intermediate categories do we need to classify ?", "speaker": "A"}, {"text": "another one is , what other types of structures in these multi - band graphical models should we consider in order to , combine evidence from the sub - bands ?", "speaker": "A"}, {"text": "and , , the third one is how do we how do we merge all the , , information from the individual , multi - band classifiers to come up with word recognition or phone recognition things .", "speaker": "A"}, {"text": "so that 's that 's what 've been doing .", "speaker": "A"}, {"text": "so you 've got two weeks , ?", "speaker": "F"}, {"text": "got two weeks to brush up on , presentation", "speaker": "A"}, {"text": "you were finishing your thesis in two weeks .", "speaker": "D"}, {"text": "are you gonna do any dry runs for your thing ,", "speaker": "F"}, {"text": "or are you just gonna", "speaker": "F"}, {"text": "'m gonna do some .", "speaker": "A"}, {"text": "would you be interested ?", "speaker": "A"}, {"text": "to help out ?", "speaker": "A"}, {"text": "is that it ?", "speaker": "F"}, {"text": "that 's it .", "speaker": "A"}, {"text": "let 's see .", "speaker": "F"}, {"text": "so we 've got forty minutes left ,", "speaker": "F"}, {"text": "and it seems like there 's lot of material .", "speaker": "F"}, {"text": "an - any suggestions about where we where we should go next ?", "speaker": "F"}, {"text": "mmm , @ @ .", "speaker": "B"}, {"text": "do you wanna go , sunil ?", "speaker": "F"}, {"text": "we 'll just start with you .", "speaker": "F"}, {"text": "but actually stuck most of this in our last meeting with guenter .", "speaker": "B"}, {"text": "so the last week , , showed some results with only speechdat - car", "speaker": "B"}, {"text": "which was like some fifty - six percent .", "speaker": "B"}, {"text": "and , , didn't", "speaker": "B"}, {"text": "found that the results", "speaker": "B"}, {"text": "wasn't getting that results on the ti - digit .", "speaker": "B"}, {"text": "so was like looking into \" why , what is wrong with the ti - digits ? \" .", "speaker": "B"}, {"text": "why why was not getting it .", "speaker": "B"}, {"text": "and found that , the noise estimation is reason for the ti - digits to perform worse than the baseline .", "speaker": "B"}, {"text": "so , , actually , picked", "speaker": "B"}, {"text": "the first thing did was scaled the noise estimate by factor which is less than one to see if that because found there are lot of zeros in the spectrogram for the ti - digits when used this approach .", "speaker": "B"}, {"text": "so the first thing did was scaled the noise estimate .", "speaker": "B"}, {"text": "so the results that 've shown here are the complete results using the new", "speaker": "B"}, {"text": "the the new technique is nothing but the noise estimate scaled by factor of point five .", "speaker": "B"}, {"text": "so it 's just an ad - hoc", "speaker": "B"}, {"text": "some intermediate result , because it 's not optimized for anything .", "speaker": "B"}, {"text": "so the results the trend the only trend could see from those results was like the the current noise estimation or the , , noise composition scheme is working good for like the car noise type of thing .", "speaker": "B"}, {"text": "because 've the only very good result in the ti - digits is the noise car noise condition for their test - ,", "speaker": "B"}, {"text": "which is like the best could see that", "speaker": "B"}, {"text": "for any non - stationary noise like \" babble \" or \" subway \" or any \" street \" , some \" restaurant \" noise , it 's like it 's not performing very .", "speaker": "B"}, {"text": "the so that 's the first thing , could make out from this .", "speaker": "B"}, {"text": "what is important to see is that there is big difference between the training modes .", "speaker": "G"}, {"text": "if you have clean training , you get also fifty percent improvement .", "speaker": "G"}, {"text": "but if you have muddy condition training you get only twenty percent .", "speaker": "G"}, {"text": "and in that twenty percent @ @ it 's very inconsistent across different noise conditions .", "speaker": "B"}, {"text": "so have like forty - five percent for \" car noise \"", "speaker": "B"}, {"text": "and then there 's minus five percent for the \" babble \" ,", "speaker": "B"}, {"text": "and there 's this thirty - three for the \" station \" .", "speaker": "B"}, {"text": "and so it 's not it 's not actually very consistent across .", "speaker": "B"}, {"text": "the only correlation between the speechdat - car and this performance is the stationarity of the noise that is there in these conditions and the speechdat - car .", "speaker": "B"}, {"text": "so the overall result is like in the last page ,", "speaker": "B"}, {"text": "which is like forty - seven ,", "speaker": "B"}, {"text": "which is still very imbalanced because there are like fifty - six percent on the speechdat - car and thirty - five percent on the ti - digits .", "speaker": "B"}, {"text": "ps the fifty - six percent is like comparable to what the french telecom gets ,", "speaker": "B"}, {"text": "but the thirty - five percent is way off .", "speaker": "B"}, {"text": "'m looking on the second page ,", "speaker": "D"}, {"text": "and it says \" fifty percent \"", "speaker": "D"}, {"text": "looking in the lower - hand corner ,", "speaker": "D"}, {"text": "\" fifty percent relative performance \" .", "speaker": "D"}, {"text": "for the clean training .", "speaker": "G"}, {"text": "and if you if you look", "speaker": "G"}, {"text": "is that fifty percent improvement ?", "speaker": "D"}, {"text": "for that 's for the clean training and the noisy testing for the ti - digits .", "speaker": "B"}, {"text": "so it 's improvement over the baseline mel cepstrum ?", "speaker": "D"}, {"text": "but the baseline mel cepstrum under those training doesn't do as", "speaker": "D"}, {"text": "'m 'm trying to understand why it 's it 's eighty percent", "speaker": "D"}, {"text": "that 's an accuracy number , ,", "speaker": "D"}, {"text": "so that 's not as good as the one up above .", "speaker": "D"}, {"text": "but the fifty is better than the one up above ,", "speaker": "D"}, {"text": "so 'm confused .", "speaker": "D"}, {"text": "actually the noise compensation whatever , , we are put in it works very for the high mismatch condition .", "speaker": "B"}, {"text": "it 's consistent in the speechdat - car", "speaker": "B"}, {"text": "and in the clean training also it gives it", "speaker": "B"}, {"text": "but this fifty percent is that the high mismatch performance equivalent to the high mismatch performance in the speech .", "speaker": "B"}, {"text": "so so since the high mismatch performance is much worse to begin with , it 's easier to get better relative improvement .", "speaker": "F"}, {"text": "so by putting this noise", "speaker": "B"}, {"text": "if we look at the figures on the , we see that the reference system is very bad .", "speaker": "E"}, {"text": "the reference drops like very fast", "speaker": "B"}, {"text": "like for clean training condition .", "speaker": "E"}, {"text": "this is this is ti digits we 're looking at ?", "speaker": "D"}, {"text": "this whole page is ti - digits", "speaker": "D"}, {"text": "or this is ?", "speaker": "D"}, {"text": "it 's not written anywhere .", "speaker": "B"}, {"text": "it 's ti - digits .", "speaker": "B"}, {"text": "the first spreadsheet is ti - digits .", "speaker": "B"}, {"text": "how does clean training do for the , , \" car \"", "speaker": "D"}, {"text": "the \" car \" ?", "speaker": "B"}, {"text": "still it still , that 's still consistent .", "speaker": "B"}, {"text": "get the best performance in the case of \" car \" , which is the third column in the condition .", "speaker": "B"}, {"text": "this is added noise .", "speaker": "D"}, {"text": "this is ti - digits .", "speaker": "D"}, {"text": "in the , , multi - language , , finnish and", "speaker": "D"}, {"text": "this is next page .", "speaker": "G"}, {"text": "that 's the next spreadsheet , is", "speaker": "B"}, {"text": "so that is the performance for italian , finnish and spanish .", "speaker": "B"}, {"text": "\" training condition \"", "speaker": "D"}, {"text": "so \" clean \" corresponds to \" high mismatch \" .", "speaker": "D"}, {"text": "and \" increase \" ,", "speaker": "D"}, {"text": "that 's \" percentage increase \" is the percentage improvement over the baseline .", "speaker": "B"}, {"text": "it 's it 's", "speaker": "G"}, {"text": "which means decrease in word error rate ?", "speaker": "D"}, {"text": "so \" percentage increase \" means decrease ?", "speaker": "D"}, {"text": "the the there was very long discussion about this on the on the , , amsterdam meeting .", "speaker": "G"}, {"text": "how to how to calculate it then .", "speaker": "G"}, {"text": "there 's there 's", "speaker": "B"}, {"text": "you are using finally this the scheme which they", "speaker": "G"}, {"text": "which is there in the spreadsheet .", "speaker": "B"}, {"text": "'m not changing anything in there .", "speaker": "B"}, {"text": "so all the hi numbers are very good ,", "speaker": "B"}, {"text": "in the sense , they are better than what the french telecom gets .", "speaker": "B"}, {"text": "but the only number that 's still , which stephane also got in his result was that medium mismatch of the finnish ,", "speaker": "B"}, {"text": "which is very which is very strange situation where we used the we changed the proto for initializing the", "speaker": "B"}, {"text": "this is because it gets stuck in some local minimum in the training .", "speaker": "B"}, {"text": "that seventy - five point seven nine in the finnish mismatch which is that the eleven point nine six what we see .", "speaker": "B"}, {"text": "so we have to jiggle it somehow ?", "speaker": "D"}, {"text": "so we start with that different proto and it becomes eighty - eight ,", "speaker": "B"}, {"text": "which is like some fifty percent improvement .", "speaker": "B"}, {"text": "start with different what ?", "speaker": "D"}, {"text": "which is like different initialization for the , , transition probabilities .", "speaker": "B"}, {"text": "it 's just that now , the initialization is to stay more in the current state ,", "speaker": "B"}, {"text": "which is point four point six , ?", "speaker": "B"}, {"text": "and if it changes to point five , which is equal @ @ for transition and self loop where it becomes eighty - eight percent .", "speaker": "B"}, {"text": "but that involves mucking with the back - end ,", "speaker": "F"}, {"text": "we can't do it .", "speaker": "B"}, {"text": "which is not allowed .", "speaker": "F"}, {"text": "it , like ,", "speaker": "G"}, {"text": "it is known , this medium match condition of the finnish data has some strange effects .", "speaker": "G"}, {"text": "it has very few at , actually , , tran , words also .", "speaker": "B"}, {"text": "it 's very , very small set , actually .", "speaker": "B"}, {"text": "there is there is lot of , there are lot of utterances with music in with music in the background .", "speaker": "G"}, {"text": "it has some music also .", "speaker": "B"}, {"text": "very horrible music like", "speaker": "B"}, {"text": "so for that one you need much smarter vad ?", "speaker": "D"}, {"text": "if it 's music .", "speaker": "D"}, {"text": "that 's the that 's about the results .", "speaker": "B"}, {"text": "the summary is like", "speaker": "B"}, {"text": "so there are the other thing what tried was , which explained in the last meeting , is using the channel zero for , , for both dropping and estimating the noise .", "speaker": "B"}, {"text": "and that 's like just to get feel of how good it is .", "speaker": "B"}, {"text": "the fifty - six percent improvement in the speechdat - car becomes like sixty - seven percent .", "speaker": "B"}, {"text": "like ten percent better .", "speaker": "B"}, {"text": "but that 's that 's not that 's cheating experiment .", "speaker": "B"}, {"text": "but the but the , , forty - seven point nine percent which you have now , that 's already remarkable improvement in comparison to the first proposal .", "speaker": "G"}, {"text": "so we had forty - four percent in the first proposal .", "speaker": "B"}, {"text": "we have big im", "speaker": "B"}, {"text": "so the major improvement that we got was in all the high mismatch cases ,", "speaker": "B"}, {"text": "because all those numbers were in sixties and seventies", "speaker": "B"}, {"text": "because we never had any noise compensations .", "speaker": "B"}, {"text": "so that 's where the biggest improvement came up .", "speaker": "B"}, {"text": "not much in the match and the medium match and ti - digits also now .", "speaker": "B"}, {"text": "so this is still at three or four percent improvement over the first proposal .", "speaker": "B"}, {"text": "so that 's good .", "speaker": "D"}, {"text": "then if we can improve the noise estimation , then it should get better .", "speaker": "D"}, {"text": "started thinking about also", "speaker": "G"}, {"text": ", discovered the same problem when started working on , on this aurora task almost two years ago ,", "speaker": "G"}, {"text": "that you have the problem with this mulit", "speaker": "G"}, {"text": "at the beginning we had only this multi condition training of the ti - digits .", "speaker": "G"}, {"text": "and , , found the same problem .", "speaker": "G"}, {"text": "just taking , what we were used to use , , , some type of spectral subtraction , you get even worse results than the basis", "speaker": "G"}, {"text": "tried to find an explanation for it ,", "speaker": "G"}, {"text": "stephane also has the same experience of using the spectral subtraction ?", "speaker": "B"}, {"text": "so here , found that it 's if changed the noise estimate could get an improvement .", "speaker": "B"}, {"text": "so that 's so it 's something which actually pursue , is the noise estimate .", "speaker": "B"}, {"text": "what you do is in when you have the this multi - condition training mode , then you have then you can train models for the speech , for the words , as as for the pauses where you really have all information about the noise available .", "speaker": "G"}, {"text": "at the beginning it was not surprising to me that you get really the best results on doing it this way ,", "speaker": "G"}, {"text": "in comparison to any type of training on clean data and any type of processing .", "speaker": "G"}, {"text": "it seems to be the best what wh what we can do in this moment is multi - condition training .", "speaker": "G"}, {"text": "and every when we now start introducing some noise reduction technique we introduce also somehow artificial distortions .", "speaker": "G"}, {"text": "and these artificial distortions , have the feeling that they are the reason why we have the problems in this multi - condition training .", "speaker": "G"}, {"text": "that means the ms we trained , they are they are based on gaussians ,", "speaker": "G"}, {"text": "and on modeling gaussians .", "speaker": "G"}, {"text": "can move little bit with this ?", "speaker": "G"}, {"text": "and if we introduce now this spectral subtraction , or wiener filtering", "speaker": "G"}, {"text": "so , usually what you have is ,", "speaker": "G"}, {"text": "'m 'm showing now an envelope", "speaker": "G"}, {"text": "you 'll for this time .", "speaker": "G"}, {"text": "so usually you have in clean condition you have something which looks like this .", "speaker": "G"}, {"text": "and if it is noisy it is somewhere here .", "speaker": "G"}, {"text": "and then you try to subtract it or wiener filter or whatever .", "speaker": "G"}, {"text": "and what you get is you have always these problems , that you have this these zeros in there .", "speaker": "G"}, {"text": "and you have to do something if you get these negative values .", "speaker": "G"}, {"text": "this is your noise estimate and you somehow subtract it or do whatever .", "speaker": "G"}, {"text": "and then you have", "speaker": "G"}, {"text": "and then what you do is you introduce some artificial distribution in this", "speaker": "G"}, {"text": "in the models .", "speaker": "G"}, {"text": "you train it also this way", "speaker": "G"}, {"text": "but , somehow there is there is no longer gaussian distribution .", "speaker": "G"}, {"text": "it is somehow strange distribution which we introduce with these artificial distortions .", "speaker": "G"}, {"text": "and and was thinking that might be the reason why you get these problems in the especially in the multi - condition training mode .", "speaker": "G"}, {"text": "- that 's true .", "speaker": "B"}, {"text": "the the models are not complex enough to absorb that additional variability that you 're introducing .", "speaker": "B"}, {"text": "also have the feeling that , the reason ye why it doesn't work is , that the models are much are , not complex enough .", "speaker": "E"}, {"text": "because actually als always had good experience with spectral subtraction ,", "speaker": "E"}, {"text": "just straight spectral subtraction algorithm when was using neural networks , big neural networks , which are more able to model strange distributions", "speaker": "E"}, {"text": "then tried the same exactly the same spectral subtraction algorithm on these aurora tasks", "speaker": "E"}, {"text": "and it simply doesn't work .", "speaker": "E"}, {"text": "it 's even it , , hurts even .", "speaker": "E"}, {"text": "we probably should at some point here try the tandem the system - two with this , with the spectral subtraction for that reason .", "speaker": "D"}, {"text": "cuz again , it should do transformation to domain where it looks more gaussian .", "speaker": "D"}, {"text": "just yesterday when was thinking about it what we could try to do , or do about it", "speaker": "G"}, {"text": "if you if you get at this in this situation that you get this negative values and you simply set it to zero or to constant or whatever if we would use there somehow , random generator which has certain distribution , not certain , special distribution we should see we have to think about it .", "speaker": "G"}, {"text": "and that we , so , introduce again some natural behavior in this trajectory .", "speaker": "G"}, {"text": "very different from speech .", "speaker": "B"}, {"text": "still , , it shouldn't confuse the", "speaker": "B"}, {"text": ", similar to what you see really in the real noisy situation .", "speaker": "G"}, {"text": "or in the clean situation .", "speaker": "G"}, {"text": "but but somehow natural distribution .", "speaker": "G"}, {"text": "but isn't that again the idea of the additive thing ,", "speaker": "D"}, {"text": "if it as we had in the ?", "speaker": "D"}, {"text": "if you have random data , , in the time domain , then when you look at the spectrum it 's gonna be pretty flat .", "speaker": "D"}, {"text": "so just add something everywhere rather than just in those places .", "speaker": "D"}, {"text": "it 's just constant , ?", "speaker": "D"}, {"text": "it 's it 's just especially in these segments ,", "speaker": "G"}, {"text": "you introduce , , very artificial behavior .", "speaker": "G"}, {"text": "see if you add something everywhere , it has almost no effect up up on top .", "speaker": "D"}, {"text": "and it and it has significant effect down there .", "speaker": "D"}, {"text": "that was , the idea .", "speaker": "D"}, {"text": "the that 's true .", "speaker": "B"}, {"text": "that those regions are the for this @ @ those negative values or whatever you get .", "speaker": "B"}, {"text": "we could trit , we could think how what we could try .", "speaker": "G"}, {"text": "it was just an idea .", "speaker": "G"}, {"text": "when it 's noisy people should just speak up .", "speaker": "D"}, {"text": "if we look at the france telecom proposal , they use some noise addition .", "speaker": "E"}, {"text": "they have random number generator , ?", "speaker": "E"}, {"text": "and they add noise on the trajectory of , , the log energy only , ?", "speaker": "E"}, {"text": "- - zero and log energy also ,", "speaker": "B"}, {"text": "but how much effect it this have ,", "speaker": "E"}, {"text": "but they do that .", "speaker": "E"}, {"text": "so it it is somehow similar to what", "speaker": "G"}, {"text": "because they have log energy ,", "speaker": "E"}, {"text": "and then just generate random number .", "speaker": "E"}, {"text": "they have some mean and variance ,", "speaker": "E"}, {"text": "and they add this number to the log energy simply .", "speaker": "E"}, {"text": "the log energy , the after the clean cleaning up .", "speaker": "B"}, {"text": "so they add random noise to it .", "speaker": "B"}, {"text": "to the just the energy , or to the mel , to the mel filter ?", "speaker": "D"}, {"text": "on - only to the log energy .", "speaker": "B"}, {"text": "so it cuz , this is most interesting for the mel filters .", "speaker": "D"}, {"text": "one or the other .", "speaker": "D"}, {"text": "but but they do not apply filtering of the log energy or what", "speaker": "G"}, {"text": "no their filter is not domain .", "speaker": "B"}, {"text": "so they did filter their time signal", "speaker": "B"}, {"text": "and then what @ @", "speaker": "B"}, {"text": "and then they calculate from this , the log energy", "speaker": "G"}, {"text": "then after that it is almost the same as the baseline prop system .", "speaker": "B"}, {"text": "and then the final log energy that they that they get , that to the to that they add some random noise .", "speaker": "B"}, {"text": "but again , that 's just log energy as opposed to filter bank energy .", "speaker": "D"}, {"text": "so it 's not the mel .", "speaker": "B"}, {"text": "it 's not the mel filter bank output .", "speaker": "B"}, {"text": "these are log energy computed from the time domain signal ,", "speaker": "B"}, {"text": "not from the mel filter banks .", "speaker": "B"}, {"text": "it 's just way to decrease the importance of this particular parameter in the in the world feature vector", "speaker": "E"}, {"text": "cu if you add noise to one of the parameters , you widen the distributions", "speaker": "E"}, {"text": "the variance , , reduces ,", "speaker": "B"}, {"text": "eee - sss - .", "speaker": "E"}, {"text": "so it could reduce the dependence on the amplitude and so on .", "speaker": "D"}, {"text": "so is , is that about it ?", "speaker": "F"}, {"text": "so the other thing is the 'm just looking at little bit on the delay issue where the delay of the system is like hundred and eighty millisecond .", "speaker": "B"}, {"text": "so tried another sk system , another filter which 've like shown at the end .", "speaker": "B"}, {"text": "which is very similar to the existing , filter .", "speaker": "B"}, {"text": "only , only thing is that the phase is like nonlinear phase", "speaker": "B"}, {"text": "because it 's it 's not symmetric filter anymore .", "speaker": "B"}, {"text": "this is for the lda ?", "speaker": "F"}, {"text": "so this is like so this makes the delay like zero for lda", "speaker": "B"}, {"text": "because it 's completely causal .", "speaker": "B"}, {"text": "so got actually just the results for the italian for that", "speaker": "B"}, {"text": "and that 's like", "speaker": "B"}, {"text": "so the fifty - one point nine has become forty - eight point six ,", "speaker": "B"}, {"text": "which is like three percent relative degradation .", "speaker": "B"}, {"text": "so have like the fifty - one point nine", "speaker": "B"}, {"text": "it fares for the other conditions .", "speaker": "B"}, {"text": "so it 's just like it 's like three percent relative degradation ,", "speaker": "B"}, {"text": "but but is there is there problem with the one hundred eighty milliseconds ?", "speaker": "G"}, {"text": "- , this is", "speaker": "D"}, {"text": ", talked to , ta , talked , , about it with hynek .", "speaker": "G"}, {"text": "so , our position is that , , we shouldn't be unduly constraining the latency at this point", "speaker": "D"}, {"text": "because we 're all still experimenting with trying to make the performance better in the presence of noise .", "speaker": "D"}, {"text": "there is minority in that group who is arguing who are arguing for , , having further constraining of the latency .", "speaker": "D"}, {"text": "so we 're just continuing to keep aware of what the trade - offs are and , , what do we gain from having longer or shorter latencies ?", "speaker": "D"}, {"text": "but since we always seem to at least get something out of longer latencies not being so constrained , we 're tending to go with that if we 're not told we can't do it .", "speaker": "D"}, {"text": "what where was the , the smallest latency of all the systems last time ?", "speaker": "F"}, {"text": "the french telecom .", "speaker": "B"}, {"text": "france telecom was was very short latency", "speaker": "D"}, {"text": "and they had very good result .", "speaker": "D"}, {"text": "what what was it ?", "speaker": "F"}, {"text": "it was thirty - five .", "speaker": "D"}, {"text": "it was in the order of thirty milliseconds", "speaker": "G"}, {"text": "thirty - four .", "speaker": "B"}, {"text": "so it 's possible to get very short latency .", "speaker": "D"}, {"text": "but , again , we 're the approaches that we 're using are ones that take advantage of", "speaker": "D"}, {"text": "was just curious about where we are compared to , , the shortest that people have done .", "speaker": "F"}, {"text": "but but this thirty milliseconds they did it did not include the delta calculation .", "speaker": "G"}, {"text": "and this is included now ,", "speaker": "G"}, {"text": "so if they include the delta , it will be an additional forty millisecond .", "speaker": "B"}, {"text": "they were not using the htk delta ?", "speaker": "G"}, {"text": "no , they 're using nine - point window ,", "speaker": "B"}, {"text": "which is like four on either side ,", "speaker": "B"}, {"text": "nine - point .", "speaker": "G"}, {"text": "they didn't include that .", "speaker": "B"}, {"text": "where does the comprish compression in decoding delay comes from ?", "speaker": "E"}, {"text": "that 's the way the the frames are packed ,", "speaker": "B"}, {"text": "like you have to for one more frame to pack .", "speaker": "B"}, {"text": "because it 's the crc is computed for two frames always .", "speaker": "B"}, {"text": "that the they would need that forty milliseconds also .", "speaker": "D"}, {"text": "they actually changed the compression scheme altogether .", "speaker": "B"}, {"text": "so they have their own compression and decoding scheme", "speaker": "B"}, {"text": "and they what they have .", "speaker": "B"}, {"text": "but they have coded zero delay for that .", "speaker": "B"}, {"text": "because they ch know they changed it ,", "speaker": "B"}, {"text": "they have their own crc ,", "speaker": "B"}, {"text": "their own error correction mechanism .", "speaker": "B"}, {"text": "so they don't have to more than one more frame to know whether the current frame is in error .", "speaker": "B"}, {"text": "so they changed the whole thing so that there 's no delay for that compression and part also .", "speaker": "B"}, {"text": "even you have reported actually zero delay for the compression .", "speaker": "B"}, {"text": "you also have some different", "speaker": "B"}, {"text": "no , used this scheme as it was before .", "speaker": "G"}, {"text": "we 've got twenty minutes", "speaker": "F"}, {"text": "so we should probably try to move along .", "speaker": "F"}, {"text": "did you wanna go next , stephane ?", "speaker": "F"}, {"text": "we have to take", "speaker": "E"}, {"text": "so you have one sheet ?", "speaker": "E"}, {"text": "this one is you don't need it ,", "speaker": "E"}, {"text": "so you have to take the whole the five .", "speaker": "E"}, {"text": "there should be five sheets .", "speaker": "E"}, {"text": "because left one with dave because was dropping one off and passing the others on .", "speaker": "D"}, {"text": "so , no ,", "speaker": "D"}, {"text": "we 're not .", "speaker": "D"}, {"text": "give me one .", "speaker": "H"}, {"text": "we need one more over here .", "speaker": "D"}, {"text": "there 's not enough for everybody .", "speaker": "E"}, {"text": "share with barry .", "speaker": "F"}, {"text": "can we look at this ?", "speaker": "E"}, {"text": "there are two figures showing actually the , mmm , , performance of the current vad .", "speaker": "E"}, {"text": "so it 's neural network based on plp parameters ,", "speaker": "E"}, {"text": "which estimate silence probabilities ,", "speaker": "E"}, {"text": "and then put median filtering on this", "speaker": "E"}, {"text": "to smooth the probabilities , ?", "speaker": "E"}, {"text": "didn't use the scheme that 's currently in the proposal", "speaker": "E"}, {"text": "because don't want to", "speaker": "E"}, {"text": "in the system we want to add like speech frame before every word and little bit of , , couple of frames after also .", "speaker": "E"}, {"text": "but to estimate the performance of the vad , we don't want to do that ,", "speaker": "E"}, {"text": "because it would artificially increase the the false alarm rate of speech detection .", "speaker": "E"}, {"text": "there is normally figure for the finnish and one for italian .", "speaker": "E"}, {"text": "and someone has two for the italian", "speaker": "E"}, {"text": "because 'm missing one figure here .", "speaker": "E"}, {"text": "so one surprising thing that we can notice first is that the speech miss rate is , higher than the false alarm rate .", "speaker": "E"}, {"text": "so so what is the lower curve and the upper curve ?", "speaker": "G"}, {"text": "there are two curves .", "speaker": "E"}, {"text": "one curve 's for the close - talking microphone , which is the lower curve .", "speaker": "E"}, {"text": "and the other one is for the distant microphone", "speaker": "E"}, {"text": "which has more noise", "speaker": "E"}, {"text": "it 's logical that it performs worse .", "speaker": "E"}, {"text": "so as was saying , the miss rate is quite important", "speaker": "E"}, {"text": "which means that we tend to label speech as silence .", "speaker": "E"}, {"text": "didn't analyze further yet ,", "speaker": "E"}, {"text": "but it 's it may be due to the fricative sounds", "speaker": "E"}, {"text": "which may be in noisy condition label labelled as silence .", "speaker": "E"}, {"text": "and it may also be due to the alignment", "speaker": "E"}, {"text": "the reference alignment .", "speaker": "E"}, {"text": "because now use an alignment obtained from system trained on channel zero .", "speaker": "E"}, {"text": "checked it little bit", "speaker": "E"}, {"text": "but there might be alignment errors .", "speaker": "E"}, {"text": "like the fact that the models tend to align their first state on silence and their last state on silence also .", "speaker": "E"}, {"text": "so the reference alignment would label as speech some silence frame before speech and after speech .", "speaker": "E"}, {"text": "this is something that we already noticed before", "speaker": "E"}, {"text": "so this cus this could also explain , , the high miss rate .", "speaker": "E"}, {"text": "and and this curves are the average over the whole database ,", "speaker": "G"}, {"text": "and the different points of the curves are for five , thresholds on the probability from point three to point seven .", "speaker": "E"}, {"text": "so the detection threshold is very", "speaker": "B"}, {"text": "there first , threshold on the probability @ @ that puts all the values to zero or one .", "speaker": "E"}, {"text": "and then the median filtering .", "speaker": "E"}, {"text": "so the median filtering is fixed .", "speaker": "B"}, {"text": "you just change the threshold ?", "speaker": "B"}, {"text": "it 's fixed ,", "speaker": "E"}, {"text": "so , going from channel zero to channel one , , almost double the error rate .", "speaker": "E"}, {"text": "so it 's reference performance that we can , if we want to work on the vad , we can work on this basis", "speaker": "E"}, {"text": "is this is this vad mlp ?", "speaker": "A"}, {"text": "how how big is it ?", "speaker": "A"}, {"text": "it 's very big one .", "speaker": "E"}, {"text": "so three hundred and fifty inputs ,", "speaker": "B"}, {"text": "six thousand hidden nodes and two outputs .", "speaker": "B"}, {"text": "middle - sized one .", "speaker": "D"}, {"text": "you have questions about that , or suggestions ?", "speaker": "E"}, {"text": "it seems the performance seems worse in finnish ,", "speaker": "E"}, {"text": "it 's not trained on finnish .", "speaker": "B"}, {"text": "it 's worse .", "speaker": "H"}, {"text": "it 's not trained on finnish ,", "speaker": "E"}, {"text": "what 's it trained on ?", "speaker": "D"}, {"text": "the mlp 's not trained on finnish .", "speaker": "B"}, {"text": "what 's it trained on ?", "speaker": "D"}, {"text": "it 's italian ti - digits .", "speaker": "B"}, {"text": "it 's trained on italian ?", "speaker": "D"}, {"text": "and also there are like funny noises on finnish", "speaker": "E"}, {"text": "more than on italian .", "speaker": "E"}, {"text": "the , it 's true .", "speaker": "B"}, {"text": "we were looking at this .", "speaker": "E"}, {"text": "but for most of the noises , noises are", "speaker": "E"}, {"text": "if we want to talk about that .", "speaker": "E"}, {"text": "the \" car \" noises are below like five hundred hertz .", "speaker": "E"}, {"text": "and we were looking at the \" music \" utterances", "speaker": "E"}, {"text": "and in this case the noise is more about two thousand hertz .", "speaker": "E"}, {"text": "the music energy 's very low .", "speaker": "E"}, {"text": "from zero to two thousand hertz .", "speaker": "E"}, {"text": "so just looking at this frequency range for from five hundred to two thousand would improve somewhat the vad", "speaker": "E"}, {"text": "so there are like some some parameters you wanted to use ?", "speaker": "B"}, {"text": "it 's there .", "speaker": "E"}, {"text": "so is the is the training is the training based on these labels files which you take as reference here ?", "speaker": "G"}, {"text": "wh - when you train the neural net you", "speaker": "G"}, {"text": "it 's not .", "speaker": "E"}, {"text": "it 's it was trained on some alignment obtained", "speaker": "E"}, {"text": "for the italian data , we trained the neural network on with embedded training .", "speaker": "E"}, {"text": "so re - estimation of the alignment using the neural network , .", "speaker": "E"}, {"text": "we actually trained , , the on the italian training part .", "speaker": "B"}, {"text": "we we had another system with", "speaker": "B"}, {"text": "so it was phonetic classification system for the italian aurora data .", "speaker": "E"}, {"text": "it must be somewhere .", "speaker": "B"}, {"text": "for the aurora data that it was trained on , it was different .", "speaker": "E"}, {"text": "like , for ti - digits you used previous system that you had , .", "speaker": "E"}, {"text": "that 's true .", "speaker": "B"}, {"text": "so the alignments from the different database that are used for training came from different system .", "speaker": "E"}, {"text": "then we put them tog together .", "speaker": "E"}, {"text": "you put them together and trained the vad on them .", "speaker": "E"}, {"text": "but did you use channel did you align channel one also ?", "speaker": "E"}, {"text": "took their entire italian training part .", "speaker": "B"}, {"text": "so it was both channel zero plus channel one .", "speaker": "B"}, {"text": "so the alignments might be wrong then on channel one , ?", "speaker": "E"}, {"text": "so we might ,", "speaker": "E"}, {"text": "we can do realignment .", "speaker": "B"}, {"text": "that 's true .", "speaker": "B"}, {"text": "at least want to retrain on these alignments ,", "speaker": "E"}, {"text": "which should be better because they come from close - talking microphone .", "speaker": "E"}, {"text": "the that was my idea .", "speaker": "G"}, {"text": "if it ha if it is not the same labeling which is taking the spaces .", "speaker": "G"}, {"text": "so the vad was trained on different set of labels for channel zero and channel one", "speaker": "B"}, {"text": "was the alignments were were different for certainly different because they were independently trained .", "speaker": "B"}, {"text": "we didn't copy the channel zero alignments to channel one .", "speaker": "B"}, {"text": "but for the new alignments what you generated , you just copied the channel zero to channel one , ?", "speaker": "B"}, {"text": "actually when we look at the vad , for some utterances it 's almost perfect ,", "speaker": "E"}, {"text": "it just dropped one frame ,", "speaker": "E"}, {"text": "the first frame of speech", "speaker": "E"}, {"text": "so there are some utterances where it 's almost one hundred percent vad performance .", "speaker": "E"}, {"text": "so the next thing is , have the spreadsheet for three different system .", "speaker": "E"}, {"text": "but for this you only have to look now on the speechdat - car performance", "speaker": "E"}, {"text": "so didn't test the spectral subtraction on ti - digits yet .", "speaker": "E"}, {"text": "so you have three she sheets .", "speaker": "E"}, {"text": "one is the proposal - one system .", "speaker": "E"}, {"text": "actually , it 's not exe exactly proposal - one .", "speaker": "E"}, {"text": "it 's the system that sunil just described .", "speaker": "E"}, {"text": "but with , wiener filtering from , france telecom included .", "speaker": "E"}, {"text": "so this gives like fifty - seven point seven percent , , , error rate reduction on the speechdat - car data .", "speaker": "E"}, {"text": "and then have two sheets where it 's for system where", "speaker": "E"}, {"text": "so it 's again the same system .", "speaker": "E"}, {"text": "but in this case we have spectral subtraction", "speaker": "E"}, {"text": "with maximum overestimation factor of two point five .", "speaker": "E"}, {"text": "there is smoothing of the gain trajectory with some , low - pass filter ,", "speaker": "E"}, {"text": "which has forty milliseconds latency .", "speaker": "E"}, {"text": "and then , after subtraction , add constant to the energies", "speaker": "E"}, {"text": "and have two cases where the first case is where the constant is twenty - five db below the mean speech energy", "speaker": "E"}, {"text": "and the other is thirty db below .", "speaker": "E"}, {"text": "and for these two system we have like fifty - five point , , five - percent improvement ,", "speaker": "E"}, {"text": "and fifty - eight point one .", "speaker": "E"}, {"text": "so again , it 's around fifty - six , fifty - seven .", "speaker": "E"}, {"text": "cuz notice the ti - digits number is exactly the same for these last two ?", "speaker": "D"}, {"text": "for the france telecom , spectral subtraction included in the our system , the ti - digits number are the one ,", "speaker": "E"}, {"text": "but not for the other system", "speaker": "E"}, {"text": "because didn't test it yet this system , including with spectral subtraction on the ti - digits data .", "speaker": "E"}, {"text": "tested it on speechdat - car .", "speaker": "E"}, {"text": "so so that means the only thing", "speaker": "D"}, {"text": "so so these numbers are simply", "speaker": "G"}, {"text": "this , we have to", "speaker": "E"}, {"text": "but this number .", "speaker": "B"}, {"text": "so you so you just should look at that fifty - eight perc point nine percent and so on .", "speaker": "D"}, {"text": "so by , by reducing the noise decent threshold like minus thirty db , it 's like , you are like reducing the floor of the noisy regions , ?", "speaker": "B"}, {"text": "the floor is lower .", "speaker": "E"}, {"text": "so when you say minus twenty - five or minus thirty db , with respect to what ?", "speaker": "D"}, {"text": "to the average , speech energy", "speaker": "E"}, {"text": "which is estimated on the world database .", "speaker": "E"}, {"text": "so you 're creating signal - to - noise ratio of twenty - five or thirty db ?", "speaker": "D"}, {"text": "but it 's not", "speaker": "E"}, {"text": "what you do is this .", "speaker": "G"}, {"text": "when when you have this , after you subtracted it , , then you get something with this , , where you set the values to zero", "speaker": "G"}, {"text": "and then you simply add an additive constant again .", "speaker": "G"}, {"text": "so you shift it somehow .", "speaker": "G"}, {"text": "this this whole curve is shifted again .", "speaker": "G"}, {"text": "but did you do that before the thresholding to zero ,", "speaker": "D"}, {"text": "but , it 's after the thresholding .", "speaker": "E"}, {"text": "so you 'd really want to do it before ,", "speaker": "D"}, {"text": "we might do it before ,", "speaker": "E"}, {"text": "because then the then you would have less of that phenomenon .", "speaker": "D"}, {"text": "but still , when you do this and you take the log after that , it reduce the variance .", "speaker": "E"}, {"text": "that will reduce the variance .", "speaker": "D"}, {"text": "that 'll help .", "speaker": "D"}, {"text": "but if you does do it before you get less of these funny - looking things he 's drawing .", "speaker": "D"}, {"text": "so before it 's like adding this , col to the exi original", "speaker": "B"}, {"text": "at the point where you 've done the subtraction .", "speaker": "D"}, {"text": "essentially you 're adding constant into everything .", "speaker": "D"}, {"text": "but the way stephane did it , it is exactly the way have implemented in the phone ,", "speaker": "G"}, {"text": "better do it different , then .", "speaker": "D"}, {"text": "just you just ta you just set it for particular signal - to - noise ratio that you want ?", "speaker": "D"}, {"text": "made similar investigations like stephane did here ,", "speaker": "G"}, {"text": "just , adding this constant and looking how dependent is it on the value of the constant", "speaker": "G"}, {"text": "and then , must choose them somehow to give on average the best results for certain range of the signal - to - noise ratios .", "speaker": "G"}, {"text": "it 's clear .", "speaker": "E"}, {"text": "should have gi given other results .", "speaker": "E"}, {"text": "also it 's clear when you don't add noise , it 's much worse .", "speaker": "E"}, {"text": "like , around five percent worse .", "speaker": "E"}, {"text": "and if you add too much noise it get worse also .", "speaker": "E"}, {"text": "and it seems that now this is constant that does not depend on anything that you can learn from the utterance .", "speaker": "E"}, {"text": "it 's just constant noise addition .", "speaker": "E"}, {"text": "then then 'm confused .", "speaker": "D"}, {"text": "you 're saying it doesn't depend on the utterance", "speaker": "D"}, {"text": "but you were adding an amount that was twenty - five db down from the signal energy .", "speaker": "D"}, {"text": "so the way did that , measured the average speech energy of the all the italian data .", "speaker": "E"}, {"text": "and then have used this as mean speech energy .", "speaker": "E"}, {"text": "it 's just constant amount over all .", "speaker": "D"}, {"text": "wha what observed is that for italian and spanish , when you go to thirty and twenty - five db , it 's good .", "speaker": "E"}, {"text": "it stays in this range ,", "speaker": "E"}, {"text": "it 's , , the", "speaker": "E"}, {"text": "the performance of the this algorithm is quite good .", "speaker": "E"}, {"text": "but for finnish , you have degradation already when you go from thirty - five to thirty", "speaker": "E"}, {"text": "and then from thirty to twenty - five .", "speaker": "E"}, {"text": "and have the feeling that it 's because just finnish has mean energy that 's lower than the other databases .", "speaker": "E"}, {"text": "and due to this the thresholds should be", "speaker": "E"}, {"text": "the the noise addition should be lower", "speaker": "E"}, {"text": "but in , in the real thing you 're not gonna be able to measure what people are doing over half an hour or an hour , or anything , ?", "speaker": "D"}, {"text": "so you have to come up with this number from something else .", "speaker": "D"}, {"text": "but you are not doing it now language dependent ?", "speaker": "G"}, {"text": "it 's not .", "speaker": "E"}, {"text": "it 's just something that 's fixed .", "speaker": "E"}, {"text": "it 's overall .", "speaker": "G"}, {"text": "but what he is doing language dependent is measuring what that number reference is that he comes down twenty - five down from .", "speaker": "D"}, {"text": "because did it started working on italian .", "speaker": "E"}, {"text": "obtained this average energy", "speaker": "E"}, {"text": "and then used this one .", "speaker": "E"}, {"text": "for all the languages .", "speaker": "B"}, {"text": "so it 's arbitrary .", "speaker": "D"}, {"text": "so the next thing is to use this as initialization", "speaker": "E"}, {"text": "and then use something on - line .", "speaker": "E"}, {"text": "something more adaptive ,", "speaker": "D"}, {"text": "but and expect improvement at least in finnish because the way", "speaker": "E"}, {"text": "for italian and spanish it 's this value works good but not necessarily for finnish .", "speaker": "E"}, {"text": "but unfortunately there is , like , this forty millisecond latency", "speaker": "E"}, {"text": "so would try to somewhat reduce this @ @ .", "speaker": "E"}, {"text": "already know that if completely remove this latency , so . , it there is three percent hit on italian .", "speaker": "E"}, {"text": "your your smoothing was @ @ , over this so to say , the factor of the wiener .", "speaker": "G"}, {"text": "and then it 's ,", "speaker": "G"}, {"text": "what was it ?", "speaker": "G"}, {"text": "this smoothing , it was over the subtraction factor , so to say .", "speaker": "G"}, {"text": "it 's smoothing over the gain of the subtraction algorithm .", "speaker": "E"}, {"text": "and and you are looking into the future , into the past .", "speaker": "G"}, {"text": "so , to smooth this thing .", "speaker": "E"}, {"text": "and did you try simply to smooth to smooth the to smooth stronger the envelope ?", "speaker": "G"}, {"text": "because , it should have similar effect if you", "speaker": "G"}, {"text": "you have now several stages of smoothing , so to say .", "speaker": "G"}, {"text": "you start up .", "speaker": "G"}, {"text": "as far as remember you smooth somehow the envelope ,", "speaker": "G"}, {"text": "you smooth somehow the noise estimate ,", "speaker": "G"}, {"text": "and later on you smooth also this subtraction factor .", "speaker": "G"}, {"text": "it 's it 's just the gain that 's smoothed actually", "speaker": "E"}, {"text": "actually do all the smoothing .", "speaker": "B"}, {"text": "but it 's smoothed", "speaker": "E"}, {"text": "it it was you .", "speaker": "G"}, {"text": "no , in this case it 's just the gain .", "speaker": "E"}, {"text": "but the way it 's done is that , for low gain , there is this non nonlinear smoothing actually .", "speaker": "E"}, {"text": "for low gains , use the smoothed sm , smoothed version", "speaker": "E"}, {"text": "but for high gain @ @ it 's don't smooth .", "speaker": "E"}, {"text": "it experience shows you , if you do the", "speaker": "G"}, {"text": "the best is to do the smoo smoothing as early as possible .", "speaker": "G"}, {"text": "so when you start up .", "speaker": "G"}, {"text": "you start up with the somehow with the noisy envelope .", "speaker": "G"}, {"text": "and , best is to smooth this somehow .", "speaker": "G"}, {"text": "could try this .", "speaker": "E"}, {"text": "so , before estimating the snr , @ @ smooth the envelope .", "speaker": "B"}, {"text": "then would need to find way to like smooth less also when there is high energy .", "speaker": "E"}, {"text": "cuz noticed that it helps little bit to like smooth more during low energy portions and less during speech ,", "speaker": "E"}, {"text": "because if you smooth then you distort the speech .", "speaker": "E"}, {"text": "you could do it in this way that you say , if you if 'm", "speaker": "G"}, {"text": "you have somehow noise estimate ,", "speaker": "G"}, {"text": "and , if you say 'm with my envelope", "speaker": "G"}, {"text": "'m close to this noise estimate ,", "speaker": "G"}, {"text": "then you have bad signal - to - noise ratio and then you would like to have stronger smoothing .", "speaker": "G"}, {"text": "so you could you could base it on your estimation of the signal - to - noise ratio on your actual", "speaker": "G"}, {"text": "or some silence probability from the vad if you have", "speaker": "B"}, {"text": "but don't trust the current vad .", "speaker": "E"}, {"text": "so not now .", "speaker": "B"}, {"text": "the vad later will be much better .", "speaker": "D"}, {"text": "so is that it ?", "speaker": "F"}, {"text": "fff that 's it .", "speaker": "E"}, {"text": "so to summarize the performance of these , speechdat - car results is similar than yours so to say .", "speaker": "G"}, {"text": "so the fifty - eight is like the be some fifty - six point", "speaker": "B"}, {"text": "you have you have fifty - six point four", "speaker": "G"}, {"text": "that 's true .", "speaker": "B"}, {"text": "and and dependent on this additive constant , it is better or worse .", "speaker": "G"}, {"text": "the condition where it 's better than your approach , it 's it just because it 's better on matched and that the weight on matched is bigger ,", "speaker": "E"}, {"text": "you caught up .", "speaker": "B"}, {"text": "that 's true .", "speaker": "B"}, {"text": "if you don't weigh differently the different condition , you can see that your , the win the two - stage wiener filtering is better", "speaker": "E"}, {"text": "it 's better for high mismatch , ?", "speaker": "E"}, {"text": "it 's better for high mismatch .", "speaker": "B"}, {"text": "but little bit worse for matched .", "speaker": "E"}, {"text": "so over all it gets , , worse for the matched condition ,", "speaker": "B"}, {"text": "so we need to combine these two .", "speaker": "F"}, {"text": "that 's that 's the best thing , is like the french telecom system is optimized for the matched condition .", "speaker": "B"}, {"text": "so they know that the weighting is good for the matched ,", "speaker": "B"}, {"text": "and so there 's", "speaker": "B"}, {"text": "everywhere the matched 's performance is very good for the french telecom .", "speaker": "B"}, {"text": "we are we may also have to do something similar @ @ .", "speaker": "B"}, {"text": "our tradition here has always been to focus on the mismatched .", "speaker": "D"}, {"text": "cuz it 's more interesting .", "speaker": "D"}, {"text": "mu - my mine was it too , .", "speaker": "G"}, {"text": "before started working on this aurora .", "speaker": "G"}, {"text": "only say that the this is , summary of the of all the vts experiments", "speaker": "H"}, {"text": "and say that the result in the last , for italian the last experiment for italian , are bad .", "speaker": "H"}, {"text": "make mistake when write .", "speaker": "H"}, {"text": "up at copy one of the bad result .", "speaker": "H"}, {"text": "there . , this .", "speaker": "H"}, {"text": "if we put everything , we improve lot the spectral use of the vts", "speaker": "H"}, {"text": "but the final result are not still mmm , good like the wiener filter .", "speaker": "H"}, {"text": "it 's @ @ it 's possible to have the same result .", "speaker": "H"}, {"text": "because have , mmm , worse result in medium mismatch and high mismatch .", "speaker": "H"}, {"text": "you you have better", "speaker": "B"}, {"text": "you have some results that are good for the high mismatch .", "speaker": "B"}, {"text": "someti are more or less similar but are worse .", "speaker": "H"}, {"text": "still don't have the result for ti - digits .", "speaker": "H"}, {"text": "the program is training .", "speaker": "H"}, {"text": "for this weekend will have result ti - digits", "speaker": "H"}, {"text": "and complete that like this .", "speaker": "H"}, {"text": "one thing that note are not here in this result but are speak are spoken before with sunil improve my result using clean lda filter .", "speaker": "H"}, {"text": "if use , , the lda filter that are training with the noisy speech , that hurts the res my results .", "speaker": "H"}, {"text": "so what are these numbers here ?", "speaker": "D"}, {"text": "are these with the clean or with the noisy ?", "speaker": "D"}, {"text": "this is with the clean .", "speaker": "H"}, {"text": "with the noise have worse result , that if doesn't use it .", "speaker": "H"}, {"text": "but that may be because with this technique we are using really clean speech .", "speaker": "H"}, {"text": "the speech the representation that go to the htk is really clean speech", "speaker": "H"}, {"text": "because it 's from the dictionary ,", "speaker": "H"}, {"text": "and from that .", "speaker": "H"}, {"text": "because that you did some experiments using the two the two lda filter , clean and noi and noise ,", "speaker": "H"}, {"text": "and it doesn't matter too much .", "speaker": "H"}, {"text": "but it doesn't matter on speechdat - car ,", "speaker": "E"}, {"text": "but , it matters , , lot on ti - digits .", "speaker": "E"}, {"text": "using the clean filter .", "speaker": "B"}, {"text": "it 's better to use clean .", "speaker": "H"}, {"text": "it 's much better when you we used the clean derived lda filter .", "speaker": "E"}, {"text": "you can do also this .", "speaker": "H"}, {"text": "to use clean speech .", "speaker": "H"}, {"text": "sunil in your result it 's", "speaker": "E"}, {"text": "'ll try the cle", "speaker": "B"}, {"text": "no , my result is with the noisy lda .", "speaker": "B"}, {"text": "it 's with the noisy one .", "speaker": "E"}, {"text": "it 's with the noisy .", "speaker": "B"}, {"text": "it 's it 's not the clean lda .", "speaker": "B"}, {"text": "it 's in in the front sheet , have like the summary .", "speaker": "B"}, {"text": "and and your result is with the", "speaker": "D"}, {"text": "it 's with the clean lda .", "speaker": "E"}, {"text": "this is your results are all with the clean lda result ?", "speaker": "B"}, {"text": "with the clean lda .", "speaker": "H"}, {"text": "and in your case it 's all noisy ,", "speaker": "E"}, {"text": "is that the reason ?", "speaker": "H"}, {"text": "but observe my case it 's in , , at least on speechdat - car it doesn't matter", "speaker": "E"}, {"text": "but ti - digits it 's like two or three percent absolute , , better .", "speaker": "E"}, {"text": "on ti - digits this matters .", "speaker": "B"}, {"text": "so you really might wanna try the clean .", "speaker": "D"}, {"text": "will have to look at it .", "speaker": "B"}, {"text": "that 's true .", "speaker": "B"}, {"text": "that could be sizeable there .", "speaker": "D"}, {"text": "and this is everything .", "speaker": "H"}, {"text": "you are leaving in about two weeks carmen .", "speaker": "G"}, {"text": "so , if if would put it put on the head of project mana manager would say , , there is not so much time left now .", "speaker": "G"}, {"text": "be my guest .", "speaker": "D"}, {"text": "what would do is would pick @ @ the best consolation , which you think ,", "speaker": "G"}, {"text": "and create all the results for the whole database that you get to the final number as sunil did it", "speaker": "G"}, {"text": "and prepare at the", "speaker": "H"}, {"text": "and also to write somehow document where you describe your approach , and what you have done .", "speaker": "G"}, {"text": "was thinking to do that next week .", "speaker": "H"}, {"text": "'ll 'll borrow the head back and agree .", "speaker": "D"}, {"text": "wi will do that next week .", "speaker": "H"}, {"text": "that 's that 's", "speaker": "D"}, {"text": "actually the , the spanish government , , requires that anyway .", "speaker": "D"}, {"text": "they want some report from everybody who 's in the program .", "speaker": "D"}, {"text": "and 'd we 'd like to see it too .", "speaker": "D"}, {"text": "what 's do you think we , , should do the digits or skip it ?", "speaker": "F"}, {"text": "or what are what do you think ?", "speaker": "F"}, {"text": "we have them now ?", "speaker": "D"}, {"text": "why don why don't we do it ?", "speaker": "D"}, {"text": "just just take minute .", "speaker": "D"}, {"text": "would you pass those down ?", "speaker": "F"}, {"text": "so 'll go ahead .", "speaker": "F"}, {"text": "is it the channel , or the mike ?", "speaker": "E"}, {"text": "it 's the mike ?", "speaker": "E"}, {"text": "it 's not four .", "speaker": "E"}, {"text": "this is date and time .", "speaker": "H"}, {"text": "on the channel , channel .", "speaker": "H"}, {"text": "what is this ?", "speaker": "G"}, {"text": "if you could just leave , , your mike on top of your , , digit form", "speaker": "F"}, {"text": "fill in any information that 's missing .", "speaker": "F"}, {"text": "didn't get chance to fill them out ahead of time .", "speaker": "F"}, {"text": "we 're gonna have to fix that .", "speaker": "F"}, {"text": "let 's see ,", "speaker": "F"}, {"text": "it starts with one here , and then goes around and ends with nine here .", "speaker": "F"}, {"text": "so 'm eight ,", "speaker": "A"}, {"text": "so he 's eight ,", "speaker": "F"}, {"text": "you 're seven .", "speaker": "A"}, {"text": "you 're seven ,", "speaker": "F"}], "id": "xxxxx", "relations": [{"y": 1, "x": 0, "type": "Explanation"}]}]