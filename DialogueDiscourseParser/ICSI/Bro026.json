[{"edus": [{"text": "so we we had meeting with , with hynek , , in which , , sunil and stephane , summarized where they were and , , talked about where we were gonna go .", "speaker": "B"}, {"text": "so that happened mid - week .", "speaker": "B"}, {"text": "did you get your code pushed together ?", "speaker": "E"}, {"text": "it 's it 's it was updated yesterday ,", "speaker": "D"}, {"text": "you probably received the mail .", "speaker": "A"}, {"text": ", saw saw the note .", "speaker": "E"}, {"text": "what was the update ?", "speaker": "B"}, {"text": "what was the update ?", "speaker": "A"}, {"text": "so there is then the all the new features that go in .", "speaker": "A"}, {"text": "the , , noise suppression , the re - synthesis of speech after suppression .", "speaker": "A"}, {"text": "is the , the cvs mechanism working ?", "speaker": "E"}, {"text": "are are people , , up at ogi grabbing code , via that ?", "speaker": "E"}, {"text": "if they use it ,", "speaker": "A"}, {"text": "don't think anybody up there is like working on it now .", "speaker": "D"}, {"text": "it more likely that what it means is that when sunil is up there he will grab it .", "speaker": "B"}, {"text": "so now nobody 's working on aurora there .", "speaker": "D"}, {"text": "they 're working on different task .", "speaker": "B"}, {"text": "but what 'll happen is he 'll go back up there and , , pratibha will come back from , , the east coast .", "speaker": "B"}, {"text": "and , and actually , , after eurospeech for little bit , , he 'll go up there too .", "speaker": "B"}, {"text": "so , actually everybody who 's working on it will be up there for at least little while .", "speaker": "B"}, {"text": "so they 'll remotely access it from there .", "speaker": "B"}, {"text": "so has has anybody tried remotely accessing the cvs using , , ssh ?", "speaker": "E"}, {"text": "if hari did that or you", "speaker": "A"}, {"text": "today . , just log into", "speaker": "D"}, {"text": "have you tried it yet ?", "speaker": "E"}, {"text": "no , didn't .", "speaker": "D"}, {"text": "so 'll try it today .", "speaker": "D"}, {"text": "actually tried wh while when installed the repository , tried from belgium .", "speaker": "A"}, {"text": "logged in there and tried to import", "speaker": "A"}, {"text": "it worked good ?", "speaker": "E"}, {"text": "but it 's so , now it 's the mechanism with ssh .", "speaker": "A"}, {"text": "don't didn't set up you can also set up cvs server on new port .", "speaker": "A"}, {"text": "it 's like , main server , or you can do cvs server .", "speaker": "A"}, {"text": "then that 's using the cvs password mechanism and all that ,", "speaker": "E"}, {"text": "but didn't do that because was not about security problems .", "speaker": "A"}, {"text": "so when you came in from belgian belgium , using ssh , , was it asking you for your own password into icsi ?", "speaker": "E"}, {"text": "so if yo you can only do that if you have an account at icsi ?", "speaker": "E"}, {"text": "cuz there is an way to set up anonymous cvs", "speaker": "E"}, {"text": "you ha in this way you ca you have to set up cvs server but then , , you can access it .", "speaker": "A"}, {"text": "you can set up priorities .", "speaker": "A"}, {"text": "so the anonymous mechanism", "speaker": "E"}, {"text": "you can access them and mostly if you if the set the server is set up like this .", "speaker": "A"}, {"text": "because lot of the open source works with anonymous cvs", "speaker": "E"}, {"text": "and 'm just wondering , , for our transcripts we may want to do that .", "speaker": "E"}, {"text": "for this don't think we 're quite up to that .", "speaker": "B"}, {"text": "we 're still so much in development .", "speaker": "B"}, {"text": "we want to have just the insiders .", "speaker": "B"}, {"text": "wasn't suggesting for this .", "speaker": "E"}, {"text": "'m thinking of the meeting recorder", "speaker": "E"}, {"text": "what 's new ?", "speaker": "E"}, {"text": ", the thing to me might be me 'm you 've just been working on , , details of that since the meeting ,", "speaker": "B"}, {"text": "mmm , since the meeting ,", "speaker": "A"}, {"text": "'ve been 've been train training new vad and new feature net .", "speaker": "A"}, {"text": "that was that was tuesday .", "speaker": "B"}, {"text": "so they should be ready .", "speaker": "A"}, {"text": "but the thing since you weren't yo you weren't at that meeting , might be just to , , recap , , the conclusions of the meeting .", "speaker": "B"}, {"text": "you 're talking about the meeting with hynek ?", "speaker": "E"}, {"text": "cuz that was , we 'd been working up to that , , he would come here this week and we would", "speaker": "B"}, {"text": "since he 's going out of town like now , and 'm going out town in couple weeks , , and time is marching , , given all the mu many wonderful things we could be working on , what will we actually focus on ?", "speaker": "B"}, {"text": "and , and what do we freeze ?", "speaker": "B"}, {"text": "and , , what do we ?", "speaker": "B"}, {"text": "this software that these created was certainly key part .", "speaker": "B"}, {"text": "so then there 's something central", "speaker": "B"}, {"text": "and there aren't at least bunch of different versions going off in ways that differ trivially .", "speaker": "B"}, {"text": ", and , ,", "speaker": "B"}, {"text": "that 's that 's .", "speaker": "E"}, {"text": "and then within that , the idea was to freeze certain set of options for now , to run it , , particular way , and decide on what things are gonna be experimented with , as opposed to just experimenting with everything .", "speaker": "B"}, {"text": "so keep certain set of things constant .", "speaker": "B"}, {"text": "describe roughly what we are keeping constant for now ,", "speaker": "B"}, {"text": "so we 've been working like six weeks on the noise compensation and we end up with something that seems reasonable .", "speaker": "A"}, {"text": "are you gonna use which of the two techniques ?", "speaker": "E"}, {"text": "so finally it 's it 's , , wiener filtering on fft bins .", "speaker": "A"}, {"text": "and it uses , , two steps , smoothing of the transfer function ,", "speaker": "A"}, {"text": "the first step , that 's along time , which use recursion .", "speaker": "A"}, {"text": "and after this step there is further smoothing along frequency , which use sliding window of twenty fft bins .", "speaker": "A"}, {"text": "so this is on the , before any mel scaling has been done ?", "speaker": "E"}, {"text": "this this smoothing is done on the estimate , , of what you 're going to subtract ? or on the thing that has already had something subtracted ?", "speaker": "B"}, {"text": "it 's on the transfer function .", "speaker": "A"}, {"text": "it 's on the transfer function for the wiener filter .", "speaker": "B"}, {"text": "so we tried different configuration within this idea .", "speaker": "A"}, {"text": "we tried applying this on mel bands , having spectral subtraction instead of wiener filtering .", "speaker": "A"}, {"text": "finally we end up with this configuration that works , , quite .", "speaker": "A"}, {"text": "so we are going to fix this for the moment and work on the other aspects of the whole system .", "speaker": "A"}, {"text": "actually , let me int , dave isn't here to talk about it , but let me just interject .", "speaker": "B"}, {"text": "this module , in principle , , you would know whether it 's true , is somewhat independent from the rest of it .", "speaker": "B"}, {"text": "because you re - synthesize speech ,", "speaker": "B"}, {"text": "you don't you don't re - synthesize speech , but you could", "speaker": "B"}, {"text": "we we do not fo", "speaker": "A"}, {"text": "but you could .", "speaker": "B"}, {"text": "we do , but we don't don't re - synthesize .", "speaker": "A"}, {"text": "in in the program we don't re - synthesize and then re - analyze once again .", "speaker": "A"}, {"text": "we just use the clean fft bins .", "speaker": "A"}, {"text": "but you have re - synthesized thing that you that 's an option here .", "speaker": "B"}, {"text": "this is an option that then you can", "speaker": "A"}, {"text": "gu my point is that , , in some of the work he 's doing in reverberation , one of the things that we 're finding is that , , it 's for the for an artificial situation , we can just deal with the reverberation and his techniques work really .", "speaker": "B"}, {"text": "but for the real situation , problem is , is that you don't just have reverberation , you have reverberation in noise .", "speaker": "B"}, {"text": "and if you don't include that in the model , it doesn't work very .", "speaker": "B"}, {"text": "so it might be very thing to do , to just take the noise removal part of it and put that in front of what he 's looking at . and , , generate new files or whatever , and , and then do the reverberation part .", "speaker": "B"}, {"text": "so dave hasn't tried that yet ?", "speaker": "E"}, {"text": "no , no . he 's ,", "speaker": "B"}, {"text": "he 's busy with", "speaker": "E"}, {"text": "pre - prelim hell .", "speaker": "C"}, {"text": "but , , that 'll", "speaker": "B"}, {"text": "it 's clear that we , we are not with the real case that we 're looking at , we can't just look at reverberation in isolation", "speaker": "B"}, {"text": "because the interaction between that and noise is considerable .", "speaker": "B"}, {"text": "and that 's , in the past we 've looked at , , and this is hard enough , the interaction between channel effects and , and additive noise , , so convolutional effects and additive effects .", "speaker": "B"}, {"text": "and that 's hard enough .", "speaker": "B"}, {"text": "don't think we really", "speaker": "B"}, {"text": "we 're trying to deal with that .", "speaker": "B"}, {"text": "in sense that 's what we 're trying to deal with in this aurora task .", "speaker": "B"}, {"text": "and we have , , the , , lda that in principle is doing something about convolutional effects .", "speaker": "B"}, {"text": "and we have the noise suppression that 's doing something about noise .", "speaker": "B"}, {"text": "even that 's hard enough .", "speaker": "B"}, {"text": "and and the on - line normalization as , in that category .", "speaker": "B"}, {"text": "there 's all these interactions between these two and that 's part of why these had to work so hard on juggling everything around .", "speaker": "B"}, {"text": "but now when you throw in the reverberation , it 's even worse ,", "speaker": "B"}, {"text": "because not only do you have these effects , but you also have some long time effects .", "speaker": "B"}, {"text": "and , , so dave has something which , , is doing some things under some conditions with long time effects", "speaker": "B"}, {"text": "but when it 's when there 's noise there too , it 's it 's pretty hard .", "speaker": "B"}, {"text": "so we have to start", "speaker": "B"}, {"text": "since any almost any real situation is gonna have , where you have the microphone distant , is going to have both things ,", "speaker": "B"}, {"text": "we actually have to think about both at the same time .", "speaker": "B"}, {"text": "so there 's this noise suppression thing , which is worked out", "speaker": "B"}, {"text": "and then , , you should just continue telling what else is in the form we have .", "speaker": "B"}, {"text": ", the , , the other parts of the system are the blocks that were already present before and that we did not modify lot .", "speaker": "A"}, {"text": "so that 's again , that 's the wiener filtering , followed by , , that 's done at the fft level .", "speaker": "B"}, {"text": "then the mel filter bank ,", "speaker": "A"}, {"text": "then the log operation ,", "speaker": "A"}, {"text": "the the filtering is done in the frequency domain ?", "speaker": "B"}, {"text": "and then the mel and then the log , and then the", "speaker": "B"}, {"text": "then the lda filter ,", "speaker": "A"}, {"text": "mmm , then the downsampling ,", "speaker": "A"}, {"text": "then , , on - line normalization ,", "speaker": "A"}, {"text": "on - line norm ,", "speaker": "B"}, {"text": "followed by upsampling .", "speaker": "A"}, {"text": "then finally , we compute delta and we put the neural network also .", "speaker": "A"}, {"text": "and then in parallel with an neural net . and then following neural net , some probably some orthogonalization .", "speaker": "B"}, {"text": "and finally frame dropping , which , would be neural network also , used for estimated silence probabilities .", "speaker": "A"}, {"text": "and the input of this neural network would be somewhere between log mel bands or one of the earlier stages of the processing .", "speaker": "A"}, {"text": "so that 's most of this is , is operating parallel with this other .", "speaker": "B"}, {"text": "so the things that we , , ,", "speaker": "B"}, {"text": "there 's there 's some , , neat ideas for", "speaker": "B"}, {"text": "so , , in", "speaker": "B"}, {"text": "there 's like there 's bunch of tuning things to improve .", "speaker": "B"}, {"text": "there 's questions about various places where there 's an exponent , if it 's the exponent , or ways that we 're estimating noise , that we can improve estimating noise .", "speaker": "B"}, {"text": "and there 's gonna be host of those .", "speaker": "B"}, {"text": "but structurally it seemed like the things the main things that we brought up that , , are gonna need to get worked on are , , significantly better vad , , putting the neural net on , , which , , we haven't been doing anything with , the , , neural net at the end there , and , , the , , opening up the second front .", "speaker": "B"}, {"text": "the other half of the channel ?", "speaker": "E"}, {"text": ", , cuz we have we have , , half the , , data rate that they allow .", "speaker": "B"}, {"text": "that what you mean ?", "speaker": "E"}, {"text": "and , , so the initial thing which came from , , the meeting that we had down south was , , that , , we 'll initially just put in mel spectrum as the second one .", "speaker": "B"}, {"text": "it 's , , cheap , easy .", "speaker": "B"}, {"text": "there 's question about exactly how we do it .", "speaker": "B"}, {"text": "we probably will go to something better later ,", "speaker": "B"}, {"text": "but the initial thing is that cepstra and spectra behave differently ,", "speaker": "B"}, {"text": "tony robinson used to do was saying this before . he used to do mel , , spectra and mel cepstra .", "speaker": "B"}, {"text": "he used them as alternate features .", "speaker": "B"}, {"text": "put them together .", "speaker": "B"}, {"text": "so if you took the system the way it is now , the way it 's fro you 're gonna freeze it , and it ran it on the last evaluation , where it would it be ?", "speaker": "E"}, {"text": "in terms of ranking ?", "speaker": "E"}, {"text": "ri - now it 's second .", "speaker": "A"}, {"text": "although , you haven't tested it actually on the german and danish ,", "speaker": "B"}, {"text": "no , we didn't .", "speaker": "A"}, {"text": "so on the ones that you did test it on it would have been second ?", "speaker": "E"}, {"text": "when you 're saying second , you 're comparing to the numbers that the , that the best system before got on , also without german and danish ?", "speaker": "B"}, {"text": "and the ranking actually didn't change after the german and danish .", "speaker": "D"}, {"text": "ranking didn't before , but 'm just asking where this is to where theirs was without the german and danish ,", "speaker": "B"}, {"text": "where where were we actually on the last test ?", "speaker": "E"}, {"text": "we were also esp essentially second , although there were , we had couple systems and they had couple systems .", "speaker": "B"}, {"text": "and so , by that we were third ,", "speaker": "B"}, {"text": "but , there were two systems that were pretty close , that came from the same place .", "speaker": "B"}, {"text": "so institutionally we were we were second , with , , the third system .", "speaker": "B"}, {"text": "we 're so this second that you 're saying now is system - wide second ?", "speaker": "E"}, {"text": "no it 's also institutional ,", "speaker": "B"}, {"text": "still institutionally second ?", "speaker": "E"}, {"text": "both of their systems probably", "speaker": "B"}, {"text": "we are between their two systems .", "speaker": "A"}, {"text": "it is triumph .", "speaker": "A"}, {"text": "their their first system is fifty - four point something .", "speaker": "D"}, {"text": "and , , we are fifty - three point something .", "speaker": "D"}, {"text": "but everything is within the range of one percent .", "speaker": "A"}, {"text": "and their second system is also fifty - three point something .", "speaker": "D"}, {"text": "so they 're all they 're all pretty close .", "speaker": "B"}, {"text": "that 's very close .", "speaker": "E"}, {"text": "and and , , , in some sense we 're all doing fairly similar things .", "speaker": "B"}, {"text": ", one could argue about the lda and", "speaker": "B"}, {"text": "but , , in lot of ways we 're doing very similar things .", "speaker": "B"}, {"text": "so how did they fill up this all these bits ?", "speaker": "E"}, {"text": "why are we using half ?", "speaker": "B"}, {"text": "so you could you", "speaker": "B"}, {"text": "or how are they using more than half , is what", "speaker": "E"}, {"text": "so , you are closer to it than me , so correct me if 'm wrong , but that what 's going on is that in both cases , some normalization is done to deal with convola convolutional effects .", "speaker": "B"}, {"text": "they have some cepstral modification ,", "speaker": "B"}, {"text": "in our case we have couple things .", "speaker": "B"}, {"text": "we have the on - line normalization and then we have the lda rasta .", "speaker": "B"}, {"text": "and they seem to comple complement each other enough and be different enough that they both seem to help us .", "speaker": "B"}, {"text": "but in any event , they 're both doing the same thing .", "speaker": "B"}, {"text": "but there 's one difference .", "speaker": "B"}, {"text": "the lda rasta , , throws away high modulation frequencies .", "speaker": "B"}, {"text": "and they 're not doing that .", "speaker": "B"}, {"text": "so that if you throw away high modulation frequencies , then you can downsample .", "speaker": "B"}, {"text": "so what if you didn't so do you explicitly downsample then ?", "speaker": "E"}, {"text": "do we explicitly downsample ?", "speaker": "E"}, {"text": "and what if we didn't do that ?", "speaker": "E"}, {"text": "would we get worse performance ?", "speaker": "E"}, {"text": "not better , not worse .", "speaker": "A"}, {"text": "it doesn't affect it ,", "speaker": "B"}, {"text": "so , since we 're not evidently throwing away useful information , let 's try to put in some useful information .", "speaker": "B"}, {"text": "and , , so , we 've found in lot of ways for quite while that having second stream , helps lot .", "speaker": "B"}, {"text": "so that 's that 's put in , and , it may even end up with mel spectrum even though 'm saying we could do much better , just because it 's simple .", "speaker": "B"}, {"text": "and , in the long run having something everybody will look at and say , \" , , understand \" , is very helpful .", "speaker": "B"}, {"text": "so you would you 're you 're thinking to put the , , mel spectrum in before any of the noise removal ? or after ?", "speaker": "E"}, {"text": "that 's question .", "speaker": "B"}, {"text": "we were talking about that .", "speaker": "B"}, {"text": "it looks like it 'd be straightforward to , , remove the noise ,", "speaker": "B"}, {"text": "cuz that happens before the mel conversion ,", "speaker": "E"}, {"text": "so , , to do it after the mel conversion , after the noise removal , after the mel conversion .", "speaker": "B"}, {"text": "there 's even question in my mind anyhow of whether you should take the log or not .", "speaker": "B"}, {"text": "what about norm normalizing also ?", "speaker": "A"}, {"text": "but normalizing spectra instead of cepstra ?", "speaker": "B"}, {"text": "some kind would be good .", "speaker": "B"}, {"text": "it so it actually makes it dependent on the overall energy of the , the frame .", "speaker": "D"}, {"text": "if you do or don't normalize ?", "speaker": "B"}, {"text": "if yo if you don't normalize and if you don't normalize .", "speaker": "D"}, {"text": "yes , so , one would think that you would want to normalize .", "speaker": "B"}, {"text": "my thought is , , particularly if you take the log , try it .", "speaker": "B"}, {"text": "and then if normalization helps , then you have something to compare against , and say , \" , this much effect \" , you don't want to change six things and then see what happens .", "speaker": "B"}, {"text": "you want to change them one at time .", "speaker": "B"}, {"text": "so adding this other stream in , that 's simple in some way .", "speaker": "B"}, {"text": "and then saying , particularly because we 've found in the past there 's all these these different results you get with slight modifications of how you do normalization .", "speaker": "B"}, {"text": "normalization 's very tricky , sensitive thing and you learn lot .", "speaker": "B"}, {"text": "so , would think you would wanna have some baseline that says , \" , we don't normalize , this is what we get \" , when we do this normalization , when we do that normalization .", "speaker": "B"}, {"text": "but but the other question is", "speaker": "B"}, {"text": "so ultimately we 'll wind up doing some normalization .", "speaker": "B"}, {"text": "so this second stream , will it add latency to the system", "speaker": "E"}, {"text": "no , it 's in parallel .", "speaker": "B"}, {"text": "we 're not talking about computation time here .", "speaker": "B"}, {"text": "we 're ta we 're pretty far out .", "speaker": "B"}, {"text": "so it 's just in terms of what data it 's depending on .", "speaker": "B"}, {"text": "it 's depending on the same data as the other .", "speaker": "B"}, {"text": "so it 's in parallel .", "speaker": "B"}, {"text": "so with this , , new stream would you train up vad on both features , somehow ?", "speaker": "C"}, {"text": "no , the vad has its own set of features .", "speaker": "D"}, {"text": "which could be this one of these streams , or it can be something derived from these streams .", "speaker": "D"}, {"text": "and there is also the idea of using traps , , for the vad ,", "speaker": "A"}, {"text": "pratibha showed , when , she was at ibm , that it 's good idea .", "speaker": "A"}, {"text": "would would that fit on the handset ,", "speaker": "C"}, {"text": "have no idea .", "speaker": "A"}, {"text": "it would have to fit", "speaker": "A"}, {"text": "if it has to fit the delays and all this .", "speaker": "D"}, {"text": "there 's the delays and the storage ,", "speaker": "B"}, {"text": "but don't think the storage is so big for that .", "speaker": "B"}, {"text": "the biggest we 've run into for storage is the neural net .", "speaker": "B"}, {"text": "and so the issue there is , are we are we using neural - net - based traps ,", "speaker": "B"}, {"text": "and how big are they ?", "speaker": "B"}, {"text": "so that 'll that 'll be , , an issue .", "speaker": "B"}, {"text": "they can be little ones .", "speaker": "B"}, {"text": "mini - traps .", "speaker": "B"}, {"text": "cuz she also does the , the correlation - based , , traps , with without the neural net , just looking at the correlation between", "speaker": "C"}, {"text": "and for vad they would be .", "speaker": "B"}, {"text": "that 's true .", "speaker": "B"}, {"text": "or simple neural net ,", "speaker": "B"}, {"text": ", if you 're doing correlation , you 're just doing simple , dot product , , with some weights which you happened to learn from this learn from the data .", "speaker": "B"}, {"text": "putting nonlinearity on it is , , not that big deal .", "speaker": "B"}, {"text": "it certainly doesn't take much space .", "speaker": "B"}, {"text": "so , , the question is , how complex function do you need ?", "speaker": "B"}, {"text": "do you need to have an added layer ?", "speaker": "B"}, {"text": "in which case , , potentially , , it could be big .", "speaker": "B"}, {"text": "so what 's next ?", "speaker": "B"}, {"text": "so the meeting with hynek that you just had was to decide exactly what you were gonna freeze in this system ?", "speaker": "E"}, {"text": "or was there ?", "speaker": "E"}, {"text": "were you talking about what new ,", "speaker": "E"}, {"text": "what to freeze and then what to do after we froze .", "speaker": "B"}, {"text": "and like was saying , the , the basic directions are , , there 's lots of little things , such as improve the noise estimator but the bigger things are adding on the neural net and , , the second stream . and then , , improving the vad .", "speaker": "B"}, {"text": "so , 'll , 'll actually after the meeting 'll add the second stream to the vad and 'll start with the feature net in that case .", "speaker": "D"}, {"text": "it 's like , you 're looking at the vad ,", "speaker": "D"}, {"text": "'ve new feature net ready also .", "speaker": "A"}, {"text": "for the vad ?", "speaker": "D"}, {"text": "no , . two network , one vad and one feature net .", "speaker": "A"}, {"text": "you already have it ?", "speaker": "D"}, {"text": "so just figure how to take the features from the final", "speaker": "D"}, {"text": "but , , there are plenty of issues to work on for the feature net @ @ .", "speaker": "A"}, {"text": "what about the , , the new part of the evaluation ,", "speaker": "E"}, {"text": "the , , wall street journal part ?", "speaker": "E"}, {"text": "have you ever ?", "speaker": "B"}, {"text": "very good question .", "speaker": "B"}, {"text": "have you ever worked with the mississippi state , software ?", "speaker": "B"}, {"text": "you may be called upon to help , , on account of , , all the work in this here has been , , with small vocabulary .", "speaker": "B"}, {"text": "so what how is the , , interaction supposed to happen ?", "speaker": "E"}, {"text": "remember the last time we talked about this , it was up in the air whether they were going to be taking , , people 's features and then running them or they were gonna give the system out or", "speaker": "E"}, {"text": "so they 're gonna just deliver system .", "speaker": "E"}, {"text": "do we already have it ?", "speaker": "B"}, {"text": "it 's almost ready .", "speaker": "D"}, {"text": "so they have released their , , document , describing the system .", "speaker": "D"}, {"text": "you could , , point it at chuck ,", "speaker": "B"}, {"text": "so we 'll have to grab this over cvs ?", "speaker": "E"}, {"text": "it - no , it 's just downloadable from their from their web site .", "speaker": "D"}, {"text": "is that how they do it ?", "speaker": "E"}, {"text": "cuz one of the things that might be helpful , if you 've if you 've got time in all of this is , is if these are really focusing on improving , , all the digit , , and you got the front - end from them , you could do the runs for the", "speaker": "B"}, {"text": "and , , iron out hassles that you have to , , tweak joe about or whatever ,", "speaker": "B"}, {"text": "because you 're more experienced with running the large vocabulary .", "speaker": "B"}, {"text": "so 'll point you to the web site and the mails corresponding .", "speaker": "D"}, {"text": "and it but it 's not ready yet , the system ?", "speaker": "E"}, {"text": "they are still , , tuning something on that .", "speaker": "D"}, {"text": "so they 're like , they 're varying different parameters like the insertion penalty and other , and then seeing what 's the performance .", "speaker": "D"}, {"text": "are those going to be parameters that are frozen , nobody can change ?", "speaker": "E"}, {"text": "there is , , time during which people are gonna make suggestions .", "speaker": "D"}, {"text": "but everybody 's gonna have to use the same values .", "speaker": "E"}, {"text": "so these sugges these this , , period during which people are gonna make suggestions is to know whether it is actually biased towards any set of features or", "speaker": "D"}, {"text": "so certainly the thing that would want to know about is whether we get really hurt , , on in insertion penalty , language model , scaling , sorts of things .", "speaker": "B"}, {"text": "using our features .", "speaker": "E"}, {"text": "in which case , , hari or hynek will need to , , push the case more about this .", "speaker": "B"}, {"text": "and we may be able to revisit this idea about , , somehow modifying our features to work with", "speaker": "E"}, {"text": "yes . in this case ,", "speaker": "B"}, {"text": "some of that may be , , last minute rush thing because if the if our features are changing", "speaker": "B"}, {"text": "the other thing is that even though it 's months away , , it 's starting to seem to me now like november fifteenth is around the corner .", "speaker": "B"}, {"text": "and , , if they haven't decided things like this , like what the parameters are gonna be for this , , when \" deciding \" is not just somebody deciding . , there should be some understanding behind the , , deciding , which means some experiments and . it it seems pretty tight to me .", "speaker": "B"}, {"text": "so wha what 's the significance of november fifteenth ?", "speaker": "E"}, {"text": "that 's when the evaluation is .", "speaker": "B"}, {"text": "so , , so", "speaker": "B"}, {"text": "after but , , they may even decide in the end to push it off .", "speaker": "B"}, {"text": "it wouldn't , , entirely surprise me .", "speaker": "B"}, {"text": "but , , due to other reasons , like some people are going away , 'm 'm hoping it 's not pushed off for long while .", "speaker": "B"}, {"text": "that would be , put us in an awkward position .", "speaker": "B"}, {"text": "that 'll be helpful .", "speaker": "B"}, {"text": "there 's there 's not anybody ogi currently who 's who 's , , working with this", "speaker": "B"}, {"text": "is is this part of the evaluation just small part ,", "speaker": "E"}, {"text": "or ho how important is this to the overall ?", "speaker": "E"}, {"text": "it 's it 's , it depends how badly you do .", "speaker": "B"}, {"text": "that it is .", "speaker": "B"}, {"text": "this is one of those things that will be debated afterwards ?", "speaker": "E"}, {"text": ", it 's conceptually , it my impression , again , you correct me if 'm wrong , but my impression is that , , they want it as double check .", "speaker": "B"}, {"text": "that you haven't come across you haven't invented features which are actually gonna do badly for significantly different task , particularly one with larger vocabulary .", "speaker": "B"}, {"text": "and , , but it 's not the main emphasis .", "speaker": "B"}, {"text": "the truth is , most of the applications they 're looking at are pretty small vocabulary .", "speaker": "B"}, {"text": "so it 's it 's double check .", "speaker": "B"}, {"text": "so they 'll probably assign it some low weight .", "speaker": "B"}, {"text": "seems to me that if it 's double check , they should give you one or zero .", "speaker": "E"}, {"text": "you passed the threshold or you didn't pass the threshold ,", "speaker": "E"}, {"text": "and they shouldn't even care about what the score is .", "speaker": "E"}, {"text": "but , , we 'll we 'll see what they come up with .", "speaker": "B"}, {"text": "but in the current thing , , where you have this - matched , moderately - matched , and mis highly - mismatched , , the emphasis is somewhat on the on the - matched , but it 's only marginal ,", "speaker": "B"}, {"text": "it 's like forty , thirty - five , twenty - five , like that .", "speaker": "B"}, {"text": "so you still if you were way , way off on the highly - mismatched , it would have big effect .", "speaker": "B"}, {"text": "and , , it wouldn't surprise me if they did something like that with this .", "speaker": "B"}, {"text": "so again , if you 're if you get if it doesn't help you much , , for noisy versions of this of large vocabulary data , then , , , it may not hurt you that much .", "speaker": "B"}, {"text": "but if it if you don't if it doesn't help you much , , or to put it another way , if it helps some people lot more than it helps other people , , if their strategies do , then", "speaker": "B"}, {"text": "so is this , ?", "speaker": "E"}, {"text": "guenter was putting bunch of wall street journal data on our disks .", "speaker": "E"}, {"text": "that 's it .", "speaker": "B"}, {"text": "so that 's the data that we 'll be running on ?", "speaker": "E"}, {"text": "so we have the data , just not the recognizer .", "speaker": "B"}, {"text": "so this test may take quite while to run , then . may - judging by the amount of data that he was putting on .", "speaker": "E"}, {"text": "there 's training and test ,", "speaker": "B"}, {"text": "no , , if it 's like the other things , there 's there 's data for training the ms and data for testing it .", "speaker": "B"}, {"text": "so , training the recognizer ,", "speaker": "B"}, {"text": "but it 's trained on clean and", "speaker": "B"}, {"text": "is it trained on clean and test on ?", "speaker": "B"}, {"text": "the wall street ?", "speaker": "D"}, {"text": "it 's training on range between ten and twenty db , , and testing between five and fifteen .", "speaker": "A"}, {"text": "that 's what got on", "speaker": "A"}, {"text": "it 's , it 's like medium - mismatch condition , .", "speaker": "D"}, {"text": "and so the noise is there is range of different noises also which are selected randomly and added randomly , , to the files .", "speaker": "A"}, {"text": "and there are noises that are different from the noises used on ti - digits .", "speaker": "A"}, {"text": ", wouldn't imagine that the amount of testing data was that huge .", "speaker": "B"}, {"text": "they probably put training , almost certain they put training data there too .", "speaker": "B"}, {"text": "that 's that .", "speaker": "B"}, {"text": "anybody have anything else ?", "speaker": "B"}, {"text": "one last question on that .", "speaker": "E"}, {"text": "when did they estimate that they would have that system available for download ?", "speaker": "E"}, {"text": "one some preliminary version is already there .", "speaker": "D"}, {"text": "so there 's something you can download to just learn ?", "speaker": "E"}, {"text": "it 's already there .", "speaker": "D"}, {"text": "but they 're actually parallel - doing some modifications also , .", "speaker": "D"}, {"text": "so the final system will be frozen by middle of , like , one more week .", "speaker": "D"}, {"text": "that 's pretty soon .", "speaker": "B"}, {"text": "that 's just one more .", "speaker": "D"}, {"text": "is this their , , svm recognizer ?", "speaker": "C"}, {"text": "no , it 's just straightforward .", "speaker": "D"}, {"text": "their they have lot of options in their recognizer and the svm is one of the things they 've done with it , but it 's not their more standard thing .", "speaker": "B"}, {"text": "for the most part it 's it 's gaussian mixtures .", "speaker": "B"}, {"text": "it 's just , gaussian mixture model .", "speaker": "D"}, {"text": "the svm thing was an also .", "speaker": "B"}, {"text": "it was just it was like hybrid , like", "speaker": "B"}, {"text": "so , just so that understand , they 're providing scripts and everything so that , , you push button and it does training , and then it does test , and everything ?", "speaker": "E"}, {"text": "is that the idea ?", "speaker": "E"}, {"text": "something like that .", "speaker": "D"}, {"text": "it 's like as painless as possible ,", "speaker": "D"}, {"text": "is what do they provide all the scripts , everything , and then just ,", "speaker": "D"}, {"text": "somehow yo there 's hooks to put your features in and", "speaker": "E"}, {"text": ", if you look into it little bit , it might be reasonable", "speaker": "B"}, {"text": "just to ask him about the issue of , , different features having different kinds of , , scaling characteristics and so on .", "speaker": "B"}, {"text": "so that , , possibly having entirely different optimal values for the usual twiddle factors", "speaker": "B"}, {"text": "and what 's what 's the plan about that ?", "speaker": "B"}, {"text": "so sh shall we , like , add chuck also to the mailing lists ?", "speaker": "D"}, {"text": "it may be better , , in that case if he 's going to", "speaker": "D"}, {"text": "because there 's mailing list for this .", "speaker": "D"}, {"text": "that 'd be .", "speaker": "E"}, {"text": "hari or hynek , one of them , has to send mail to joe .", "speaker": "D"}, {"text": "could send him an email .", "speaker": "E"}, {"text": ", to add or wh", "speaker": "D"}, {"text": "know him really .", "speaker": "E"}, {"text": "so that 's just fine .", "speaker": "D"}, {"text": "was just talking with him on email the other day actually .", "speaker": "E"}, {"text": ", and just , , se see .", "speaker": "B"}, {"text": "about other things ,", "speaker": "E"}, {"text": "do you have hari 's , ?", "speaker": "B"}, {"text": "so just cc hari and say that you 've just been asked to handle the large vocabulary part here ,", "speaker": "B"}, {"text": "and , , ,", "speaker": "B"}, {"text": "would it be better if asked hari to ask joe ?", "speaker": "E"}, {"text": "why don't you just ask joe but cc hari , and then in the note say , \" hari , hopefully this is with you \" .", "speaker": "B"}, {"text": "and then if joe feels like he needs confirmation , har answer it .", "speaker": "B"}, {"text": "that way you can get started asking joe quickly while he 's while he 's still , , putting in nails and screws and", "speaker": "B"}, {"text": "and there is an , , archive of all the mails that has been gon that has gone , , between these people among these people .", "speaker": "D"}, {"text": "so just you can see all this mails in the isip web site", "speaker": "D"}, {"text": "mississippi web site .", "speaker": "D"}, {"text": "is that password controlled ?", "speaker": "E"}, {"text": "it 's password protected .", "speaker": "D"}, {"text": "so , like , it 's , like", "speaker": "D"}, {"text": "have you thought about how long would be , most useful for you to go up to ogi ?", "speaker": "B"}, {"text": "we can for september , we can set up work schedule and we can work independently .", "speaker": "A"}, {"text": "and then at some point it be better to work together again .", "speaker": "A"}, {"text": "so you 're you 're imagining more that you would come back here first for while and then and then go up there ?", "speaker": "B"}, {"text": "it 's to you .", "speaker": "B"}, {"text": "anyway , you don't have to decide this second but about it about what you would think would be the best way to work it . 'll", "speaker": "B"}, {"text": "support it either way , so .", "speaker": "B"}, {"text": "got anything to tell us ?", "speaker": "B"}, {"text": "'ve been reading some literature about clustering of data .", "speaker": "C"}, {"text": "just , , , let me put it in context .", "speaker": "C"}, {"text": "so we 're talking about discovering intermediate categories to , to classify .", "speaker": "C"}, {"text": "and , , was looking at some of the work that , , sangita was doing on these traps things .", "speaker": "C"}, {"text": "so she has , she has temporal patterns for , , certain set of phonemes , from timit ,", "speaker": "C"}, {"text": "the most common phonemes .", "speaker": "C"}, {"text": "and each one of them has pattern over time , one second window .", "speaker": "C"}, {"text": "and it has these patterns .", "speaker": "C"}, {"text": "so she has , trap for each one of the phonemes , , times fifteen , for each of the fifteen critical bands .", "speaker": "C"}, {"text": "and , , she does this agglomerative hierarchical clustering which , , is clustering algorithm that , , starts with many , many different points many different clusters , corresponding to the number of data , , patterns that you have in the data .", "speaker": "C"}, {"text": "and then you have this distance mej metric which , , measures how closely related they are .", "speaker": "C"}, {"text": "and you start , by merging the patterns that are most closely related .", "speaker": "C"}, {"text": "and you create tree .", "speaker": "E"}, {"text": "and then you can pick , , values anywhere along that tree to fix your set of clusters .", "speaker": "E"}, {"text": "usually it 's when , when the sol similarity measures , , don't go down as much .", "speaker": "C"}, {"text": "and so , so you stop at that point .", "speaker": "C"}, {"text": "and what she found was , sh , was there were five broad , broad categories , , corresponding to , , things like , , fricatives and , , vocalic , , and , , stops .", "speaker": "C"}, {"text": "and , , one for silence and another one for schwa sounds .", "speaker": "C"}, {"text": "and , , was thinking about ways to generalize this", "speaker": "C"}, {"text": "because you 're it 's like it 's not completely automatic way of clustering ,", "speaker": "C"}, {"text": "because yo beforehand you have these traps and you 're saying that these frames correspond to this particular phoneme .", "speaker": "C"}, {"text": "and that 's that 's constraining your clustering to the set of phonemes that you already have .", "speaker": "C"}, {"text": "whereas we want to just take look at , , arbitrary windows in time , , of varying length , , and cluster those .", "speaker": "C"}, {"text": "and 'm thinking if we if we do that , then we would probably , , at some point in the clustering algorithm find that we 've clustered things like , , thi this is transition , , this is relatively stable point .", "speaker": "C"}, {"text": "and 'm hoping to find other things of similarity and use these things as the intermediate , intermediate categories that , , , 'll later classify .", "speaker": "C"}, {"text": "are you looking at these in narrow bands ?", "speaker": "B"}, {"text": "cuz that 's what you 're gonna be using ,", "speaker": "B"}, {"text": "haven't exactly figured out , , the exact details for that", "speaker": "C"}, {"text": "but , , the representation of the data that was thinking of , was using , , critical band , , energies , , over different lengths of time .", "speaker": "C"}, {"text": ", it seems somehow that needs , there 's couple things that wonder about with this .", "speaker": "B"}, {"text": "so one is , again , looking at the same representation ,", "speaker": "B"}, {"text": "if you 're going for this thing where you have , little detectors that are looking at narrow bands , then what you 're going to be looking for should be some category that you can find with the narrow bands .", "speaker": "B"}, {"text": "that that seems to be fundamental to it .", "speaker": "B"}, {"text": "and then the other thing , , is that wonder about with it , and don't take this in the wrong way , like 'm doing or anything ,", "speaker": "B"}, {"text": "but , . , just wondering really .", "speaker": "B"}, {"text": "the standard answer about this thing is that if you 're trying to find the system in some sense , whether you 're trying by categories or parameters , and your goal is discrimination , then having choices based on discrimination as opposed to , , unsupervised nearness of things , , is actually better .", "speaker": "B"}, {"text": "and if that , since you 're dealing with issues of robustness , , this isn't , but it 'd be something 'd be concerned about .", "speaker": "B"}, {"text": "because , , you can imagine , , if you remember from , from your quals , john ohala saying that , , \" buh \" and \" puh \" differed , , not really cuz of voicing but because of aspiration .", "speaker": "B"}, {"text": "in as far as wha what 's really there in the acoustics .", "speaker": "B"}, {"text": "so , , if you looked if you were doing some coarse clustering , you probably would put those two sounds together .", "speaker": "B"}, {"text": "and yet , would gue would that many of your recognition errors were coming from , , , pfft , screwing up on this distinction .", "speaker": "B"}, {"text": "so , , it 's little hard because recognizers , to first order , work .", "speaker": "B"}, {"text": "and the reasons we 're doing the things we 're doing is because they don't work as as we 'd like .", "speaker": "B"}, {"text": "and since they work , , it means that they are already doing", "speaker": "B"}, {"text": "if you go and take any recognizer that 's already out there and you say , \" how is it distinguishing between schwas and stops ? \"", "speaker": "B"}, {"text": "boy , bet they 're all doing nearly perfectly on this ,", "speaker": "B"}, {"text": "so these big categories that differ in huge obvious ways , we already know how to do .", "speaker": "B"}, {"text": "so , what are we bringing to the party ?", "speaker": "B"}, {"text": "what we wanna do is have something that , particularly in the presence of noise , , is better at distinguishing between , , categories that are actually close to one another , and hence , would probably be clustered together .", "speaker": "B"}, {"text": "so that 's that 's the hard thing .", "speaker": "B"}, {"text": "understand that there 's this other constraint that you 're considering , is that you wanna have categories that , that would be straightforward for , say , human being to mark if you had manual annotation .", "speaker": "B"}, {"text": "and it 's something that you really think you can pick up .", "speaker": "B"}, {"text": "but it 's also essential that you wanna look at what are the confusions that you 're making and how can you come up with , , categories that , , can clarify these confusions .", "speaker": "B"}, {"text": "so , , the standard way of doing that is take look at the algorithms you 're looking at , but then throw in some discriminative aspect to it .", "speaker": "B"}, {"text": "this is more like , , how does lda differ from pca ?", "speaker": "B"}, {"text": "they 're the same thing .", "speaker": "B"}, {"text": "they 're both orthogonalizing .", "speaker": "B"}, {"text": "and , , this is little harder because you 're not just trying to find parameters . you 're actually trying to find the the categories themselves .", "speaker": "B"}, {"text": "so little more like brain surgery ,", "speaker": "B"}, {"text": "anyway . that 's my thought .", "speaker": "B"}, {"text": "you 've been thinking about this problem for long time actually .", "speaker": "B"}, {"text": "actually , you stopped thinking about it for long time , but you used to think about it lot .", "speaker": "B"}, {"text": "and you 've been thinking about it more now ,", "speaker": "B"}, {"text": "don't , it 's not clear to me how to reconcile , , what you 're saying , which is , with the way 've been looking at it .", "speaker": "E"}, {"text": "that it 's it 's all not very clear to me .", "speaker": "E"}, {"text": "but it seems to me that the desire the desirable feature to have is something that , , is bottom - up .", "speaker": "E"}, {"text": "however we do that .", "speaker": "E"}, {"text": "and and so what don't understand is how to do that and still be discriminative ,", "speaker": "E"}, {"text": "because to be discriminative you have to have categories and the only categories that we know of to use are these human sig significant categories that are significant to humans , like phonemes , things like that .", "speaker": "E"}, {"text": "but that 's what you want to avoid .", "speaker": "E"}, {"text": "and so that feels how to get out of this .", "speaker": "E"}, {"text": "here 's here 's , here 's generic and possibly useless thought , which is , , what do you really , in sense the only systems that make sense , , are ones that have something from top - down in in them .", "speaker": "B"}, {"text": "because if even the smallest organism that 's trying to learn to do anything , if it doesn't have any reward for doing or penal penalty for doing anything , then it 's just going to behave randomly .", "speaker": "B"}, {"text": "so whether you 're talking about something being learned through evolution or being learned through experience , it 's gotta have something come down to it that gives its reward or , , at least some reinforcement learning ,", "speaker": "B"}, {"text": "so the question is , how far down ?", "speaker": "E"}, {"text": "we could stop at words , but we don't ,", "speaker": "E"}, {"text": "we go all the way down to phonemes .", "speaker": "E"}, {"text": "but me that in some ways part of the difficulty is trying to deal with the with these phonemes .", "speaker": "B"}, {"text": "and and it 's almost like you want categories if our if our , , , metric of goodness , , if our", "speaker": "B"}, {"text": "correction if our metric of badness is word error rate then , , we should be looking at words .", "speaker": "B"}, {"text": "for for very , , reasons we 've looked for while at syllables , and they have lot of good properties ,", "speaker": "B"}, {"text": "but if you go all the way to words , , that 's really , in many applications you wanna go further .", "speaker": "B"}, {"text": "you wanna go to concepts , or have have concepts , actions , this thing .", "speaker": "B"}, {"text": "but words would be", "speaker": "E"}, {"text": "words aren't bad , .", "speaker": "B"}, {"text": "so the common , the common wisdom is you can't do words because there 's too many of them ,", "speaker": "E"}, {"text": "so you have to have some smaller set that you can use ,", "speaker": "E"}, {"text": "and so everybody goes to phonemes .", "speaker": "E"}, {"text": "but the problem is that we build models of words in terms of phonemes and these models are really cartoon - ish ,", "speaker": "E"}, {"text": "so when you look at conversational speech , , you don't see the phonemes that you that you have in your word models .", "speaker": "E"}, {"text": "but but we 're not trying for models of words here .", "speaker": "B"}, {"text": "see , so her here 's where", "speaker": "B"}, {"text": "if the issue is that we 're trying to come up with , , some intermediate categories which will then be useful for later , , then it doesn't matter that we can't have enough", "speaker": "B"}, {"text": "what you wanna do is build up these categories that are that are best for word recognition .", "speaker": "B"}, {"text": "and and somehow if that 's built into the loop of what the categories", "speaker": "B"}, {"text": "we do this every day in this very gross way of running thousand experiments", "speaker": "B"}, {"text": "because we have fast computers and picking the thing that has the best word error rate .", "speaker": "B"}, {"text": "in some way , we derive the time .", "speaker": "B"}, {"text": "in some ways it 's really not bad thing to do", "speaker": "B"}, {"text": "because it tells you how your adjustments at the very low level affect the final goal .", "speaker": "B"}, {"text": "so there 's way to even put that in much more automatic way ,", "speaker": "B"}, {"text": "where you take , , something about the error at the level of the word or some other it could be syllable but in some large unit ,", "speaker": "B"}, {"text": "and , you may not have word models , you have phone models , whatever ,", "speaker": "B"}, {"text": "but you don't worry about that , and just somehow feed it back through .", "speaker": "B"}, {"text": "so that 's , , wh what called useless comments because 'm not really telling you how to do it .", "speaker": "B"}, {"text": "but , it 's it 's it 's , it", "speaker": "B"}, {"text": "no , but the important part in there is that , , if you want to be discriminative , you have to have , , categories .", "speaker": "E"}, {"text": "and this the important categories are the words , and not the phones .", "speaker": "E"}, {"text": "if you can put the words in to the loop somehow for determining goodness of your sets of clusters", "speaker": "E"}, {"text": "now , that being said , that if you have something that is ,", "speaker": "B"}, {"text": "once you start dealing with spontaneous speech , all the things you 're saying are really true .", "speaker": "B"}, {"text": "if you have read speech that 's been manually annotated , like timit , then , , you the phones are gonna be , actually , for the most part .", "speaker": "B"}, {"text": "so so , , it doesn't really hurt them to do that , to put in discrimination at that level .", "speaker": "B"}, {"text": "if you go to spontaneous speech then it 's it 's trickier", "speaker": "B"}, {"text": "and and , , the phones are", "speaker": "B"}, {"text": ", it 's gonna be based on bad pronunciation models that you have of", "speaker": "B"}, {"text": "and , and it won't allow for the overlapping phenomenon", "speaker": "B"}, {"text": "so it 's almost like there 's this mechanism that we have that , , when we 're hearing read speech and all the phonemes are there , we deal with that ,", "speaker": "E"}, {"text": "but when we go to conversational , and then all of sudden not all the phonemes are there , it doesn't really matter that much to us as humans", "speaker": "E"}, {"text": "because we have some mechanism thows for these word models , whatever those models are , to be munged , ,", "speaker": "E"}, {"text": "and it doesn't really hurt ,", "speaker": "E"}, {"text": "'m not how to build that in .", "speaker": "E"}, {"text": "the other thing is to think of little bit", "speaker": "B"}, {"text": "we when when you start looking at these results it usually is pretty intuitive ,", "speaker": "B"}, {"text": "but start looking at , what are the kinds of confusions that you do make , , , between words if you want or or , , even phones in in read speech , say , , when there is noise .", "speaker": "B"}, {"text": "so is it more across place or more across manner ?", "speaker": "B"}, {"text": "or is it cor , is it ?", "speaker": "B"}, {"text": "know one thing that happens is that you you , , you lose the , , , low energy phones .", "speaker": "B"}, {"text": "if there 's added noise then low energy phones sometimes don't get heard .", "speaker": "B"}, {"text": "and if that if that is if it , if that turns it into another word or different , or another pair of words , then it 's more likely to happen .", "speaker": "B"}, {"text": "but , , , would would that you 'd", "speaker": "B"}, {"text": "anyway , that 's", "speaker": "B"}, {"text": "part of the difficulty is that lot of the robustness that we have is probably coming from much higher level .", "speaker": "E"}, {"text": "we understand the context of the situation when we 're having conversation .", "speaker": "E"}, {"text": "and so if there 's noise in there , , our brain fills in and imagines what should be there .", "speaker": "E"}, {"text": "we 're we 're doing some prediction of what", "speaker": "C"}, {"text": ", that 's really big .", "speaker": "B"}, {"text": "but , even if you do , , diagnostic rhyme test things , , where there really isn't an any information like that , , people are still better in noise than they than they are in , , than the machines are .", "speaker": "B"}, {"text": "so , , that 's", "speaker": "B"}, {"text": "we can't we can't get it without any language models .", "speaker": "B"}, {"text": "language models are there and important", "speaker": "B"}, {"text": "if we 're not working on that then we should work on something else and improve it ,", "speaker": "B"}, {"text": "but especially if it looks like the potential is there .", "speaker": "B"}, {"text": "should we do some digits ?", "speaker": "B"}, {"text": "since we 're here ?", "speaker": "B"}, {"text": "go ahead , morgan .", "speaker": "E"}, {"text": "that 's all folks .", "speaker": "B"}], "id": "xxxxx", "relations": [{"y": 1, "x": 0, "type": "Explanation"}]}]