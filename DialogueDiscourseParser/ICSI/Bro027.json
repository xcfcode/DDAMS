[{"edus": [{"text": "we 're going .", "speaker": "A"}, {"text": "eight , eight ?", "speaker": "C"}, {"text": "this is three .", "speaker": "D"}, {"text": "let 's see .", "speaker": "B"}, {"text": "move it bit .", "speaker": "B"}, {"text": "it 's alright .", "speaker": "B"}, {"text": "let 's see .", "speaker": "B"}, {"text": "barry 's not here and dave 's not here .", "speaker": "B"}, {"text": "say about just just quickly to get through it , that dave and submitted this asru .", "speaker": "B"}, {"text": "it 's it 's interesting .", "speaker": "B"}, {"text": "we 're dealing with rever reverberation ,", "speaker": "B"}, {"text": "and , , when we deal with pure reverberation , the technique he 's using works really , really .", "speaker": "B"}, {"text": "and when they had the reverberation here , , we 'll measure the signal - to - noise ratio", "speaker": "B"}, {"text": "and it 's , , about nine db .", "speaker": "B"}, {"text": "you mean , from the actual , , recordings ?", "speaker": "A"}, {"text": "it 's nine db ?", "speaker": "A"}, {"text": "and actually it brought up question which may be relevant to the aurora too .", "speaker": "B"}, {"text": "know that when you figured out the filters that we 're using for the mel scale , there was some experimentation that went on at , at ogi .", "speaker": "B"}, {"text": "but one of the differences that we found between the two systems that we were using , the aurora htk system baseline system and the system that we were the , other system we were using , the , the sri system , was that the sri system had , , hundred hertz high - pass .", "speaker": "B"}, {"text": "and the , , aurora htk , it was like twenty .", "speaker": "B"}, {"text": "sixty - four .", "speaker": "D"}, {"text": "sixty - four .", "speaker": "D"}, {"text": "sixty - four ?", "speaker": "B"}, {"text": "if you 're using the baseline .", "speaker": "D"}, {"text": "is that the ba band center ?", "speaker": "B"}, {"text": "the edge is really , , sixty - four ?", "speaker": "B"}, {"text": "for some reason , , dave thought it was twenty ,", "speaker": "B"}, {"text": "so the , , center would be somewhere around like hundred", "speaker": "D"}, {"text": "and hundred and it 's like fi hundred hertz .", "speaker": "D"}, {"text": "but do , , how far down it would be at twenty hertz ?", "speaker": "B"}, {"text": "what the how much rejection would there be at twenty hertz , let 's say ?", "speaker": "B"}, {"text": "at twenty hertz .", "speaker": "D"}, {"text": "any idea what the curve looks like ?", "speaker": "B"}, {"text": "it 's it 's zero at twenty hertz , ?", "speaker": "D"}, {"text": "yea - actually , the left edge of the first filter is at sixty - four .", "speaker": "C"}, {"text": "sixt - sixty - four .", "speaker": "D"}, {"text": "so anything less than sixty - four is zero .", "speaker": "D"}, {"text": "it 's actually set to zero ?", "speaker": "B"}, {"text": "what filter is that ?", "speaker": "B"}, {"text": "is this , from the from", "speaker": "B"}, {"text": "this is the filter bank in the frequency domain that starts at sixty - four .", "speaker": "C"}, {"text": "so you , so you really set it to zero , the fft ?", "speaker": "B"}, {"text": "so it 's it 's weight on the ball spectrum .", "speaker": "D"}, {"text": "so that 's that 's little different than dave thought , .", "speaker": "B"}, {"text": "still , it 's possible that we 're getting in some more noise .", "speaker": "B"}, {"text": "so wonder , is it @ @ was there their experimentation with , , say , throwing away that filter ?", "speaker": "B"}, {"text": "throwing away the first ?", "speaker": "D"}, {"text": "we 've tried including the full bank .", "speaker": "D"}, {"text": "from zero to four .", "speaker": "D"}, {"text": "and that 's always worse than using sixty - four hertz .", "speaker": "D"}, {"text": "but the question is , whether sixty - four hertz is , , too , , low .", "speaker": "B"}, {"text": "make it hundred or so ?", "speaker": "D"}, {"text": "'ve tried hundred and it was more or less the same , or slightly worse .", "speaker": "D"}, {"text": "on what test set ?", "speaker": "B"}, {"text": "on the same , , speechdat - car ,", "speaker": "D"}, {"text": "it was on the speechdat - car .", "speaker": "B"}, {"text": "so tried hundred to four .", "speaker": "D"}, {"text": "and on and on the , , ti - digits also ?", "speaker": "B"}, {"text": "no , no .", "speaker": "D"}, {"text": "tried it on speechdat - car .", "speaker": "D"}, {"text": "that 'd be something to look at sometime", "speaker": "B"}, {"text": "because what , , , he was looking at was performance in this room .", "speaker": "B"}, {"text": "would that be more like", "speaker": "B"}, {"text": "you 'd think that 'd be more like speechdat - car ,", "speaker": "B"}, {"text": "in terms of the noise .", "speaker": "B"}, {"text": "the speechdat - car is more , , roughly stationary , lot of it .", "speaker": "B"}, {"text": "and and ti - digits is not so much as", "speaker": "B"}, {"text": "it 's not big deal .", "speaker": "B"}, {"text": "anyway , that was just something we wondered about .", "speaker": "B"}, {"text": "certainly lot of the noise , , is , , below hundred hertz .", "speaker": "B"}, {"text": "the signal - to - noise ratio , , looks fair amount better if you if you high - pass filter it from this room .", "speaker": "B"}, {"text": "but it 's still pretty noisy .", "speaker": "B"}, {"text": "even even for hundred hertz up , it 's it 's still fairly noisy .", "speaker": "B"}, {"text": "the signal - to - noise ratio is is actually still pretty bad .", "speaker": "B"}, {"text": "so that 's on that 's on the the far field ones though , ?", "speaker": "A"}, {"text": "that 's on the far field .", "speaker": "B"}, {"text": "the near field 's pretty good .", "speaker": "B"}, {"text": "so wha what is , what 's causing that ?", "speaker": "A"}, {"text": "we got video projector in here ,", "speaker": "B"}, {"text": "and , which we keep on during every session we record ,", "speaker": "B"}, {"text": "which , , we were aware of", "speaker": "B"}, {"text": "but we thought it wasn't bad thing .", "speaker": "B"}, {"text": "that 's noise source .", "speaker": "B"}, {"text": "and there 's also the , , air conditioning .", "speaker": "B"}, {"text": "which , , , is pretty low frequency thing .", "speaker": "B"}, {"text": "so , those are those are major components , ,", "speaker": "B"}, {"text": "for the stationary .", "speaker": "B"}, {"text": "said this last week too", "speaker": "B"}, {"text": "but it it really became apparent to us that we need to take account of noise .", "speaker": "B"}, {"text": "so when he gets done with his prelim study one of the next things we 'd want to do is to take this , , noise , , processing and , , synthesize some speech from it .", "speaker": "B"}, {"text": "when are his prelims ?", "speaker": "A"}, {"text": "in about , , little less than two weeks .", "speaker": "B"}, {"text": "it might even be sooner .", "speaker": "B"}, {"text": "let 's see , this is the sixteenth ,", "speaker": "B"}, {"text": "if he 's before", "speaker": "B"}, {"text": "it might even be in week .", "speaker": "B"}, {"text": "ed that they were gonna do it some time during the semester", "speaker": "A"}, {"text": "week and half .", "speaker": "B"}, {"text": "but they 'll do it any time , ?", "speaker": "A"}, {"text": "they seem to be", "speaker": "B"}, {"text": "the semester actually is starting up .", "speaker": "B"}, {"text": "is it already ?", "speaker": "A"}, {"text": "the semester 's late august they start here .", "speaker": "B"}, {"text": "so they do it at the beginning of the semester .", "speaker": "B"}, {"text": "the overall results seemed to be first place in in the case of either , , artificial reverberation or modest sized training set .", "speaker": "B"}, {"text": "either way , , , it helped lot .", "speaker": "B"}, {"text": "and but if you had really big training set , recognizer , , system that was capable of taking advantage of really large training set", "speaker": "B"}, {"text": "that one thing with the htk is that is has the as we 're using the configuration we 're using is is being bound by the terms of aurora ,", "speaker": "B"}, {"text": "we have all those parameters just set as they are .", "speaker": "B"}, {"text": "so even if we had hundred times as much data , we wouldn't go out to , , ten or or hundred times as many gaussians or anything .", "speaker": "B"}, {"text": "it 's hard to take advantage of of big chunks of data .", "speaker": "B"}, {"text": "whereas the other one does expand as you have more training data .", "speaker": "B"}, {"text": "it does it automatically , actually .", "speaker": "B"}, {"text": "that one really benefited from the larger set .", "speaker": "B"}, {"text": "and it was also diverse set with different noises and .", "speaker": "B"}, {"text": "that , that seemed to be", "speaker": "B"}, {"text": "so , if you have that better recognizer that can that can build up more parameters , and if you , , have the natural room , which in this case has pretty bad signal - to - noise ratio , then in that case , , the thing to do is just do use speaker adaptation . and and not bother with this acoustic , , processing .", "speaker": "B"}, {"text": "but that would not be true if we did some explicit noise - processing as as , , the convolutional things we were doing .", "speaker": "B"}, {"text": "that 's what we found .", "speaker": "B"}, {"text": ", started working on the mississippi state recognizer .", "speaker": "A"}, {"text": "so , got in touch with joe and , , from your email and things like that .", "speaker": "A"}, {"text": "and , , they added me to the list", "speaker": "A"}, {"text": "the mailing list .", "speaker": "A"}, {"text": "and he gave me all of the pointers and everything that needed .", "speaker": "A"}, {"text": "and so downloaded the ,", "speaker": "A"}, {"text": "there were two things , , that they had to download .", "speaker": "A"}, {"text": "one was the , , the software .", "speaker": "A"}, {"text": "and another wad was , , like sample sample run .", "speaker": "A"}, {"text": "so downloaded the software and compiled all of that .", "speaker": "A"}, {"text": "and it compiled fine .", "speaker": "A"}, {"text": "and , , grabbed the sample", "speaker": "A"}, {"text": "but haven't , , compiled it .", "speaker": "A"}, {"text": "that sample was released only yesterday or the day before , ?", "speaker": "D"}, {"text": "haven't grabbed that one yet .", "speaker": "A"}, {"text": "so there 's two .", "speaker": "A"}, {"text": "there is another short sample set", "speaker": "D"}, {"text": "there was another short one ,", "speaker": "A"}, {"text": "and so haven't grabbed the latest one that he just , , put out yet .", "speaker": "A"}, {"text": "but , the software seemed to compile fine and everything ,", "speaker": "A"}, {"text": "is there any word yet about the issues about , , adjustments for different feature sets or anything ?", "speaker": "B"}, {"text": "you asked me to write to him", "speaker": "A"}, {"text": "and forgot to ask him about that .", "speaker": "A"}, {"text": "or if did ask him , he didn't reply .", "speaker": "A"}, {"text": "don't remember yet .", "speaker": "A"}, {"text": "'ll 'll 'll double check that and ask him again .", "speaker": "A"}, {"text": "it 's like that could turn out to be an important issue for us .", "speaker": "B"}, {"text": "cuz they have it", "speaker": "D"}, {"text": "'ll send it to the list .", "speaker": "A"}, {"text": "cuz they have , , already frozen those in insertion penalties and all those is what feel .", "speaker": "D"}, {"text": "because they have this document explaining the recognizer .", "speaker": "D"}, {"text": "and they have these tables with , , various language model weights , insertion penalties .", "speaker": "D"}, {"text": "haven't seen that one yet .", "speaker": "A"}, {"text": "it 's it 's there on that web .", "speaker": "D"}, {"text": "and , , on that , , they have run some experiments using various insertion penalties and all those", "speaker": "D"}, {"text": "and so they 've picked the values .", "speaker": "A"}, {"text": "they picked the values from", "speaker": "D"}, {"text": "for what test set ?", "speaker": "B"}, {"text": "the one that they have reported is nist evaluation , wall street journal .", "speaker": "D"}, {"text": "but that has nothing to do with what we 're testing on , ?", "speaker": "B"}, {"text": "so they 're , like", "speaker": "D"}, {"text": "so they are actually trying to , , fix that those values using the clean , , training part of the wall street journal .", "speaker": "D"}, {"text": "aurora has clean subset .", "speaker": "D"}, {"text": "they want to train it", "speaker": "D"}, {"text": "and then this they 're going to run some evaluations .", "speaker": "D"}, {"text": "so they 're set they 're setting it based on that ?", "speaker": "B"}, {"text": "so now , we may come back to the situation where we may be looking for modification of the features to account for the fact that we can't modify these parameters .", "speaker": "B"}, {"text": "but it 's still worth , , just since , just chatting with joe about the issue .", "speaker": "B"}, {"text": "do you think that 's something should just send to him", "speaker": "A"}, {"text": "or do you should send it to this there 's an mailing list .", "speaker": "A"}, {"text": "it 's not secret .", "speaker": "B"}, {"text": "we 're , , certainly willing to talk about it with everybody ,", "speaker": "B"}, {"text": "but that , , it 's probably best to start talking with him just to", "speaker": "B"}, {"text": "@ @ , it 's dialogue between two of you about what , what does he think about this and what what could be done about it .", "speaker": "B"}, {"text": "if you get ten people in involved in it there 'll be lot of perspectives based on , , how", "speaker": "B"}, {"text": "but , , it all should come up eventually ,", "speaker": "B"}, {"text": "but if if there is any , , way to move in way that would that would , , be more open to different kinds of features .", "speaker": "B"}, {"text": "but if , if there isn't , and it 's just shut down and then also there 's probably not worthwhile bringing it into larger forum where political issues will come in .", "speaker": "B"}, {"text": "so this is now it 's compiled under solaris ?", "speaker": "D"}, {"text": "because he there was some mail saying that it 's may not be stable for linux and all those .", "speaker": "D"}, {"text": "that was particular version .", "speaker": "A"}, {"text": "susi or whatever it was", "speaker": "A"}, {"text": "but we don't have that .", "speaker": "A"}, {"text": "that 's fine .", "speaker": "D"}, {"text": "it compiled fine actually .", "speaker": "A"}, {"text": "no no errors .", "speaker": "A"}, {"text": "this is slightly off topic", "speaker": "B"}, {"text": "that 's good .", "speaker": "D"}, {"text": "noticed , just glancing at the , , hopkins workshop , , web site that , , one of the thing , we 'll see how much they accomplish , but one of the things that they were trying to do in the graphical models thing was to put together , , tool kit for doing , , arbitrary graphical models for , , speech recognition .", "speaker": "B"}, {"text": "so and jeff , the two jeffs were", "speaker": "B"}, {"text": "who 's the second jeff ?", "speaker": "A"}, {"text": ", do geoff zweig ?", "speaker": "B"}, {"text": "he , he was here for couple years", "speaker": "B"}, {"text": "and he , got his phd .", "speaker": "B"}, {"text": "and he 's , , been at ibm for the last couple years .", "speaker": "B"}, {"text": "that would be neat .", "speaker": "A"}, {"text": "so he did he did his phd on dynamic bayes - nets ,", "speaker": "B"}, {"text": "for speech recognition .", "speaker": "B"}, {"text": "he had some continuity built into the model ,", "speaker": "B"}, {"text": "presumably to handle some , , inertia in the in the production system ,", "speaker": "B"}, {"text": "'ve been playing with , first , the , , vad .", "speaker": "C"}, {"text": "so it 's exactly the same approach ,", "speaker": "C"}, {"text": "but the features that the vad neural network use are , , mfcc after noise compensation .", "speaker": "C"}, {"text": "have the results .", "speaker": "C"}, {"text": "what was it using before ?", "speaker": "B"}, {"text": "before it was just", "speaker": "C"}, {"text": "it was just the noisy features .", "speaker": "D"}, {"text": "this is what we get after this", "speaker": "C"}, {"text": "so , actually , we , , here the features are noise compensated", "speaker": "C"}, {"text": "and there is also the lda filter .", "speaker": "C"}, {"text": "and then it 's pretty small neural network which use , , nine frames of six features from - zero to - fives , plus the first derivatives .", "speaker": "C"}, {"text": "and it has one hundred hidden units .", "speaker": "C"}, {"text": "is that nine frames , centered around the current frame ? or", "speaker": "A"}, {"text": "so , 'm 'm ,", "speaker": "B"}, {"text": "there 's there 's how many how many inputs ?", "speaker": "B"}, {"text": "so it 's twelve times nine .", "speaker": "C"}, {"text": "twelve times nine inputs ,", "speaker": "B"}, {"text": "and hundred , , hidden .", "speaker": "B"}, {"text": "so about eleven thousand parameters ,", "speaker": "B"}, {"text": "which actually shouldn't be problem , even in small phones . .", "speaker": "B"}, {"text": "so what is different between this and what you", "speaker": "A"}, {"text": "it should be .", "speaker": "C"}, {"text": "so the previous syst", "speaker": "C"}, {"text": "it 's based on the system that has fifty - three point sixty - six percent improvement .", "speaker": "C"}, {"text": "it 's the same system .", "speaker": "C"}, {"text": "the only thing that changed is the es the estimation of the silence probabilities .", "speaker": "C"}, {"text": "which now is based on , , cleaned features .", "speaker": "C"}, {"text": "and , it 's it 's lot better .", "speaker": "B"}, {"text": "so it 's it 's not bad ,", "speaker": "C"}, {"text": "but the problem is still that the latency is too large .", "speaker": "C"}, {"text": "what 's the latency ?", "speaker": "B"}, {"text": "the latency of the vad is two hundred and twenty milliseconds .", "speaker": "C"}, {"text": "and , , the vad is used , for on - line normalization ,", "speaker": "C"}, {"text": "and it 's used before the delta computation .", "speaker": "C"}, {"text": "so if you add these components it goes to hundred and seventy ,", "speaker": "C"}, {"text": "you started off with two - twenty and you ended up with one - seventy ?", "speaker": "B"}, {"text": "with two an two hundred and seventy .", "speaker": "C"}, {"text": "two - seventy .", "speaker": "B"}, {"text": "if you add the delta comp delta computation", "speaker": "C"}, {"text": "which is done afterwards .", "speaker": "C"}, {"text": "so it 's two - twenty .", "speaker": "B"}, {"text": "the is this are these twenty - millisecond frames ?", "speaker": "B"}, {"text": "is that why ?", "speaker": "B"}, {"text": "is it after downsampling ?", "speaker": "B"}, {"text": "the two - twenty is one hundred milliseconds for the", "speaker": "C"}, {"text": "no , it 's forty milliseconds for for the , , cleaning of the speech .", "speaker": "C"}, {"text": "then there is , , the neural network which use nine frames .", "speaker": "C"}, {"text": "so it adds forty milliseconds .", "speaker": "C"}, {"text": "after that , , you have the , filtering of the silence probabilities .", "speaker": "C"}, {"text": "which is million filter", "speaker": "C"}, {"text": "and it creates one hundred milliseconds delay .", "speaker": "C"}, {"text": "plus there is delta at the input .", "speaker": "D"}, {"text": "and there is the delta at the input", "speaker": "C"}, {"text": "one hundred milliseconds for smoothing .", "speaker": "B"}, {"text": "so it 's @ @", "speaker": "C"}, {"text": "it 's like forty plus forty plus", "speaker": "D"}, {"text": "this forty plus twenty , plus one hundred .", "speaker": "C"}, {"text": "so it 's two hundred actually .", "speaker": "D"}, {"text": "there are twenty that comes from", "speaker": "C"}, {"text": "there is ten that comes from the lda filters also .", "speaker": "C"}, {"text": "so it 's two hundred and ten ,", "speaker": "C"}, {"text": "if you are using", "speaker": "D"}, {"text": "plus the frame ,", "speaker": "C"}, {"text": "if you are using three frames", "speaker": "D"}, {"text": "so it 's two - twenty .", "speaker": "C"}, {"text": "if you are phrasing using three frames , it is thirty here for delta .", "speaker": "D"}, {"text": "it 's it 's five frames ,", "speaker": "C"}, {"text": "so five frames , that 's twenty .", "speaker": "D"}, {"text": "so it 's who un two hundred and ten .", "speaker": "D"}, {"text": "it 's forty for the for the cleaning of the speech ,", "speaker": "B"}, {"text": "forty for the ann ,", "speaker": "B"}, {"text": "hundred for the smoothing .", "speaker": "B"}, {"text": "but at ten ,", "speaker": "B"}, {"text": "twenty for the delta .", "speaker": "C"}, {"text": "twenty for delta .", "speaker": "B"}, {"text": "at {nonvocalsound} at the input .", "speaker": "D"}, {"text": "that 's at the input to the net .", "speaker": "D"}, {"text": "delta at input to net ?", "speaker": "B"}, {"text": "so it 's like five , six cepstrum plus delta", "speaker": "D"}, {"text": "at nine frames of", "speaker": "D"}, {"text": "and then ten milliseconds for", "speaker": "B"}, {"text": "fi - there 's an lda filter .", "speaker": "D"}, {"text": "ten milliseconds for lda filter ,", "speaker": "B"}, {"text": "and and ten another ten milliseconds you said for the frame ?", "speaker": "B"}, {"text": "for the frame .", "speaker": "C"}, {"text": "computed two - twenty", "speaker": "C"}, {"text": "it 's for the fr the", "speaker": "C"}, {"text": "and then there 's delta besides that ?", "speaker": "B"}, {"text": "so this is the features that are used by our network", "speaker": "C"}, {"text": "and then afterwards , you have to compute the delta on the , , main feature stream ,", "speaker": "C"}, {"text": "which is , delta and double - deltas ,", "speaker": "C"}, {"text": "which is fifty milliseconds .", "speaker": "C"}, {"text": "no , , the", "speaker": "B"}, {"text": "after the noise part , the forty the other hundred and eighty", "speaker": "B"}, {"text": "some of this is , is in parallel , isn't it ?", "speaker": "B"}, {"text": "you have the lda as part of the - , vad ?", "speaker": "B"}, {"text": "the vad use , , lda filtered features also .", "speaker": "C"}, {"text": "so in that case there isn't too much in parallel .", "speaker": "B"}, {"text": "there is , , just downsampling , upsampling , and the lda .", "speaker": "C"}, {"text": "so the delta at the end is how much ?", "speaker": "B"}, {"text": "it 's fifty .", "speaker": "C"}, {"text": "but , we could probably put the delta , , before on - line normalization .", "speaker": "C"}, {"text": "it should not that make big difference ,", "speaker": "C"}, {"text": "what if you used smaller window for the delta ?", "speaker": "A"}, {"text": "could that help little bit ?", "speaker": "A"}, {"text": "there 's lot of things you could do to", "speaker": "A"}, {"text": "so if you if you put the delta before the , , ana on - line if", "speaker": "B"}, {"text": "then it could go in parallel .", "speaker": "B"}, {"text": "and then then you don't have that additive", "speaker": "B"}, {"text": "cuz the time constant of the on - line normalization is pretty long compared to the delta window ,", "speaker": "C"}, {"text": "it should not make", "speaker": "C"}, {"text": "and you ought to be able to shove tw , sh pull off twenty milliseconds from somewhere else to get it under two hundred ,", "speaker": "B"}, {"text": "is two hundred the", "speaker": "A"}, {"text": "mill hundred milliseconds for smoothing is an arbitrary amount .", "speaker": "B"}, {"text": "it could be eighty", "speaker": "B"}, {"text": "and probably do @ @", "speaker": "B"}, {"text": "wh - what 's the baseline you need to be under ?", "speaker": "A"}, {"text": "they 're still arguing about it .", "speaker": "B"}, {"text": "if it 's two if it 's , if it 's two - fifty , then we could keep the delta where it is if we shaved off twenty .", "speaker": "B"}, {"text": "if it 's two hundred , if we shaved off twenty , we could we could , , meet it by moving the delta back .", "speaker": "B"}, {"text": "so , how do that what you have is too much if they 're still deciding ?", "speaker": "A"}, {"text": "but it 's just", "speaker": "B"}, {"text": "the main thing is that since that we got burned last time , and , by not worrying about it very much , we 're just staying conscious of it .", "speaker": "B"}, {"text": "if if week before we have to be done someone says , \" , you have to have fifty milliseconds less than you have now \" , it would be pretty frantic around here .", "speaker": "B"}, {"text": "but still , that 's that 's pretty big , , win .", "speaker": "A"}, {"text": "and it doesn't seem like you 're in terms of your delay , you 're , , that", "speaker": "A"}, {"text": "he added bit on ,", "speaker": "B"}, {"text": "because before we were had were able to have the noise , , , , and the lva be in parallel .", "speaker": "B"}, {"text": "and now he 's he 's requiring it to be done first .", "speaker": "B"}, {"text": "but the main thing , , is the cleaning of the speech , which takes forty milliseconds or so .", "speaker": "C"}, {"text": "let 's say ten milliseconds seconds for the lda .", "speaker": "B"}, {"text": "and but the lda is , , pretty short now .", "speaker": "C"}, {"text": "and then forty for the other .", "speaker": "B"}, {"text": "the lda we , is , like is it very crucial for the features , ?", "speaker": "D"}, {"text": "this is the first try .", "speaker": "C"}, {"text": "the lda 's not very useful then .", "speaker": "C"}, {"text": "so you could start pulling back ,", "speaker": "B"}, {"text": "you have twenty for delta computation", "speaker": "B"}, {"text": "which now you 're doing twice ,", "speaker": "B"}, {"text": "but yo were you doing that before ?", "speaker": "B"}, {"text": "in the proposal , , the input of the vad network were just three frames , .", "speaker": "C"}, {"text": "on the in the", "speaker": "D"}, {"text": "just the static , no delta .", "speaker": "D"}, {"text": "so , what you have now is fort , forty for the noise , twenty for the delta , and ten for the lda .", "speaker": "B"}, {"text": "that 's seventy milliseconds of which was formerly in parallel ,", "speaker": "B"}, {"text": "that 's that 's the difference as far as the timing ,", "speaker": "B"}, {"text": "and you could experiment with cutting various pieces of these back bit ,", "speaker": "B"}, {"text": "we 're we 're not we 're not in terrible shape .", "speaker": "B"}, {"text": "that 's what it seems like to me .", "speaker": "A"}, {"text": "it 's pretty good .", "speaker": "A"}, {"text": "it 's it 's not like it 's adding up to four hundred milliseconds .", "speaker": "B"}, {"text": "where where is this fifty - seven point two in comparison to the last evaluation ?", "speaker": "A"}, {"text": "it 's it 's better than anything , , anybody got .", "speaker": "B"}, {"text": "the best was fifty - four point five .", "speaker": "C"}, {"text": "and our system was forty - nine ,", "speaker": "C"}, {"text": "but with the neural network .", "speaker": "C"}, {"text": "so this is almost ten percent .", "speaker": "A"}, {"text": "with the with the neural net .", "speaker": "B"}, {"text": "so this is this is like the first proposal .", "speaker": "D"}, {"text": "the proposal - one .", "speaker": "D"}, {"text": "it was forty - four , actually .", "speaker": "D"}, {"text": "and we still don't have the neural net in .", "speaker": "B"}, {"text": "so so it 's", "speaker": "B"}, {"text": "we 're we 're doing better .", "speaker": "B"}, {"text": "this is this is really good .", "speaker": "A"}, {"text": "we 're getting better recognition .", "speaker": "B"}, {"text": "'m other people working on this are not sitting still either ,", "speaker": "B"}, {"text": "the important thing is that we learn how to do this better ,", "speaker": "B"}, {"text": "so , our ,", "speaker": "B"}, {"text": "you can see the numbers that we 're having , say , on speechdat - car", "speaker": "B"}, {"text": "which is hard task ,", "speaker": "B"}, {"text": "cuz it 's really , it 's just reasonable numbers , starting to be .", "speaker": "B"}, {"text": "it 's still terri", "speaker": "B"}, {"text": "even for - matched case it 's sixty percent error rate reduction ,", "speaker": "C"}, {"text": "so actually , this is in between what we had with the previous vad and what sunil did with an idl vad .", "speaker": "C"}, {"text": "which gave sixty - two percent improvement , ?", "speaker": "C"}, {"text": "it 's almost that .", "speaker": "D"}, {"text": "it 's almost an average", "speaker": "D"}, {"text": "what was that ?", "speaker": "A"}, {"text": "say that last part again ?", "speaker": "A"}, {"text": "so , if you use , like , an idl vad , , for dropping the frames ,", "speaker": "C"}, {"text": "or the best we can get .", "speaker": "D"}, {"text": "the best that we can get that means that we estimate the silence probability on the clean version of the utterances .", "speaker": "C"}, {"text": "then you can go up to sixty - two percent error rate reduction , globally .", "speaker": "C"}, {"text": "so that would be even that wouldn't change this number down here to sixty - two ?", "speaker": "A"}, {"text": "so you were get", "speaker": "B"}, {"text": "if you add good very good vad , that works as as vad working on clean speech ,", "speaker": "C"}, {"text": "then you wou you would go", "speaker": "C"}, {"text": "so that 's the best you could hope for .", "speaker": "A"}, {"text": "so fi si fifty - three is what you were getting with the old vad .", "speaker": "B"}, {"text": "and sixty - two with the , , quote , unquote , cheating vad .", "speaker": "B"}, {"text": "and fifty - seven is what you got with the real vad .", "speaker": "B"}, {"text": ", the next thing is , started to play", "speaker": "C"}, {"text": "don't want to worry too much about the delay ,", "speaker": "C"}, {"text": "it 's better to", "speaker": "C"}, {"text": "from the committee .", "speaker": "C"}, {"text": "but started to play with the , , , tandem neural network .", "speaker": "C"}, {"text": "did the configuration that 's very similar to what we did for the february proposal .", "speaker": "C"}, {"text": "so . there is first feature stream that use straight mfcc features .", "speaker": "C"}, {"text": "these features actually .", "speaker": "C"}, {"text": "and the other stream is the output of neural network , using as input , also , these , , cleaned mfcc .", "speaker": "C"}, {"text": "those are those are what is going into the tandem net ?", "speaker": "A"}, {"text": "don't have the comp", "speaker": "C"}, {"text": "so there is just this feature stream , the fifteen mfcc plus delta and double - delta .", "speaker": "C"}, {"text": "so it 's makes forty - five features that are used as input to the htk .", "speaker": "C"}, {"text": "and then , there is there are more inputs that comes from the tandem mlp .", "speaker": "C"}, {"text": "he likes to use them both ,", "speaker": "B"}, {"text": "cuz then it has one part that 's discriminative ,", "speaker": "B"}, {"text": "one part that 's not .", "speaker": "B"}, {"text": "now it seems that tested on speechdat - car while the experiment are running on your on ti - digits .", "speaker": "C"}, {"text": "it improves on the - matched and the mismatched conditions ,", "speaker": "C"}, {"text": "but it get worse on the highly mismatched .", "speaker": "C"}, {"text": "compared to these numbers ?", "speaker": "A"}, {"text": "compared to these numbers , .", "speaker": "C"}, {"text": "like , on the - match and medium mismatch , the gain is around five percent relative ,", "speaker": "C"}, {"text": "but it goes down lot more , like fifteen percent on the case .", "speaker": "C"}, {"text": "you 're just using the full ninety features ?", "speaker": "B"}, {"text": "you have ninety features ?", "speaker": "B"}, {"text": "from the networks , it 's twenty - eight .", "speaker": "C"}, {"text": "and from the other side it 's forty - five .", "speaker": "B"}, {"text": "so , it 's forty - five .", "speaker": "C"}, {"text": "so it 's you have seventy - three features ,", "speaker": "B"}, {"text": "and you 're just feeding them like that .", "speaker": "B"}, {"text": "there isn't any klt or anything ?", "speaker": "B"}, {"text": "there 's klt after the neural network , as before .", "speaker": "C"}, {"text": "that 's how you get down to twenty - eight ?", "speaker": "A"}, {"text": "why twenty - eight ?", "speaker": "A"}, {"text": "it 's it 's because it 's what we did for the first proposal .", "speaker": "C"}, {"text": "we tested , , trying to go down", "speaker": "C"}, {"text": "it 's multiple of seven .", "speaker": "B"}, {"text": "wanted to do something very similar to the proposal as first try .", "speaker": "C"}, {"text": "that makes sense .", "speaker": "A"}, {"text": "but we have to for , we have to go down ,", "speaker": "C"}, {"text": "because the limit is now sixty features .", "speaker": "C"}, {"text": "we have to find way to decrease the number of features .", "speaker": "C"}, {"text": "so , it seems funny that , don't quite understand everything , but that adding features", "speaker": "A"}, {"text": "if you 're keeping the back - end fixed .", "speaker": "A"}, {"text": "that 's it .", "speaker": "A"}, {"text": "because it seems like just adding information shouldn't give worse results .", "speaker": "A"}, {"text": "but if you 're keeping the number of gaussians fixed in the recognizer , then", "speaker": "A"}, {"text": "but , , just in general , adding information", "speaker": "B"}, {"text": "suppose the information you added , , was really terrible feature and all it brought in was noise .", "speaker": "B"}, {"text": "or or suppose it wasn't completely terrible ,", "speaker": "B"}, {"text": "but it was completely equivalent to another one feature that you had ,", "speaker": "B"}, {"text": "except it was noisier .", "speaker": "B"}, {"text": "in that case you wouldn't necessarily expect it to be better .", "speaker": "B"}, {"text": ", wasn't necessarily saying it should be better .", "speaker": "A"}, {"text": "'m just surprised that you 're getting fifteen percent relative worse on the wel", "speaker": "A"}, {"text": "but it 's worse .", "speaker": "C"}, {"text": "on the highly mismatched condition .", "speaker": "B"}, {"text": "on the highly mismatch .", "speaker": "A"}, {"text": "so , \" highly mismatched condition \" means that your training is bad estimate of your test .", "speaker": "B"}, {"text": "so having , , greater number of features , if they aren't the features that you use , certainly can can easily , , make things worse .", "speaker": "B"}, {"text": "if you have if you have , , lots and lots of data , and you have and your your training is representative of your test , then getting more sources of information should just help .", "speaker": "B"}, {"text": "but but it 's it doesn't necessarily work that way .", "speaker": "B"}, {"text": "what 's your what 's your thought about what to do next with it ?", "speaker": "B"}, {"text": "because expected the neural net to help more when there is more mismatch , as it was the case for the", "speaker": "C"}, {"text": "so , was the training set same as the the february proposal ?", "speaker": "D"}, {"text": "it 's the same training set ,", "speaker": "C"}, {"text": "so it 's timit with the ti - digits ' , , noises , , added .", "speaker": "C"}, {"text": "we might , we might have to experiment with , better training sets .", "speaker": "B"}, {"text": "the other thing is , , before you found that was the best configuration , but you might have to retest those things now that we have different the rest of it is different ,", "speaker": "B"}, {"text": "what 's the effect of just putting the neural net on without the other path ?", "speaker": "B"}, {"text": "what the straight features do .", "speaker": "B"}, {"text": "that gives you this .", "speaker": "B"}, {"text": "what it does in combination .", "speaker": "B"}, {"text": "what if you did the", "speaker": "A"}, {"text": "would it make sense to do the klt on the full set of combined features ?", "speaker": "A"}, {"text": "instead of just on the", "speaker": "A"}, {"text": "the reason did it this ways is that in february , it we tested different things like that ,", "speaker": "C"}, {"text": "so , having two klt , having just klt for network , or having global klt .", "speaker": "C"}, {"text": "so you tried the global klt before", "speaker": "A"}, {"text": "and it didn't really", "speaker": "A"}, {"text": "the differences between these configurations were not huge ,", "speaker": "C"}, {"text": "but it was marginally better with this configuration .", "speaker": "C"}, {"text": "but , , that 's another thing to try ,", "speaker": "B"}, {"text": "since things are things are different .", "speaker": "B"}, {"text": "so all of these seventy - three features are going into , , the , the .", "speaker": "B"}, {"text": "and is are are any deltas being computed of tha of them ?", "speaker": "B"}, {"text": "of the straight features , .", "speaker": "C"}, {"text": "but the , ,", "speaker": "C"}, {"text": "tandem features are used as they are .", "speaker": "C"}, {"text": "we can add some context from these features also as dan did in his last work .", "speaker": "C"}, {"text": "but the other thing was thinking was ,", "speaker": "B"}, {"text": "now lost track of what was thinking .", "speaker": "B"}, {"text": "you said there was limit of sixty features ?", "speaker": "A"}, {"text": "what 's the relation between that limit and the , , forty - eight , forty eight hundred bits per second ?", "speaker": "A"}, {"text": "was gonna say .", "speaker": "B"}, {"text": "not no relation .", "speaker": "C"}, {"text": "so don't understand ,", "speaker": "A"}, {"text": "the the forty - eight hundred bits is for transmission of some features .", "speaker": "C"}, {"text": "if you 're only using", "speaker": "A"}, {"text": "and generally , it allows you to transmit like , fifteen , , cepstrum .", "speaker": "C"}, {"text": "the issue was that , , this is supposed to be standard that 's then gonna be fed to somebody 's recognizer somewhere", "speaker": "B"}, {"text": "which might be , , it might be concern how many parameters are use used and .", "speaker": "B"}, {"text": "they felt they wanted to set limit .", "speaker": "B"}, {"text": "so they chose sixty .", "speaker": "B"}, {"text": "some people wanted to use hundreds of parameters", "speaker": "B"}, {"text": "and that bothered some other people .", "speaker": "B"}, {"text": "they just chose that .", "speaker": "B"}, {"text": "it 's arbitrary too .", "speaker": "B"}, {"text": "but but that 's that 's what was chosen .", "speaker": "B"}, {"text": "remembered what was going to say .", "speaker": "B"}, {"text": "what was going to say is that , , with the noise removal , , these things are now more correlated .", "speaker": "B"}, {"text": "so you have two sets of things that are uncorrelated , , within themselves ,", "speaker": "B"}, {"text": "but they 're pretty correlated with one another .", "speaker": "B"}, {"text": "they 're being fed into these , , variants , only gaussians and ,", "speaker": "B"}, {"text": "so it would be better idea now than it was before to , , have , , one klt over everything ,", "speaker": "B"}, {"text": "to de - correlate it .", "speaker": "B"}, {"text": "what are the rs in the training set , timit ?", "speaker": "D"}, {"text": "it 's , , ranging from zero to clean ?", "speaker": "C"}, {"text": "from zero to clean .", "speaker": "C"}, {"text": "so we found this , this macrophone data , and , that we were using for these other experiments , to be pretty good .", "speaker": "B"}, {"text": "so that 's after you explore these other alternatives , that might be another way to start looking , is just improving the training set .", "speaker": "B"}, {"text": "we were getting , , lots better recognition using that , than", "speaker": "B"}, {"text": "you do have the problem that , , we are not able to increase the number of gaussians , , or anything to , , to match anything .", "speaker": "B"}, {"text": "so we 're only improving the training of our feature set ,", "speaker": "B"}, {"text": "but that 's still probably something .", "speaker": "B"}, {"text": "so you 're saying , add the macrophone data to the training of the neural net ? the tandem net ?", "speaker": "A"}, {"text": "that 's the only place that we can train .", "speaker": "B"}, {"text": "we can't train the other with anything other than the standard amount ,", "speaker": "B"}, {"text": "what what was it trained on again ?", "speaker": "A"}, {"text": "the one that you used ?", "speaker": "A"}, {"text": "it 's timit with noise .", "speaker": "C"}, {"text": "so , , it 's rather small", "speaker": "C"}, {"text": "how big is the net , ?", "speaker": "B"}, {"text": "it 's , , five hundred hidden units .", "speaker": "C"}, {"text": "you did experiments back then where you made it bigger", "speaker": "B"}, {"text": "and it and that was that was the threshold point .", "speaker": "B"}, {"text": "much less than that , it was worse ,", "speaker": "B"}, {"text": "much more than that , it wasn't much better .", "speaker": "B"}, {"text": "so is it is it though the performance , big relation in the high ma high mismatch has something to do with the , , cleaning up that you that is done on the timit after adding noise ?", "speaker": "D"}, {"text": "it 's all the noises are from the ti - digits ,", "speaker": "D"}, {"text": "it 's like the high mismatch of the speechdat - car", "speaker": "D"}, {"text": "after cleaning up , having more noise than the training set of timit after clean after you do the noise clean - up .", "speaker": "D"}, {"text": "earlier you never had any compensation ,", "speaker": "D"}, {"text": "you just trained it straight away .", "speaker": "D"}, {"text": "so it had like all these different conditions of rs , actually in their training set of neural net .", "speaker": "D"}, {"text": "but after cleaning up you have now different set of rs , ?", "speaker": "D"}, {"text": "for the training of the neural net .", "speaker": "D"}, {"text": "is it something to do with the mismatch that 's created after the cleaning up , like the high mismatch", "speaker": "D"}, {"text": "you mean the most noisy occurrences on speechdat - car might be lot more noisy than", "speaker": "C"}, {"text": "the snr after the noise compensation of the speechdat - car .", "speaker": "D"}, {"text": "so the training the neural net is being trained with noise compensated .", "speaker": "B"}, {"text": "which makes sense ,", "speaker": "B"}, {"text": "but , , you 're saying , the noisier ones are still going to be , even after our noise compensation , are still gonna be pretty noisy .", "speaker": "B"}, {"text": "so now the after - noise compensation the neural net is seeing different set of rs than that was originally there in the training set . of timit .", "speaker": "D"}, {"text": "because in the timit it was zero to some clean .", "speaker": "D"}, {"text": "so the net saw all the snr @ @ conditions .", "speaker": "D"}, {"text": "now after cleaning up it 's different set of snr .", "speaker": "D"}, {"text": "and that snr may not be , like , com covering the whole set of rs that you 're getting in the speechdat - car .", "speaker": "D"}, {"text": "but the speechdat - car data that you 're seeing is also reduced in noise", "speaker": "B"}, {"text": "by the noise compensation .", "speaker": "B"}, {"text": "but , 'm saying , there could be some issues of", "speaker": "D"}, {"text": "if the initial range of snr is different , we the problem was already there before .", "speaker": "C"}, {"text": ", it depends on whether you believe that the noise compensation is equally reducing the noise on the test set and the training set .", "speaker": "B"}, {"text": "on the test set , .", "speaker": "D"}, {"text": "you 're saying there 's mismatch in noise that wasn't there before ,", "speaker": "B"}, {"text": "but if they were both the same before , then if they were both reduic reduced equally , then , there would not be mismatch .", "speaker": "B"}, {"text": "heaven forbid , this noise compensation process may be imperfect ,", "speaker": "B"}, {"text": "so it 's treating some things differently .", "speaker": "B"}, {"text": "that could be seen from the ti - digits , , testing condition", "speaker": "D"}, {"text": "because , , the noises are from the ti - digits , ?", "speaker": "D"}, {"text": "so cleaning up the ti - digits", "speaker": "D"}, {"text": "and if the performance goes down in the ti - digits mismatch high mismatch like this", "speaker": "D"}, {"text": "clean training , .", "speaker": "C"}, {"text": "on clean training , or zero db testing .", "speaker": "D"}, {"text": "we 'll so we 'll see .", "speaker": "C"}, {"text": "then it 's something to do .", "speaker": "D"}, {"text": "one of the things about", "speaker": "B"}, {"text": "the macrophone data , , , , it was recorded over many different telephones .", "speaker": "B"}, {"text": "so , there 's lots of different kinds of acoustic conditions .", "speaker": "B"}, {"text": "it 's not artificially added noise or anything .", "speaker": "B"}, {"text": "so it 's not the same .", "speaker": "B"}, {"text": "don't think there 's anybody recording over car from car ,", "speaker": "B"}, {"text": "but it 's it 's varied enough that if doing this adjustments , , and playing around with it doesn't , , make it better , the most , it seems like the most obvious thing to do is to improve the training set .", "speaker": "B"}, {"text": "the condition it it gave us an enormous amount of improvement in what we were doing with meeting recorder digits ,", "speaker": "B"}, {"text": "even though there , again , these macrophone digits were very , very different from , , what we were going on here .", "speaker": "B"}, {"text": "we weren't talking over telephone here .", "speaker": "B"}, {"text": "but it was just just having variation in acoustic conditions was just good thing .", "speaker": "B"}, {"text": "actually to , what observed in the case is that the number of deletion dramatically increases .", "speaker": "C"}, {"text": "it it doubles .", "speaker": "C"}, {"text": "number of deletions .", "speaker": "B"}, {"text": "when added the num the neural network it doubles the number of deletions .", "speaker": "C"}, {"text": "so don't how to interpret that ,", "speaker": "C"}, {"text": "and and did an other numbers stay the same ?", "speaker": "A"}, {"text": "insertion substitutions stay the same ?", "speaker": "A"}, {"text": "they stayed the same ,", "speaker": "C"}, {"text": "they they are little bit , lower .", "speaker": "C"}, {"text": "they are little bit better .", "speaker": "C"}, {"text": "did they increase the number of deletions even for the cases that got better ?", "speaker": "B"}, {"text": "say , for the , it", "speaker": "B"}, {"text": "no , it doesn't .", "speaker": "C"}, {"text": "so it 's only the highly mismatched ?", "speaker": "B"}, {"text": "and it remind me again ,", "speaker": "B"}, {"text": "the \" highly mismatched \" means that the", "speaker": "B"}, {"text": "it 's clean training", "speaker": "C"}, {"text": "close microphone training and distant microphone , , high speed , .", "speaker": "C"}, {"text": "the most noisy cases are the distant microphone for testing .", "speaker": "C"}, {"text": "the noise subtraction is subtracting off speech .", "speaker": "B"}, {"text": "but without the neural network it 's , it 's better .", "speaker": "C"}, {"text": "it 's just when we add the neural networks .", "speaker": "C"}, {"text": "the feature are the same except that", "speaker": "C"}, {"text": "that says that , , the , the models in , , the recognizer are really paying attention to the neural net features .", "speaker": "A"}, {"text": "actually {nonvocalsound} the timit noises are range of noises", "speaker": "B"}, {"text": "and they 're not so much the stationary driving noises , ?", "speaker": "B"}, {"text": "it 's it 's pretty different .", "speaker": "B"}, {"text": "there is car noise .", "speaker": "C"}, {"text": "so there are just four noises .", "speaker": "C"}, {"text": "\" car \" , ,", "speaker": "C"}, {"text": "\" babble \" ,", "speaker": "C"}, {"text": "\" babble . \"", "speaker": "D"}, {"text": "\" subway \" , ?", "speaker": "C"}, {"text": "\" street \" or \" airport \" .", "speaker": "D"}, {"text": "and \" street \" isn't", "speaker": "C"}, {"text": "or \" train station \" .", "speaker": "D"}, {"text": "\" train station \" , .", "speaker": "C"}, {"text": "so it 's mostly , \" car \" is stationary ,", "speaker": "C"}, {"text": "\" babble \" , it 's stationary background plus some voices ,", "speaker": "C"}, {"text": "some speech over it .", "speaker": "C"}, {"text": "and the other two are rather stationary also .", "speaker": "C"}, {"text": "that if you run it", "speaker": "B"}, {"text": "actually , you you remember this .", "speaker": "B"}, {"text": "when you in the old experiments when you ran with the neural net only , and didn't have this side path , , , with the pure features as , did it make things better to have the neural net ?", "speaker": "B"}, {"text": "was it about the same ?", "speaker": "B"}, {"text": "it was little bit worse .", "speaker": "C"}, {"text": "than just the features ,", "speaker": "C"}, {"text": "until you put the second path in with the pure features , the neural net wasn't helping .", "speaker": "B"}, {"text": "that 's interesting .", "speaker": "B"}, {"text": "it was helping , , if the features are were bad ,", "speaker": "C"}, {"text": "just plain ps or", "speaker": "C"}, {"text": "as soon as we added lda on - line normalization , and all these things , then", "speaker": "C"}, {"text": "they were doing similar enough things .", "speaker": "B"}, {"text": "still would be interesting to see what would happen if you just had the neural net without the side thing .", "speaker": "B"}, {"text": "and and the thing have in mind is , , you 'll see that the results are not just little bit worse .", "speaker": "B"}, {"text": "that they 're lot worse .", "speaker": "B"}, {"text": "but if on the ha other hand , , it 's , say , somewhere in between what you 're seeing now and and , , what you 'd have with just the pure features , then there is some problem of of , , combination of these things , or correlation between them somehow .", "speaker": "B"}, {"text": "if it really is that the net is hurting you at the moment , then the issue is to focus on , , improving the net .", "speaker": "B"}, {"text": "so what 's the overall effe", "speaker": "B"}, {"text": "you haven't done all the experiments", "speaker": "B"}, {"text": "but you said it was somewhat better , say , five percent better , for the first two conditions , and fifteen percent worse for the other one ?", "speaker": "B"}, {"text": "but it 's but that one 's weighted lower ,", "speaker": "B"}, {"text": "so wonder what the net effect is .", "speaker": "B"}, {"text": "it 's it was one or two percent .", "speaker": "C"}, {"text": "that 's not that bad ,", "speaker": "C"}, {"text": "but it was like two percent relative worse on speechdat - car .", "speaker": "C"}, {"text": "have to check that .", "speaker": "C"}, {"text": "it will overall it will be still better", "speaker": "D"}, {"text": "even if it is fifteen percent worse ,", "speaker": "D"}, {"text": "because the fifteen percent worse is given like twenty - five point two five eight .", "speaker": "D"}, {"text": "so the so the worst it could be , if the others were exactly the same , is four ,", "speaker": "B"}, {"text": "and , , since the others are somewhat better", "speaker": "B"}, {"text": "so it 's four .", "speaker": "D"}, {"text": "so either it 'll get cancelled out , or you 'll get , like , almost the same .", "speaker": "D"}, {"text": "it was it was slightly worse .", "speaker": "C"}, {"text": "it should be pretty close to cancelled out .", "speaker": "B"}, {"text": "'ve been wondering about something .", "speaker": "A"}, {"text": "in the , lot of the , the hub - five systems , , recently have been using lda .", "speaker": "A"}, {"text": "and they , they run lda on the features before they train the models .", "speaker": "A"}, {"text": "so there 's the lda is there before the", "speaker": "A"}, {"text": "so , you are using lda", "speaker": "A"}, {"text": "but it seems like it 's pretty far back in the process .", "speaker": "A"}, {"text": "this lda is different from the lda that you are talking about .", "speaker": "D"}, {"text": "the lda that you saying is , like , you take block of features , like nine frames , and then do an lda on it ,", "speaker": "D"}, {"text": "and then reduce the dimensionality to something like twenty - four like that .", "speaker": "D"}, {"text": "you you can .", "speaker": "A"}, {"text": "and then feed it to .", "speaker": "D"}, {"text": "it 's , you 're just", "speaker": "A"}, {"text": "so this is like two two dimensional tile .", "speaker": "D"}, {"text": "you 're shifting the feature space .", "speaker": "A"}, {"text": "so this is two dimensional tile .", "speaker": "D"}, {"text": "and the lda that we are applying is only in time ,", "speaker": "D"}, {"text": "high cost frequency .", "speaker": "D"}, {"text": "so it 's like more like filtering in time ,", "speaker": "D"}, {"text": "so what what about , what", "speaker": "A"}, {"text": "if this is good idea or not ,", "speaker": "A"}, {"text": "but what if you put ran the other lda , , on your features before they go into the ?", "speaker": "A"}, {"text": "no , actually ,", "speaker": "C"}, {"text": "what do we do with the ann is something like that", "speaker": "C"}, {"text": "except that it 's not linear .", "speaker": "C"}, {"text": "but it 's it 's like nonlinear discriminant analysis .", "speaker": "C"}, {"text": "it 's the it 's", "speaker": "A"}, {"text": "so it 's like", "speaker": "A"}, {"text": "the tandem is like nonlinear lda .", "speaker": "A"}, {"text": "but , but the other features that you have , , the non - tandem ones ,", "speaker": "A"}, {"text": "in the proposal , they were transformed using pca ,", "speaker": "C"}, {"text": "it might be that lda could be better .", "speaker": "C"}, {"text": "the the argument is in", "speaker": "B"}, {"text": "and it 's not like we really know ,", "speaker": "B"}, {"text": "but the argument anyway is that , , , we always have the prob", "speaker": "B"}, {"text": "discriminative things are good .", "speaker": "B"}, {"text": "lda , neural nets , they 're good .", "speaker": "B"}, {"text": "they 're good because you you learn to distinguish between these categories that you want to be good at distinguishing between .", "speaker": "B"}, {"text": "and pca doesn't do that .", "speaker": "B"}, {"text": "it pac - pca low - order pca throws away pieces that are , not gonna be helpful just because they 're small , .", "speaker": "B"}, {"text": "but , , the problem is , training sets aren't perfect and testing sets are different .", "speaker": "B"}, {"text": "so you you face the potential problem with discriminative , be it lda or neural nets , that you are training to discriminate between categories in one space", "speaker": "B"}, {"text": "but what you 're really gonna be getting is something else .", "speaker": "B"}, {"text": "and so , , stephane 's idea was , , let 's feed , , both this discriminatively trained thing and something that 's not .", "speaker": "B"}, {"text": "so you have good set of features that everybody 's worked really hard to make ,", "speaker": "B"}, {"text": "and then , , you discriminately train it ,", "speaker": "B"}, {"text": "but you also take the path that doesn't have that ,", "speaker": "B"}, {"text": "and putting those in together .", "speaker": "B"}, {"text": "so it 's like combination of the , what , , dan has been calling , , feature , , feature combination versus posterior combination .", "speaker": "B"}, {"text": "it 's it 's , , you have the posterior combination", "speaker": "B"}, {"text": "but then you get the features from that and use them as feature combination with these other things .", "speaker": "B"}, {"text": "and that seemed , at least in the last one , as he was just saying , he when he only did discriminative , it actually was it didn't help in this particular case .", "speaker": "B"}, {"text": "there was enough of difference , , between the testing and training .", "speaker": "B"}, {"text": "but by having them both there", "speaker": "B"}, {"text": "the fact is some of the time , the discriminative is gonna help you .", "speaker": "B"}, {"text": "and some of the time it 's going to hurt you ,", "speaker": "B"}, {"text": "and by combining two information sources if , if", "speaker": "B"}, {"text": "so you wouldn't necessarily then want to do lda on the non - tandem features because now you 're doing something to them that", "speaker": "A"}, {"text": "that 's counter to that idea .", "speaker": "B"}, {"text": "now , again , it 's we 're just trying these different things .", "speaker": "B"}, {"text": "we don't really 's gonna work best .", "speaker": "B"}, {"text": "but if that 's the hypothesis , at least it would be counter to that hypothesis to do that .", "speaker": "B"}, {"text": "and in principle you would think that the neural net would do better at the discriminant part than lda .", "speaker": "B"}, {"text": "though , not .", "speaker": "B"}, {"text": "we , we were getting ready to do the tandem , , for the hub - five system ,", "speaker": "A"}, {"text": "and , , andreas and talked about it ,", "speaker": "A"}, {"text": "and the idea the thought was , \" , , , that the neural net should be better ,", "speaker": "A"}, {"text": "but we should at least have , number , , to show that we did try the lda in place of the neural net ,", "speaker": "A"}, {"text": "so that we can , show clear path .", "speaker": "A"}, {"text": "that you have it without it ,", "speaker": "A"}, {"text": "then you have the lda ,", "speaker": "A"}, {"text": "then you have the neural net ,", "speaker": "A"}, {"text": "and you can see , theoretically .", "speaker": "A"}, {"text": "that 's good idea .", "speaker": "B"}, {"text": "did did you do that", "speaker": "B"}, {"text": "or tha that 's", "speaker": "B"}, {"text": "that 's what that 's what we 're gonna do next", "speaker": "A"}, {"text": "as soon as finish this other thing .", "speaker": "A"}, {"text": "no , , that 's good idea .", "speaker": "B"}, {"text": "we just want to show .", "speaker": "A"}, {"text": "it everybody believes it ,", "speaker": "A"}, {"text": "but , we just", "speaker": "A"}, {"text": "no , no ,", "speaker": "B"}, {"text": "but it might not even be true .", "speaker": "B"}, {"text": "it 's it 's it 's idea .", "speaker": "B"}, {"text": "one of the things that always disturbed me , , in the resurgence of neural nets that happened in the eighties was that , , lot of people because neural nets were pretty easy to use lot of people were just using them for all sorts of things without , , looking into the linear , , versions of them .", "speaker": "B"}, {"text": "and , , people were doing recurrent nets but not looking at iir filters ,", "speaker": "B"}, {"text": "and , , ,", "speaker": "B"}, {"text": "so , , it 's definitely good idea to try it .", "speaker": "B"}, {"text": "and everybody 's putting that on their systems now ,", "speaker": "A"}, {"text": "that 's what made me wonder about this ,", "speaker": "A"}, {"text": "they 've been putting them in their systems off and on for ten years ,", "speaker": "B"}, {"text": "but but , ,", "speaker": "B"}, {"text": "what is it 's it 's like in the hub - five evaluations , ,", "speaker": "A"}, {"text": "and you read the system descriptions and everybody 's got , , lda on their features .", "speaker": "A"}, {"text": "and now they all have that .", "speaker": "B"}, {"text": "it 's the transformation they 're estimating on", "speaker": "C"}, {"text": "they are trained on the same data as the final are .", "speaker": "C"}, {"text": "so it 's different .", "speaker": "A"}, {"text": "cuz they don't have these , , mismatches that you have .", "speaker": "A"}, {"text": "so that 's why was wondering if it 's not even good idea .", "speaker": "A"}, {"text": "enough about it ,", "speaker": "A"}, {"text": "part of why you were getting into the klt you were describing to me at one point that you wanted to see if , , , getting good orthogonal features was and combining the different temporal ranges was the key thing that was happening or whether it was this discriminant thing , ?", "speaker": "B"}, {"text": "so you were just trying", "speaker": "B"}, {"text": "this is it doesn't have the lda aspect", "speaker": "B"}, {"text": "but as far as the orthogonalizing transformation , you were trying that at one point , ?", "speaker": "B"}, {"text": "it doesn't work as .", "speaker": "B"}, {"text": "'ve been exploring parallel vad without neural network", "speaker": "D"}, {"text": "with , like , less latency using snr and energy , , after the cleaning up .", "speaker": "D"}, {"text": "so what 'd been trying was , ,", "speaker": "D"}, {"text": "after the after the noise compensation , was trying to find feature based on the ratio of the energies , that is , cl after clean and before clean .", "speaker": "D"}, {"text": "so that if they are , like , pretty close to one , which means it 's speech .", "speaker": "D"}, {"text": "and if it is if it is close to zero , which is so it 's like scale @ @ probability value .", "speaker": "D"}, {"text": "so was trying , , with full band and multiple bands ,", "speaker": "D"}, {"text": "ps separating them to different frequency bands", "speaker": "D"}, {"text": "and deriving separate decisions on each bands , and trying to combine them .", "speaker": "D"}, {"text": "the advantage being like it doesn't have the latency of the neural net if it if it can", "speaker": "D"}, {"text": "and it gave me like , , one point one more than one percent relative improvement .", "speaker": "D"}, {"text": "so , from fifty - three point six it went to fifty four point eight .", "speaker": "D"}, {"text": "so it 's , like , only slightly more than percent improvement ,", "speaker": "D"}, {"text": "which means that it 's it 's doing slightly better job than the previous vad ,", "speaker": "D"}, {"text": "at lower delay .", "speaker": "D"}, {"text": "does it still have the median filter ?", "speaker": "B"}, {"text": "it still has the median filter .", "speaker": "D"}, {"text": "so it still has most of the delay ,", "speaker": "B"}, {"text": "so with the delay , that 's gone is the input , which is the sixty millisecond .", "speaker": "D"}, {"text": "the forty plus twenty .", "speaker": "D"}, {"text": "at the input of the neural net you have this , , nine frames of context plus the delta .", "speaker": "D"}, {"text": "plus the delta ,", "speaker": "B"}, {"text": "so that delay , plus the lda .", "speaker": "D"}, {"text": "so the delay is only the forty millisecond of the noise cleaning , plus the hundred millisecond smoothing at the output .", "speaker": "D"}, {"text": "so the di the biggest", "speaker": "D"}, {"text": "the problem for me was to find consistent threshold that works across the different databases ,", "speaker": "D"}, {"text": "because try to make it work on tr speechdat - car", "speaker": "D"}, {"text": "and it fails on ti - digits ,", "speaker": "D"}, {"text": "or if try to make it work on that it 's just the italian , it doesn't work on the finnish .", "speaker": "D"}, {"text": "so there are there was , like , some problem in balancing the deletions and insertions when try different thresholds .", "speaker": "D"}, {"text": "'m still trying to make it better by using some other features from the after the clean up", "speaker": "D"}, {"text": "some , , correlation auto - correlation or some additional features of to mainly the improvement of the vad . 've been trying .", "speaker": "D"}, {"text": "now this this , , \" before and after clean \" , it sounds like you think that 's good feature .", "speaker": "B"}, {"text": "that that , it you think that the , the it appears to be good feature , ?", "speaker": "B"}, {"text": "what about using it in the neural net ?", "speaker": "B"}, {"text": "eventually we could just", "speaker": "C"}, {"text": "so that 's the", "speaker": "D"}, {"text": "so we 've been thinking about putting it into the neural net also .", "speaker": "D"}, {"text": "because they did that itself", "speaker": "D"}, {"text": "then you don't have to worry about the thresholds and", "speaker": "C"}, {"text": "so that 's ,", "speaker": "D"}, {"text": "so if we if we can live with the latency or cut the latencies elsewhere , then that would be , , good thing .", "speaker": "B"}, {"text": "anybody has anybody you or naren , , somebody , tried the , , , second second stream thing ?", "speaker": "B"}, {"text": "put the second stream in place and , ran one experiment ,", "speaker": "D"}, {"text": "but just like just to know that everything is fine .", "speaker": "D"}, {"text": "so it was like , , forty - five cepstrum plus twenty - three mel log mel .", "speaker": "D"}, {"text": "and and , just , like , it gave me the baseline performance of the aurora ,", "speaker": "D"}, {"text": "which is like zero improvement .", "speaker": "D"}, {"text": "so tried it on italian just to know that everything is", "speaker": "D"}, {"text": "but didn't export anything out of it", "speaker": "D"}, {"text": "because it was , like , weird feature set .", "speaker": "D"}, {"text": "what , , would be more what you 'd want to do is is , , put it into another neural net .", "speaker": "B"}, {"text": "we 're we 're not quite there yet .", "speaker": "B"}, {"text": "so we have to figure out the neural nets , .", "speaker": "B"}, {"text": "the , other thing was wondering was , , if the neural net , , has any because of the different noise con unseen noise conditions for the neural net ,", "speaker": "D"}, {"text": "where , like , you train it on those four noise conditions , while you are feeding it with , like , additional some four plus some few more conditions which it hasn't seen , actually ,", "speaker": "D"}, {"text": "from the while testing .", "speaker": "D"}, {"text": "instead of just having , those cleaned up cepstrum , sh should we feed some additional information , like the the", "speaker": "D"}, {"text": "we have the vad flag .", "speaker": "D"}, {"text": "should we feed the vad flag , also , at the input so that it has some additional discriminating information at the input ?", "speaker": "D"}, {"text": "wh - , the vad what ?", "speaker": "B"}, {"text": "we have the vad information also available at the back - end .", "speaker": "D"}, {"text": "so if it is something the neural net is not able to discriminate the classes", "speaker": "D"}, {"text": "because most of it is sil", "speaker": "D"}, {"text": "we have dropped some silence we have dropped so silence frames ?", "speaker": "D"}, {"text": "no , we haven't dropped silence frames still .", "speaker": "D"}, {"text": "the biggest classification would be the speech and silence .", "speaker": "D"}, {"text": "so , by having an additional , , feature which says \" this is speech and this is nonspeech \" , , it certainly helps in some unseen noise conditions for the neural net .", "speaker": "D"}, {"text": "do do you have that feature available for the test data ?", "speaker": "A"}, {"text": ", we have we are transferring the vad to the back - end", "speaker": "D"}, {"text": "feature to the back - end .", "speaker": "D"}, {"text": "because we are dropping it at the back - end after everything all the features are computed .", "speaker": "D"}, {"text": "so that is coming from separate neural net or some vad .", "speaker": "D"}, {"text": "which is which is certainly giving", "speaker": "D"}, {"text": "so you 're saying , feed that , also , into the neural net .", "speaker": "A"}, {"text": "so it 's an additional discriminating information .", "speaker": "D"}, {"text": "you could feed it into the neural net .", "speaker": "B"}, {"text": "the other thing you could do is just , , modify the , , output probabilities of the of the , , , neural net , tandem neural net , based on the fact that you have silence probability .", "speaker": "B"}, {"text": "so you have an independent estimator of what the silence probability is ,", "speaker": "B"}, {"text": "and you could multiply the two things , and renormalize .", "speaker": "B"}, {"text": ", you 'd have to do the nonlinearity part and deal with that .", "speaker": "B"}, {"text": ", go backwards from what the nonlinearity would , would be .", "speaker": "B"}, {"text": "through to the soft max .", "speaker": "D"}, {"text": "but in principle wouldn't it be better to feed it in ?", "speaker": "A"}, {"text": "and let the net do that ?", "speaker": "A"}, {"text": "let 's put it this way .", "speaker": "B"}, {"text": "you have this complicated system with thousands and thousand parameters", "speaker": "B"}, {"text": "and you can tell it , , \" learn this thing . \"", "speaker": "B"}, {"text": "or you can say , \" it 's silence !", "speaker": "B"}, {"text": "go away ! \"", "speaker": "B"}, {"text": "the second one sounds lot more direct .", "speaker": "B"}, {"text": "so , what if you then , since this , what if you only use the neural net on the speech portions ?", "speaker": "A"}, {"text": "that 's the same .", "speaker": "A"}, {"text": "that 's similar .", "speaker": "A"}, {"text": "you 'd have to actually run it continuously ,", "speaker": "B"}, {"text": "but , train the net only on", "speaker": "A"}, {"text": "but it 's @ @", "speaker": "B"}, {"text": "you want to train on the nonspeech also ,", "speaker": "B"}, {"text": "because that 's part of what you 're learning in it ,", "speaker": "B"}, {"text": "to to generate , that it 's it has to distinguish between .", "speaker": "B"}, {"text": "but , if you 're gonna if you 're going to multiply the output of the net by this other decision , , would then you don't care about whether the net makes that distinction , ?", "speaker": "A"}, {"text": "but this other thing isn't perfect .", "speaker": "B"}, {"text": "so that you bring in some information from the net itself .", "speaker": "B"}, {"text": "that 's good point .", "speaker": "A"}, {"text": "now the only thing that bothers me about all this is that the the fact", "speaker": "B"}, {"text": "it 's bothersome that you 're getting more deletions .", "speaker": "B"}, {"text": "so might look at ,", "speaker": "C"}, {"text": "is it due to the fact that , the probability of the silence at the output of the network , is ,", "speaker": "C"}, {"text": "is too high .", "speaker": "B"}, {"text": "if it 's the case , then multiplying it again by by something ?", "speaker": "C"}, {"text": "it may not be it", "speaker": "D"}, {"text": "it may be too it 's too high in sense , like , everything is more like , , flat probability .", "speaker": "D"}, {"text": "- eee - hhh .", "speaker": "C"}, {"text": "so , like , it 's not really doing any distinction between speech and nonspeech", "speaker": "D"}, {"text": "or , , different among classes .", "speaker": "D"}, {"text": "be interesting to look at the", "speaker": "A"}, {"text": "wonder if you could do this .", "speaker": "A"}, {"text": "but if you look at the , , highly mism high mismat the output of the net on the high mismatch case and just look at , , the distribution versus the other ones , do you do you see more peaks ?", "speaker": "A"}, {"text": "like the entropy of the output ,", "speaker": "C"}, {"text": "it it seems that the vad network doesn't , it doesn't drop , , too many frames", "speaker": "C"}, {"text": "because the dele the number of deletion is reasonable .", "speaker": "C"}, {"text": "but it 's just when we add the tandem , the final mlp , and then", "speaker": "C"}, {"text": "now the only problem is you don't want to ta for the output of the vad before you can put something into the other system ,", "speaker": "B"}, {"text": "cuz that 'll shoot up the latency lot ,", "speaker": "B"}, {"text": "am missing something here ?", "speaker": "B"}, {"text": "so that 's problem with what was just saying .", "speaker": "B"}, {"text": "but if you were gonna put it in as feature it means you already have it by the time you get to the tandem net ,", "speaker": "A"}, {"text": "we we don't have it , actually ,", "speaker": "D"}, {"text": "because it 's it has high rate energy", "speaker": "D"}, {"text": "it 's done in", "speaker": "B"}, {"text": "some of the things are , not in parallel ,", "speaker": "B"}, {"text": "but certainly , it would be in parallel with the with tandem net .", "speaker": "B"}, {"text": "so , if that doesn't work ,", "speaker": "B"}, {"text": "but it would be interesting to see if that was the problem , anyway .", "speaker": "B"}, {"text": "and and then another alternative would be to take the feature that you 're feeding into the vad , and feeding it into the other one as .", "speaker": "B"}, {"text": "and then it would just learn it better .", "speaker": "B"}, {"text": "that 's an interesting thing to try to see , if what 's going on is that in the highly mismatched condition , it 's , , causing deletions by having this silence probability up too high ,", "speaker": "B"}, {"text": "at some point where the vad is saying it 's actually speech .", "speaker": "B"}, {"text": "which is probably true .", "speaker": "B"}, {"text": "if the vad said", "speaker": "B"}, {"text": "since the vad is is lot ,", "speaker": "B"}, {"text": "we just started working with it .", "speaker": "B"}, {"text": "but these are these are some good ideas .", "speaker": "B"}, {"text": "and the other thing", "speaker": "C"}, {"text": "there are other issues for the tandem ,", "speaker": "C"}, {"text": "like , , , do we want to , do we want to work on the targets ?", "speaker": "C"}, {"text": "or , like , instead of using phonemes , using more context dependent units ?", "speaker": "C"}, {"text": "for the tandem net you mean ?", "speaker": "A"}, {"text": "'m thinking , also , about dan 's work where he trained network , not on phoneme targets but on the state targets .", "speaker": "C"}, {"text": "it was giving slightly better results .", "speaker": "C"}, {"text": "problem is , if you are going to run this on different test sets , including large vocabulary ,", "speaker": "B"}, {"text": "was just thinking about , like , generalized diphones ,", "speaker": "C"}, {"text": "and come up with reasonable , not too large , set of context dependent units ,", "speaker": "C"}, {"text": "and then anyway we would have to reduce this with the klt .", "speaker": "C"}, {"text": "but it it 's all worth looking at ,", "speaker": "B"}, {"text": "but it sounds to me like , , looking at the relationship between this and the speech noise is is probably key thing .", "speaker": "B"}, {"text": "that and the correlation between .", "speaker": "B"}, {"text": "if the , , high mismatch case had been more like the , , the other two cases in terms of giving you just better performance , how would this number have changed ?", "speaker": "A"}, {"text": "around five percent better , .", "speaker": "C"}, {"text": "we what 's it 's gonna be the ti - digits yet .", "speaker": "B"}, {"text": "he hasn't got the results back yet .", "speaker": "B"}, {"text": "if you extrapolate the speechdat - car - matched and medium - mismatch , it 's around , , five .", "speaker": "C"}, {"text": "so this would be sixty - two ?", "speaker": "A"}, {"text": "sixty - two .", "speaker": "B"}, {"text": "sixty - two ,", "speaker": "C"}, {"text": "somewhere around sixty , must be .", "speaker": "D"}, {"text": "it 's around five percent , because it 's", "speaker": "C"}, {"text": "if everything is five percent .", "speaker": "C"}, {"text": "all the other ones were five percent ,", "speaker": "A"}, {"text": "have the speechdat - car now ,", "speaker": "C"}, {"text": "it shou we should have the results today during the afternoon ,", "speaker": "C"}, {"text": "so won't be here for", "speaker": "B"}, {"text": "when do you leave ?", "speaker": "A"}, {"text": "'m leaving next wednesday .", "speaker": "B"}, {"text": "may or may not be in the morning .", "speaker": "B"}, {"text": "leave in the afternoon .", "speaker": "B"}, {"text": "you 're not gonna be around this afternoon ?", "speaker": "A"}, {"text": "'m talking about next week .", "speaker": "B"}, {"text": "'m leaving next wednesday .", "speaker": "B"}, {"text": "for the meeting meeting ?", "speaker": "B"}, {"text": "that 's just cuz of something on campus .", "speaker": "B"}, {"text": "so next week won't ,", "speaker": "B"}, {"text": "and the week after won't ,", "speaker": "B"}, {"text": "cuz 'll be in finland .", "speaker": "B"}, {"text": "and the week after that won't .", "speaker": "B"}, {"text": "by that time you 'll be , you 'll both be gone from here .", "speaker": "B"}, {"text": "so there 'll be no definitely no meeting on september sixth .", "speaker": "B"}, {"text": "what 's september sixth ?", "speaker": "A"}, {"text": "that 's during eurospeech .", "speaker": "B"}, {"text": "so , , sunil will be in oregon .", "speaker": "B"}, {"text": "stephane and will be in denmark .", "speaker": "B"}, {"text": "so it 'll be few weeks , really , before we have meeting of the same cast of characters .", "speaker": "B"}, {"text": "you should probably meet .", "speaker": "B"}, {"text": "and barry will be around .", "speaker": "B"}, {"text": "and then , we 'll start up again with dave and dave and barry and stephane and us on the , , twentieth .", "speaker": "B"}, {"text": "you 're gonna be gone for the next three weeks ?", "speaker": "A"}, {"text": "'m gone for two and half weeks starting next wed - late next wednesday .", "speaker": "B"}, {"text": "so that 's you won't be at the next three of these meetings .", "speaker": "A"}, {"text": "it 's probably four because of", "speaker": "B"}, {"text": "is it three ?", "speaker": "B"}, {"text": "let 's see ,", "speaker": "B"}, {"text": "twenty - third ,", "speaker": "B"}, {"text": "and the third one won't probably won't be meeting ,", "speaker": "B"}, {"text": "cuz , , su - sunil , stephane , and will all not be here .", "speaker": "B"}, {"text": "mmm . so it 's just , , the next two where there will be there , , may as be meetings ,", "speaker": "B"}, {"text": "but won't be at them .", "speaker": "B"}, {"text": "and then starting up on the thirteenth , {nonvocalsound} , we 'll have meetings again", "speaker": "B"}, {"text": "but we 'll have to do without sunil here somehow .", "speaker": "B"}, {"text": "when do you go back ?", "speaker": "A"}, {"text": "thirty - first , august .", "speaker": "D"}, {"text": "when is the evaluation ?", "speaker": "A"}, {"text": "it was supposed to be november fifteenth .", "speaker": "B"}, {"text": "has anybody heard anything different ?", "speaker": "B"}, {"text": "the meeting in is the five and six of december .", "speaker": "C"}, {"text": "it 's like , it 's tentatively all full .", "speaker": "D"}, {"text": "that 's proposed date , .", "speaker": "D"}, {"text": "so the evaluation should be on week before", "speaker": "C"}, {"text": "but , no , this is good progress .", "speaker": "B"}, {"text": "should we do digits ?", "speaker": "A"}, {"text": "we 're done .", "speaker": "B"}, {"text": "it 's wrap .", "speaker": "B"}], "id": "xxxxx", "relations": [{"y": 1, "x": 0, "type": "Explanation"}]}]