[{"edus": [{"text": "mike . mike - one ?", "speaker": "A"}, {"text": "we 're on ?", "speaker": "D"}, {"text": "we 're testing noise robustness", "speaker": "D"}, {"text": "but let 's not get silly .", "speaker": "D"}, {"text": "so , , you 've got some , , xerox things to pass out ?", "speaker": "D"}, {"text": "'m for the table ,", "speaker": "A"}, {"text": "but as it grows in size , , it .", "speaker": "A"}, {"text": "so for the last column we use our imagination .", "speaker": "D"}, {"text": "do you want @ @ .", "speaker": "B"}, {"text": "this one 's , though .", "speaker": "D"}, {"text": "this has big font .", "speaker": "D"}, {"text": "let 's see .", "speaker": "C"}, {"text": "when you get older you have these different perspectives .", "speaker": "D"}, {"text": "lowering the word hour rate is fine , but having big font !", "speaker": "D"}, {"text": "next time we will put colors .", "speaker": "A"}, {"text": "that 's what 's", "speaker": "D"}, {"text": "it 's mostly big font .", "speaker": "D"}, {"text": "so there is summary of what has been done", "speaker": "A"}, {"text": "it 's this .", "speaker": "A"}, {"text": "summary of experiments since , , since last week", "speaker": "A"}, {"text": "and also since the we 've started to run work on this .", "speaker": "A"}, {"text": "so since last week we 've started to fill the column with features with nets trained on plp with on - line normalization", "speaker": "A"}, {"text": "but with delta also ,", "speaker": "A"}, {"text": "because the column was not completely", "speaker": "A"}, {"text": "- . - .", "speaker": "D"}, {"text": "it 's still not completely filled ,", "speaker": "A"}, {"text": "but we have more results to compare with network using without plp", "speaker": "A"}, {"text": "and finally , hhh , ehhh pl - delta seems very important .", "speaker": "A"}, {"text": "if you take , let 's say , anyway aurora - two - ,", "speaker": "A"}, {"text": "so , the next the second , , part of the table ,", "speaker": "A"}, {"text": "when we use the large training set using french , spanish , and english , you have one hundred and six without delta", "speaker": "A"}, {"text": "and eighty - nine with the delta .", "speaker": "A"}, {"text": "all of these numbers are with hundred percent being , , the baseline performance ,", "speaker": "D"}, {"text": "on the baseline , .", "speaker": "A"}, {"text": "but with mel cepstra system going straight into the htk ?", "speaker": "D"}, {"text": "so now we see that the gap between the different training set is much much smaller", "speaker": "A"}, {"text": "it 's out of the way .", "speaker": "C"}, {"text": "but , actually , , for english training on timit is still better than the other languages .", "speaker": "A"}, {"text": "and also for italian , actually .", "speaker": "A"}, {"text": "if you take the second set of experiment for italian ,", "speaker": "A"}, {"text": "so , the mismatched condition ,", "speaker": "A"}, {"text": "when we use the training on timit", "speaker": "A"}, {"text": "so , it 's multi - english ,", "speaker": "A"}, {"text": "we have ninety - one number ,", "speaker": "A"}, {"text": "and training with other languages is little bit worse .", "speaker": "A"}, {"text": "down near the bottom of this sheet .", "speaker": "D"}, {"text": "and , , and here the gap is still more important between using delta and not using delta .", "speaker": "A"}, {"text": "if if take the training the large training set , it 's we have one hundred and seventy - two ,", "speaker": "A"}, {"text": "and one hundred and four when we use delta .", "speaker": "A"}, {"text": "even if the contexts used is quite the same ,", "speaker": "A"}, {"text": "because without delta we use seventeenths seventeen frames .", "speaker": "A"}, {"text": ", so the second point is that we have no single cross - language experiments , , that we did not have last week .", "speaker": "A"}, {"text": "so this is training the net on french only ,", "speaker": "A"}, {"text": "or on english only ,", "speaker": "A"}, {"text": "and testing on italian .", "speaker": "A"}, {"text": "and training the net on french only", "speaker": "A"}, {"text": "and testing on , ti - digits .", "speaker": "A"}, {"text": "what we see is that these nets are not as good ,", "speaker": "A"}, {"text": "except for the multi - english , which is always one of the best .", "speaker": "A"}, {"text": "then we started to work on large dat database containing , , sentences from the french , from the spanish , from the timit , from spine , from english digits , and from italian digits .", "speaker": "A"}, {"text": "so this is the another line another set of lines in the table .", "speaker": "A"}, {"text": "@ @ with spine", "speaker": "A"}, {"text": "and , actually we did this before knowing the result of all the data ,", "speaker": "A"}, {"text": "so we have to redo the the experiment training the net with , plp , but with delta .", "speaker": "A"}, {"text": "this net performed quite .", "speaker": "A"}, {"text": "it 's it 's better than the net using french , spanish , and english only .", "speaker": "A"}, {"text": "we have also started feature combination experiments .", "speaker": "A"}, {"text": "many experiments using features and net outputs together .", "speaker": "A"}, {"text": "and this is the results are on the other document .", "speaker": "A"}, {"text": "we can discuss this after , perhaps , just , @ @ .", "speaker": "A"}, {"text": "so there are four systems .", "speaker": "A"}, {"text": "the first one , , is combining , , two feature streams , using", "speaker": "A"}, {"text": "and each feature stream has its own mpl .", "speaker": "A"}, {"text": "so it 's the similar to the tandem that was proposed for the first .", "speaker": "A"}, {"text": "the multi - stream tandem for the first proposal .", "speaker": "A"}, {"text": "the second is using features and klt transformed mlp outputs .", "speaker": "A"}, {"text": "and the third one is to use single klt trans transform features as as mlp outputs .", "speaker": "A"}, {"text": "you can you can comment these results ,", "speaker": "A"}, {"text": "would like to say that , , , mmm , if we doesn't use the delta - delta , we have an improve when we use some combination .", "speaker": "B"}, {"text": "we ju just to be clear , the numbers here are recognition accuracy .", "speaker": "A"}, {"text": "this , this number recognition acc", "speaker": "B"}, {"text": "so it 's not the again we switch to another", "speaker": "A"}, {"text": "yes , and the baseline the baseline have is eighty - two .", "speaker": "B"}, {"text": "baseline is eighty - two .", "speaker": "D"}, {"text": "so it 's experiment only on the italian mismatched for the moment for this .", "speaker": "A"}, {"text": "this is italian mismatched .", "speaker": "D"}, {"text": "by the moment .", "speaker": "B"}, {"text": "and first in the experiment - one do use different mlp ,", "speaker": "B"}, {"text": "and is that the multi - english mlp is the better .", "speaker": "B"}, {"text": "for the ne rest of experiment use multi - english ,", "speaker": "B"}, {"text": "only multi - english .", "speaker": "B"}, {"text": "and try to combine different type of feature ,", "speaker": "B"}, {"text": "but the result is that the msg - three feature doesn't work for the italian database", "speaker": "B"}, {"text": "because never help to increase the accuracy .", "speaker": "B"}, {"text": ", actually , if we look at the table ,", "speaker": "A"}, {"text": "the huge table ,", "speaker": "A"}, {"text": "we see that for ti - digits msg perform as as the plp ,", "speaker": "A"}, {"text": "but this is not the case for italian what where the error rate is is almost twice the error rate of plp .", "speaker": "A"}, {"text": "so , , , don't think this is bug", "speaker": "A"}, {"text": "but this is something in probably in the msg process that", "speaker": "A"}, {"text": "perhaps the fact that the there 's no low - pass filter ,", "speaker": "A"}, {"text": "or no pre - emp pre - emphasis filter", "speaker": "A"}, {"text": "and that there is some dc offset in the italian ,", "speaker": "A"}, {"text": "something simple like that .", "speaker": "A"}, {"text": "but that we need to sort out if want to get improvement by combining plp and msg", "speaker": "A"}, {"text": "because for the moment msg do doesn't bring much information .", "speaker": "A"}, {"text": "and as carmen said , if we combine the two , we have the result , , of plp .", "speaker": "A"}, {"text": "the , baseline system", "speaker": "D"}, {"text": "when you said the baseline system was , eighty - two percent , that was trained on what and tested on what ?", "speaker": "D"}, {"text": "that was , italian mismatched , digits , , is the testing ,", "speaker": "D"}, {"text": "and the training is italian digits ?", "speaker": "D"}, {"text": "so the \" mismatch \" just refers to the noise and , microphone and ,", "speaker": "D"}, {"text": "so , did we have", "speaker": "D"}, {"text": "so would that then correspond to the first line here of where the training is the italian digits ?", "speaker": "D"}, {"text": "the train the training of the htk ?", "speaker": "B"}, {"text": "training of the net ,", "speaker": "D"}, {"text": "so , so what that says is that in matched condition , we end up with fair amount worse putting in the plp .", "speaker": "D"}, {"text": "now would do we have number , suppose for the matched", "speaker": "D"}, {"text": "don't mean matched ,", "speaker": "D"}, {"text": "but use of italian training in italian digits for plp only ?", "speaker": "D"}, {"text": "so this is this is in the table .", "speaker": "A"}, {"text": "so the number is fifty - two ,", "speaker": "A"}, {"text": "fifty - two percent .", "speaker": "D"}, {"text": "fift - so no , it 's it 's the", "speaker": "A"}, {"text": "no , fifty - two percent of eighty - two ?", "speaker": "D"}, {"text": "so it 's it 's error rate , .", "speaker": "A"}, {"text": "it 's plus six .", "speaker": "B"}, {"text": "it 's er error rate ratio .", "speaker": "A"}, {"text": "this is accuracy !", "speaker": "D"}, {"text": "so we have nine let 's say ninety percent .", "speaker": "A"}, {"text": "which is what we have also if use plp and msg together ,", "speaker": "A"}, {"text": "eighty - nine point seven .", "speaker": "A"}, {"text": "so even just plp , , it is not , in the matched condition", "speaker": "D"}, {"text": "wonder if it 's difference between plp and mel cepstra , or whether it 's that the net half , for some reason , is not helping .", "speaker": "D"}, {"text": "- plp and mel cepstra give the same results .", "speaker": "A"}, {"text": "we have these results .", "speaker": "A"}, {"text": "do you have this result with plp alone , fee feeding htk ?", "speaker": "A"}, {"text": "that 's what you mean ?", "speaker": "A"}, {"text": "just plp at the input of htk .", "speaker": "A"}, {"text": "at the first and the", "speaker": "B"}, {"text": "eighty - eight point six .", "speaker": "D"}, {"text": "but that 's , that 's without the neural net ,", "speaker": "D"}, {"text": "that 's without the neural net", "speaker": "A"}, {"text": "and that 's the result that ogi has also with the mfcc with on - line normalization .", "speaker": "A"}, {"text": "but she had said eighty - two .", "speaker": "D"}, {"text": "this is the , but this is without on - line normalization .", "speaker": "A"}, {"text": "this the eighty - two .", "speaker": "D"}, {"text": "eighty - two is the it 's the aurora baseline ,", "speaker": "A"}, {"text": "then we can use", "speaker": "A"}, {"text": "ogi , they use mfcc the baseline mfcc plus on - line normalization", "speaker": "A"}, {"text": "because this is accuracy .", "speaker": "D"}, {"text": "alright . so this is was thinking all this was worse .", "speaker": "D"}, {"text": "so this is all better", "speaker": "D"}, {"text": "yes , better .", "speaker": "B"}, {"text": "because eighty - nine is bigger than eighty - two .", "speaker": "D"}, {"text": "'m 'm all better now .", "speaker": "D"}, {"text": "so what happ what happens is that when we apply on - line normalization we jump to almost ninety percent .", "speaker": "A"}, {"text": "when we apply neural network , is the same .", "speaker": "A"}, {"text": "we jump to ninety percent .", "speaker": "A"}, {"text": "nnn , we exactly .", "speaker": "B"}, {"text": "whatever the normalization , actually .", "speaker": "A"}, {"text": "if we use neural network , even if the features are not correctly normalized , we jump to ninety percent .", "speaker": "A"}, {"text": "so we go from eighty - si eighty - eight point six to ninety , .", "speaker": "D"}, {"text": "it 's around eighty - nine ,", "speaker": "A"}, {"text": "eighty - eight .", "speaker": "A"}, {"text": "eighty - nine .", "speaker": "D"}, {"text": "there are minor differences .", "speaker": "A"}, {"text": "and then adding the msg does nothing , .", "speaker": "D"}, {"text": "for italian , .", "speaker": "A"}, {"text": "for this case , ?", "speaker": "D"}, {"text": "so , so actually , the answer for experiments with one is that adding msg , if you does not help in that case .", "speaker": "D"}, {"text": "the other ones , we 'd have to look at it ,", "speaker": "D"}, {"text": "and the multi - english , does", "speaker": "D"}, {"text": "so if we think of this in error rates , we start off with , eighteen percent error rate , roughly .", "speaker": "D"}, {"text": "and we almost , cut that in half by putting in the on - line normalization and the neural net .", "speaker": "D"}, {"text": "and the msg doesn't however particularly affect things .", "speaker": "D"}, {"text": "and we cut off , about twenty - five percent of the error .", "speaker": "D"}, {"text": "no , not quite that ,", "speaker": "D"}, {"text": "two point six out of eighteen .", "speaker": "D"}, {"text": "about , sixteen percent of the error , , if we use multi - english instead of the matching condition .", "speaker": "D"}, {"text": "not matching condition ,", "speaker": "D"}, {"text": "but , the , italian training .", "speaker": "D"}, {"text": "we select these these tasks", "speaker": "B"}, {"text": "because it 's the more difficult .", "speaker": "B"}, {"text": "yes , good .", "speaker": "D"}, {"text": "so then you 're assuming multi - english is closer to the thing that you could use", "speaker": "D"}, {"text": "since you 're not gonna have matching , , data for the for the new for the other languages and .", "speaker": "D"}, {"text": "one qu thing is that ,", "speaker": "D"}, {"text": "asked you this before ,", "speaker": "D"}, {"text": "but wanna double check .", "speaker": "D"}, {"text": "when you say \" me \" in these other tests , that 's the multi - english ,", "speaker": "D"}, {"text": "that 's it 's part it 's", "speaker": "A"}, {"text": "but it is not all of the multi - english ,", "speaker": "D"}, {"text": "it is some piece of part of it .", "speaker": "D"}, {"text": "or , one million frames .", "speaker": "A"}, {"text": "and the multi - english is how much ?", "speaker": "D"}, {"text": "you have here the information .", "speaker": "B"}, {"text": "it 's one million and half .", "speaker": "A"}, {"text": "so you used almost all", "speaker": "D"}, {"text": "you used two thirds of it ,", "speaker": "D"}, {"text": "so , it 's still it hurts you seems to hurt you fair amount to add in this french and spanish .", "speaker": "D"}, {"text": "stephane was saying that they weren't hand - labeled ,", "speaker": "C"}, {"text": "the french and the spanish .", "speaker": "C"}, {"text": "alright , go ahead .", "speaker": "D"}, {"text": "mmm , with the experiment type - two ,", "speaker": "B"}, {"text": "first tried to combine , nnn , some feature from the mlp and other feature", "speaker": "B"}, {"text": "and we we can", "speaker": "B"}, {"text": "first the feature are without delta and delta - delta ,", "speaker": "B"}, {"text": "and we can see that in the situation , , the msg - three , the same help nothing .", "speaker": "B"}, {"text": "and then do the same", "speaker": "B"}, {"text": "but with the delta and delta - delta plp delta and delta - delta .", "speaker": "B"}, {"text": "and they all but they all put off the mlp is it without delta and delta - delta .", "speaker": "B"}, {"text": "and we have little bit less result than the the baseline plp with delta and delta - delta .", "speaker": "B"}, {"text": "if when we have the new neural network trained with plp delta and delta - delta ,", "speaker": "B"}, {"text": "the final result must be better .", "speaker": "B"}, {"text": "actually , just to be some more", "speaker": "A"}, {"text": "this number , this eighty - seven point one number , has to be compared with the", "speaker": "A"}, {"text": "yes , , it can't be compared with the other", "speaker": "D"}, {"text": "cuz this is , with multi - english , , training .", "speaker": "D"}, {"text": "so you have to compare it with the one over that you 've got in box ,", "speaker": "D"}, {"text": "which is that , the eighty - four point six .", "speaker": "D"}, {"text": "but in this case for the eighty - seven point one we used mlp outputs for the plp net", "speaker": "A"}, {"text": "and straight features with delta - delta .", "speaker": "A"}, {"text": "and straight features with delta - delta gives you what 's on the first sheet .", "speaker": "A"}, {"text": "it 's eight eighty - eight point six .", "speaker": "A"}, {"text": "tr no . no .", "speaker": "D"}, {"text": "not trained with multi - english .", "speaker": "D"}, {"text": ", but this is the second configuration .", "speaker": "A"}, {"text": "no , but they feature @ @ without", "speaker": "B"}, {"text": "so we use feature out , net outputs together with features .", "speaker": "A"}, {"text": "this is not perhaps not clear here", "speaker": "A"}, {"text": "but in this table , the first column is for mlp and the second for the features .", "speaker": "A"}, {"text": "so you 're saying so asking the question , \" what what has adding the mlp done to improve over the ,", "speaker": "D"}, {"text": "so , actually it it decreased the accuracy .", "speaker": "A"}, {"text": "because we have eighty - eight point six .", "speaker": "A"}, {"text": "and even the mlp alone", "speaker": "A"}, {"text": "what gives the mlp alone ?", "speaker": "A"}, {"text": "multi - english plp .", "speaker": "A"}, {"text": "it gives eighty - three point six .", "speaker": "A"}, {"text": "so we have our eighty - three point six and now eighty - eighty point six ,", "speaker": "A"}, {"text": "that gives eighty - seven point one .", "speaker": "A"}, {"text": "eighty - three point six and eighty - eight point six .", "speaker": "D"}, {"text": "eighty - three point six .", "speaker": "A"}, {"text": "is is that ?", "speaker": "A"}, {"text": "but if we have the neural network trained with the plp delta and delta - delta , tha this can help .", "speaker": "B"}, {"text": "that 's that 's one thing ,", "speaker": "D"}, {"text": "but see the other thing is that ,", "speaker": "D"}, {"text": "it 's good to take the difficult case ,", "speaker": "D"}, {"text": "but let 's let 's consider what that means .", "speaker": "D"}, {"text": "what what we 're saying is that one one of the things that", "speaker": "D"}, {"text": "my interpretation of your original suggestion is something like this , as motivation .", "speaker": "D"}, {"text": "when we train on data that is in one sense or another , similar to the testing data , then we get win by having discriminant training .", "speaker": "D"}, {"text": "when we train on something that 's quite different , we have potential to have some problems .", "speaker": "D"}, {"text": "and , , if we get something that helps us when it 's somewhat similar , and doesn't hurt us too much when it when it 's quite different , that 's not so bad .", "speaker": "D"}, {"text": "so the question is , if you took the same combination , and you tried it out on , on say digits ,", "speaker": "D"}, {"text": "on ti - digits ?", "speaker": "A"}, {"text": "was that experiment done ?", "speaker": "D"}, {"text": "no , not yet .", "speaker": "A"}, {"text": "then does that , with similar noise conditions and , does it does it then look much better ?", "speaker": "D"}, {"text": "and so what is the range over these different kinds of of tests ?", "speaker": "D"}, {"text": "so , an anyway .", "speaker": "D"}, {"text": "and , with this type of configuration which do on experiment using the new neural net with name broad klatt twenty - seven ,", "speaker": "B"}, {"text": "have found more or less the same result .", "speaker": "B"}, {"text": "so , it 's slightly better ,", "speaker": "A"}, {"text": "little bit better ?", "speaker": "B"}, {"text": "slightly bet better .", "speaker": "B"}, {"text": "yes , is better .", "speaker": "B"}, {"text": "and if you use the , , delta there , , you would bring it up to where it was , at least about the same for difficult case .", "speaker": "D"}, {"text": "so perhaps let 's let 's jump at the last experiment .", "speaker": "A"}, {"text": "it 's either less information from the neural network if we use only the silence output .", "speaker": "A"}, {"text": "it 's again better .", "speaker": "A"}, {"text": "so it 's eighty - nine point one .", "speaker": "A"}, {"text": "and we have only forty feature", "speaker": "B"}, {"text": "because in this situation we have one hundred and three feature .", "speaker": "B"}, {"text": "and then with the first configuration , am found that work , , doesn't work", "speaker": "B"}, {"text": "but is better , the second configuration .", "speaker": "B"}, {"text": "because for the del engli - plp delta and delta - delta , here have eighty - five point three accuracy ,", "speaker": "B"}, {"text": "and with the second configuration have eighty - seven point one .", "speaker": "B"}, {"text": ", there is another , , suggestion that would apply , , to the second configuration ,", "speaker": "D"}, {"text": "which , , was made , , by , , hari .", "speaker": "D"}, {"text": "and that was that , , if you have feed two streams into htk , , and you , , change the , variances", "speaker": "D"}, {"text": "if you scale the variances associated with , these streams , you can effectively scale the streams .", "speaker": "D"}, {"text": "so , , , without changing the scripts for htk ,", "speaker": "D"}, {"text": "which is the rule here , , you can still change the variances", "speaker": "D"}, {"text": "which would effectively change the scale of these , , two streams that come in .", "speaker": "D"}, {"text": "and , , so , , if you do that , it may be the case that , , the mlp should not be considered as strongly , .", "speaker": "D"}, {"text": "and , , so this is just setting them to be ,", "speaker": "D"}, {"text": "of equal weight .", "speaker": "D"}, {"text": "it shouldn't be equal weight .", "speaker": "D"}, {"text": "? , 'm to say that gives more experiments if we wanted to look at that ,", "speaker": "D"}, {"text": "but , , , on the other hand it 's just experiments at the level of the htk recognition .", "speaker": "D"}, {"text": "it 's not even the htk ,", "speaker": "D"}, {"text": "you have to do the htk training also .", "speaker": "D"}, {"text": "so this is what we decided to do .", "speaker": "B"}, {"text": "let me think .", "speaker": "D"}, {"text": "you have to change the", "speaker": "D"}, {"text": "no , you can just do it in as once you 've done the training", "speaker": "D"}, {"text": "and then you can vary it .", "speaker": "C"}, {"text": "the training is just coming up with the variances", "speaker": "D"}, {"text": "so you could you could just scale them all .", "speaker": "D"}, {"text": "is it the htk models are diagonal covariances ,", "speaker": "A"}, {"text": "that 's , exactly the point , ,", "speaker": "D"}, {"text": "that if you change , change what they are", "speaker": "D"}, {"text": "it 's diagonal covariance matrices ,", "speaker": "D"}, {"text": "but you say what those variances are .", "speaker": "D"}, {"text": "so , that , it 's diagonal , but the diagonal means that then you 're gonna it 's gonna internally multiply it and , , it im implicitly exponentiated to get probabilities , and so it 's it 's gonna it 's going to affect the range of things if you change the change the variances of some of the features .", "speaker": "D"}, {"text": "so , it 's precisely given that model you can very simply affect , , the the strength that you apply the features .", "speaker": "D"}, {"text": "that was that was , , hari 's suggestion .", "speaker": "D"}, {"text": "so it could just be that treating them equally , tea treating two streams equally is just not the thing to do .", "speaker": "D"}, {"text": "it 's potentially opening can of worms", "speaker": "D"}, {"text": "because , , it should be different number for each test set , ,", "speaker": "D"}, {"text": "so the other thing is to take if one were to take , , , couple of the most successful of these ,", "speaker": "D"}, {"text": "and test across everything .", "speaker": "A"}, {"text": "try all these different tests .", "speaker": "D"}, {"text": "so , the next point ,", "speaker": "A"}, {"text": "we 've had some discussion with steve and shawn ,", "speaker": "A"}, {"text": "about their , , articulatory ,", "speaker": "A"}, {"text": "so we 'll perhaps start something next week .", "speaker": "A"}, {"text": "discussion with hynek , sunil and pratibha for trying to plug in their our networks with their within their block diagram ,", "speaker": "A"}, {"text": "where to plug in the network , , after the feature ,", "speaker": "A"}, {"text": "before as as plugin or as anoth another path ,", "speaker": "A"}, {"text": "discussion about multi - band and traps ,", "speaker": "A"}, {"text": "actually hynek would like to see ,", "speaker": "A"}, {"text": "perhaps if you remember the block diagram there is , , temporal lda followed by spectral lda for each critical band .", "speaker": "A"}, {"text": "and he would like to replace these by network", "speaker": "A"}, {"text": "which would , , make the system look like trap .", "speaker": "A"}, {"text": ", it would be trap system .", "speaker": "A"}, {"text": "this is trap system", "speaker": "A"}, {"text": "trap system , ,", "speaker": "A"}, {"text": "but where the neural network are replaced by lda .", "speaker": "A"}, {"text": "and about multi - band ,", "speaker": "A"}, {"text": "started multi - band mlp trainings ,", "speaker": "A"}, {"text": "mmh actually , hhh prefer to do exactly what did when was in belgium .", "speaker": "A"}, {"text": "so take exactly the same configurations ,", "speaker": "A"}, {"text": "seven bands with nine frames of context ,", "speaker": "A"}, {"text": "and we just train on timit ,", "speaker": "A"}, {"text": "and on the large database ,", "speaker": "A"}, {"text": "so , with spine and everything .", "speaker": "A"}, {"text": "mmm , 'm starting to train also , networks with larger contexts .", "speaker": "A"}, {"text": "so , this would be something between traps and multi - band", "speaker": "A"}, {"text": "because we still have quite large bands ,", "speaker": "A"}, {"text": "and but with lot of context also .", "speaker": "A"}, {"text": "we still have to work on finnish ,", "speaker": "A"}, {"text": ", to make decision on which mlp can be the best across the different languages .", "speaker": "A"}, {"text": "for the moment it 's the timit network , and perhaps the network trained on everything .", "speaker": "A"}, {"text": "so . now we can test these two networks on with delta and large networks .", "speaker": "A"}, {"text": "test them also on finnish", "speaker": "A"}, {"text": "and see which one is the the best .", "speaker": "A"}, {"text": ", the next part of the document is , , , summary of what everything that has been done .", "speaker": "A"}, {"text": "so . we have seventy - nine ps trained on", "speaker": "A"}, {"text": "one , two , three , four , , three , four , five , six , seven", "speaker": "A"}, {"text": "ten on ten different databases .", "speaker": "A"}, {"text": "the number of frames is bad also ,", "speaker": "A"}, {"text": "so we have one million and half for some ,", "speaker": "A"}, {"text": "three million for other ,", "speaker": "A"}, {"text": "and six million for the last one .", "speaker": "A"}, {"text": "! as we mentioned , timit is the only that 's hand - labeled ,", "speaker": "A"}, {"text": "and perhaps this is what makes the difference .", "speaker": "A"}, {"text": "the other are just viterbi - aligned .", "speaker": "A"}, {"text": "so these seventy - nine mlp differ on different things .", "speaker": "A"}, {"text": "first , with respect to the on - line normalization ,", "speaker": "A"}, {"text": "there are that use bad on - line normalization ,", "speaker": "A"}, {"text": "and other good on - line normalization .", "speaker": "A"}, {"text": "with respect to the features ,", "speaker": "A"}, {"text": "with respect to the use of delta", "speaker": "A"}, {"text": "with respect to the hidden layer size and to the targets .", "speaker": "A"}, {"text": "but we don't have all the combination of these different parameters", "speaker": "A"}, {"text": "what 's this ?", "speaker": "A"}, {"text": "we only have two hundred eighty six different tests", "speaker": "A"}, {"text": "and no not two thousand .", "speaker": "A"}, {"text": "ugh ! was impressed", "speaker": "D"}, {"text": "say this morning that @ @ thought it was the", "speaker": "B"}, {"text": "alright , now 'm just slightly impressed ,", "speaker": "D"}, {"text": "the observation is what we discussed already .", "speaker": "A"}, {"text": "the msg problem ,", "speaker": "A"}, {"text": "the fact that the mlp trained on target task decreased the error rate .", "speaker": "A"}, {"text": "but when the - mlp is trained on the is not trained on the target task , it increased the error rate compared to using straight features .", "speaker": "A"}, {"text": "except if the features are bad", "speaker": "A"}, {"text": "actually except if the features are not correctly on - line normalized .", "speaker": "A"}, {"text": "in this case the tandem is still better", "speaker": "A"}, {"text": "even if it 's trained on not on the target digits .", "speaker": "A"}, {"text": "so it sounds like , the net corrects some of the problems with some poor normalization .", "speaker": "D"}, {"text": "but if you can do good normalization it 's it 's .", "speaker": "D"}, {"text": "so the fourth point is , , the timit plus noise seems to be the training set that gives better the best network .", "speaker": "A"}, {"text": "so - let me bef before you go on to the possible issues .", "speaker": "D"}, {"text": "so , on the msg problem", "speaker": "D"}, {"text": "that in the , in the short time solution", "speaker": "D"}, {"text": "that is , , trying to figure out what we can proceed forward with to make the greatest progress ,", "speaker": "D"}, {"text": "much as said with jrasta ,", "speaker": "D"}, {"text": "even though really like jrasta", "speaker": "D"}, {"text": "and really like msg ,", "speaker": "D"}, {"text": "it 's in category that it 's , it may be complicated .", "speaker": "D"}, {"text": "and it might be if someone 's interested in it , , certainly encourage anybody to look into it in the longer term ,", "speaker": "D"}, {"text": "once we get out of this particular rush for results .", "speaker": "D"}, {"text": "but in the short term , unless you have some strong idea of what 's wrong ,", "speaker": "D"}, {"text": "but 've perhaps have the feeling that it 's something that 's quite simple", "speaker": "A"}, {"text": "or just like nnn , no high - pass filter", "speaker": "A"}, {"text": "there 's supposed to msg is supposed to have an on - line normalization though ,", "speaker": "D"}, {"text": "it 's there is , , an agc - agc .", "speaker": "A"}, {"text": "but also there 's an on - line norm besides the agc , there 's an on - line normalization that 's supposed to be", "speaker": "D"}, {"text": "taking out means and variances and .", "speaker": "D"}, {"text": "in fac the on - line normalization that we 're using came from the msg design ,", "speaker": "D"}, {"text": "but this was the bad on - line normalization . actually .", "speaker": "A"}, {"text": "are your results are still with the bad the bad", "speaker": "A"}, {"text": "with the - oln - two ?", "speaker": "A"}, {"text": "you have you have oln - two ,", "speaker": "A"}, {"text": "with \" two \" , with \" on - line - two \" .", "speaker": "B"}, {"text": "\" on - line - two \" is good .", "speaker": "D"}, {"text": "so it 's , is the good .", "speaker": "A"}, {"text": "it 's good .", "speaker": "B"}, {"text": "\" two \" is good ?", "speaker": "D"}, {"text": "no , \" two \" is bad .", "speaker": "D"}, {"text": "actually , it 's good with the ch with the good .", "speaker": "B"}, {"text": "so , agree .", "speaker": "D"}, {"text": "it 's probably something simple", "speaker": "D"}, {"text": "if someone , , , wants to play with it for little bit .", "speaker": "D"}, {"text": "you 're gonna do what you 're gonna do", "speaker": "D"}, {"text": "but my would be that it 's something that is simple thing that could take while to find .", "speaker": "D"}, {"text": "and the other the results , observations two and three , , is", "speaker": "D"}, {"text": "that 's what we 've seen .", "speaker": "D"}, {"text": "that 's that what we were concerned about is that if it 's not on the target task", "speaker": "D"}, {"text": "if it 's on the target task then it it helps to have the mlp transforming it .", "speaker": "D"}, {"text": "if it if it 's not on the target task , then , depending on how different it is , you can get , reduction in performance .", "speaker": "D"}, {"text": "and the question is now how to how to get one and not the other ?", "speaker": "D"}, {"text": "or how to how to ameliorate the problems .", "speaker": "D"}, {"text": "because it certainly does is to have in there , when it when there is something like the training data .", "speaker": "D"}, {"text": "so , the reason , the reason is that the perhaps the target the task dependency the language dependency , and the noise dependency", "speaker": "A"}, {"text": "so that 's what you say there .", "speaker": "D"}, {"text": "the but this is still not clear", "speaker": "A"}, {"text": "don't think we have enough result to talk about the language dependency .", "speaker": "A"}, {"text": "the timit network is still the best", "speaker": "A"}, {"text": "but there is also an the other difference ,", "speaker": "A"}, {"text": "the fact that it 's it 's hand - labeled .", "speaker": "A"}, {"text": "just you can just sit here .", "speaker": "D"}, {"text": "don't think we want to mess with the microphones", "speaker": "D"}, {"text": "just , have seat .", "speaker": "D"}, {"text": "summary of the first , forty - five minutes is that some work and works , and some doesn't", "speaker": "D"}, {"text": "we still have this", "speaker": "A"}, {"text": "one of these perhaps ?", "speaker": "A"}, {"text": "we can do little better than that", "speaker": "D"}, {"text": "but if you if you start off with the other one , actually , that has it in words", "speaker": "D"}, {"text": "and then that has it the associated results .", "speaker": "D"}, {"text": "so you 're saying that , although from what we see ,", "speaker": "D"}, {"text": "yes there 's what you would expect in terms of language dependency and noise dependency . that is , , when the neural net is trained on one of those and tested on something different , we don't do as as in the target thing .", "speaker": "D"}, {"text": "but you 're saying that , it is although that general thing is observable so far , there 's something you 're not completely convinced about .", "speaker": "D"}, {"text": "and and what is that ?", "speaker": "D"}, {"text": "you say \" not clear yet \" .", "speaker": "D"}, {"text": "what what do you mean ?", "speaker": "D"}, {"text": "mmm , , , that the fact that , for ti - digits the timit net is the best ,", "speaker": "A"}, {"text": "which is the english net .", "speaker": "A"}, {"text": "but the other are slightly worse .", "speaker": "A"}, {"text": "but you have two effects , the effect of changing language", "speaker": "A"}, {"text": "and the effect of training on something that 's viterbi - aligned instead of hand - labeled .", "speaker": "A"}, {"text": "do you think the alignments are bad ?", "speaker": "D"}, {"text": "have you looked at the alignments ?", "speaker": "D"}, {"text": "what the viterbi alignment 's doing ?", "speaker": "D"}, {"text": "did - did you look at the spanish alignments carmen ?", "speaker": "A"}, {"text": "mmm , no .", "speaker": "B"}, {"text": "might be interesting to look at it .", "speaker": "D"}, {"text": "because , , that is just looking", "speaker": "D"}, {"text": "but , it 's not clear to me you necessarily would do so badly from viterbi alignment .", "speaker": "D"}, {"text": "it depends how good the recognizer is", "speaker": "D"}, {"text": "that 's that the engine is that 's doing the alignment .", "speaker": "D"}, {"text": "but . but , perhaps it 's not really the alignment that 's bad", "speaker": "A"}, {"text": "but the just the ph phoneme string that 's used for the alignment", "speaker": "A"}, {"text": "the pronunciation models and", "speaker": "D"}, {"text": "it 's single pronunciation ,", "speaker": "A"}, {"text": "french , phoneme strings were corrected manually", "speaker": "A"}, {"text": "so we asked people to listen to the the sentence", "speaker": "A"}, {"text": "and we gave the phoneme string and they correct them .", "speaker": "A"}, {"text": "there might be errors just in the in the ph string of phonemes .", "speaker": "A"}, {"text": "so this is not really the viterbi alignment ,", "speaker": "A"}, {"text": "the third the third issue is the noise dependency perhaps", "speaker": "A"}, {"text": "but , , this is not clear yet", "speaker": "A"}, {"text": "because all our nets are trained on the same noises", "speaker": "A"}, {"text": "some of the nets were trained with spine and .", "speaker": "D"}, {"text": "so it and that has other noise .", "speaker": "D"}, {"text": "results are only coming for this net .", "speaker": "A"}, {"text": ", just don't just need more results there with that @ @ .", "speaker": "D"}, {"text": "so . , from these results we have some questions with answers .", "speaker": "A"}, {"text": "what should be the network input ?", "speaker": "A"}, {"text": "plp work as as mfcc , .", "speaker": "A"}, {"text": "but it seems impor important to use the delta .", "speaker": "A"}, {"text": "with respect to the network size ,", "speaker": "A"}, {"text": "there 's one experiment that 's still running", "speaker": "A"}, {"text": "and we should have the result today ,", "speaker": "A"}, {"text": "comparing network with five hundred and one thousand units .", "speaker": "A"}, {"text": "nnn , still no answer actually .", "speaker": "A"}, {"text": "the training set ,", "speaker": "A"}, {"text": "we can , we can tell which training set gives the best result ,", "speaker": "A"}, {"text": "but we exactly why .", "speaker": "A"}, {"text": ", the multi - english so far is the best .", "speaker": "D"}, {"text": "\" multi - multi - english \" just means \" timit \" ,", "speaker": "D"}, {"text": "so . and and when you add other things in to broaden it , it gets worse typically .", "speaker": "D"}, {"text": "then some questions without answers .", "speaker": "A"}, {"text": "the training set is both questions , with answers and without answers .", "speaker": "D"}, {"text": "it 's , yes it 's mul it 's multi - - purpose .", "speaker": "D"}, {"text": "so , the training targets actually ,", "speaker": "A"}, {"text": "the two of the main issues perhaps are still the language dependency and the noise dependency .", "speaker": "A"}, {"text": "and perhaps to try to reduce the language dependency , we should focus on finding some other training targets .", "speaker": "A"}, {"text": "and labeling labeling seems important", "speaker": "A"}, {"text": "because of timit results .", "speaker": "A"}, {"text": "for moment you use we use phonetic targets", "speaker": "A"}, {"text": "but we could also use articulatory targets , soft targets ,", "speaker": "A"}, {"text": "and perhaps even , use networks that doesn't do classification", "speaker": "A"}, {"text": "so , train to have neural networks that", "speaker": "A"}, {"text": "and , com compute features and noit not , nnn , features without noise . , transform the fea noisy features in other features that are not noisy .", "speaker": "A"}, {"text": "but continuous features .", "speaker": "A"}, {"text": "not , hard targets .", "speaker": "A"}, {"text": "that seems like good thing to do , probably ,", "speaker": "D"}, {"text": "not again short - term thing .", "speaker": "D"}, {"text": "one of the things about that is that it 's", "speaker": "D"}, {"text": "the ri the major risk you have there of being is being dependent on very dependent on the noise and .", "speaker": "D"}, {"text": "but it 's another thing to try .", "speaker": "D"}, {"text": "so , this is wa this is one thing , this could be could help perhaps to reduce language dependency", "speaker": "A"}, {"text": "and for the noise part we could combine this with other approaches , like , , the kleinschmidt approach .", "speaker": "A"}, {"text": "so the the idea of putting all the noise that we can find inside database .", "speaker": "A"}, {"text": "kleinschmidt was using more than fifty different noises to train his network ,", "speaker": "A"}, {"text": "and so this is one approach", "speaker": "A"}, {"text": "and the other is multi - band , that is more robust to the noisy changes .", "speaker": "A"}, {"text": "so perhaps , something like multi - band trained on lot of noises with , features - based targets could could help .", "speaker": "A"}, {"text": "if you it 's interesting thought", "speaker": "D"}, {"text": "if you just trained up", "speaker": "D"}, {"text": "one fantasy would be you have something like articulatory targets", "speaker": "D"}, {"text": "and you have some reasonable database ,", "speaker": "D"}, {"text": "but then which is copied over many times with range of different noises ,", "speaker": "D"}, {"text": "and if cuz what you 're trying to do is come up with core , reasonable feature set which is then gonna be used , by the system .", "speaker": "D"}, {"text": "the future work is , , try to connect to the to make to plug in the system to the ogi", "speaker": "A"}, {"text": "there are still open questions there ,", "speaker": "A"}, {"text": "where to put the mlp .", "speaker": "A"}, {"text": "and , , the the real open question ,", "speaker": "D"}, {"text": "there 's lots of open questions ,", "speaker": "D"}, {"text": "but one of the core quote \" open questions \" for that is , if we take the , the best ones here ,", "speaker": "D"}, {"text": "not just the best one ,", "speaker": "D"}, {"text": "but the best few", "speaker": "D"}, {"text": "you want the most promising group from these other experiments .", "speaker": "D"}, {"text": "how do they do over range of these different tests , not just the italian ?", "speaker": "D"}, {"text": "and then then see , again , how", "speaker": "D"}, {"text": "we know that there 's mis there 's loss in performance when the neural net is trained on conditions that are different than , we 're gonna test on ,", "speaker": "D"}, {"text": "but , if you look over range of these different tests , how do these different ways of combining the straight features with the mlp features , stand up over that range ?", "speaker": "D"}, {"text": "that 's that seems like the the real question .", "speaker": "D"}, {"text": "so if you just take plp with , the double - deltas .", "speaker": "D"}, {"text": "assume that 's the the feature .", "speaker": "D"}, {"text": "look at these different ways of combining it .", "speaker": "D"}, {"text": "and , take let 's say , just take multi - english", "speaker": "D"}, {"text": "that works pretty for the training .", "speaker": "D"}, {"text": "and just look take that case and then look over all the different things .", "speaker": "D"}, {"text": "how does that how does that compare between the", "speaker": "D"}, {"text": "so all the all the test sets you mean ,", "speaker": "A"}, {"text": "all the different test sets ,", "speaker": "D"}, {"text": "and for and for the couple different ways that you have of of combining them .", "speaker": "D"}, {"text": "how do they stand up , over the", "speaker": "D"}, {"text": "and perhaps doing this for cha changing the variance of the streams and so on getting different scaling", "speaker": "A"}, {"text": "that 's another possibility if you have time ,", "speaker": "D"}, {"text": "so thi this sh would be more working on the mlp as an additional path instead of an insert to the to their diagram .", "speaker": "A"}, {"text": "perhaps the insert idea is strange", "speaker": "A"}, {"text": "because nnn , they make lda", "speaker": "A"}, {"text": "and then we will again add network does discriminate anal nnn , that discriminates ,", "speaker": "A"}, {"text": "it 's little strange", "speaker": "D"}, {"text": "but on the other hand they did it before .", "speaker": "D"}, {"text": "and because also perhaps we know that the when we have very good features the mlp doesn't help .", "speaker": "A"}, {"text": "the other thing , though , is that", "speaker": "D"}, {"text": "so . , we wanna get their path running here ,", "speaker": "D"}, {"text": "if so , we can add this other .", "speaker": "D"}, {"text": "as an additional path", "speaker": "D"}, {"text": "the way we want to do", "speaker": "A"}, {"text": "cuz they 're doing lda rasta .", "speaker": "D"}, {"text": "they 're doing lda rasta ,", "speaker": "D"}, {"text": "the way we want to do it perhaps is to just to get the vad labels and the final features .", "speaker": "A"}, {"text": "so they will send us the , provide us with the feature files ,", "speaker": "A"}, {"text": "and with vad , binary labels", "speaker": "A"}, {"text": "so that we can , get our mlp features", "speaker": "A"}, {"text": "and filter them with the vad", "speaker": "A"}, {"text": "and then combine them with their feature stream .", "speaker": "A"}, {"text": "so we so . first thing we 'd wanna do there is to make that when we get those labels of final features is that we get the same results as them .", "speaker": "D"}, {"text": "without putting in second path .", "speaker": "D"}, {"text": "just re retraining retraining the htk ?", "speaker": "A"}, {"text": "just to make that we have we understand properly what things are , our very first thing to do is to is to double check that we get the exact same results as them on htk .", "speaker": "D"}, {"text": ", that we need to", "speaker": "D"}, {"text": "do we need to retrain", "speaker": "D"}, {"text": "we can just take the re their training files also .", "speaker": "D"}, {"text": "but . but ,", "speaker": "D"}, {"text": "just for the testing , jus just make that we get the same results so we can duplicate it before we add in another", "speaker": "D"}, {"text": "cuz otherwise , , we won't things mean .", "speaker": "D"}, {"text": "so fff , lograsta ,", "speaker": "A"}, {"text": "if we want to", "speaker": "A"}, {"text": "we can try networks with lograsta filtered features .", "speaker": "A"}, {"text": "! , the other thing is when you say comb", "speaker": "D"}, {"text": "'m 'm , 'm interrupting . that", "speaker": "D"}, {"text": ", when you 're talking about combining multiple features ,", "speaker": "D"}, {"text": "suppose we said , \" , we 've got these different features and ,", "speaker": "D"}, {"text": "but plp seems pretty good . \"", "speaker": "D"}, {"text": "if we take the approach that mike did and have", "speaker": "D"}, {"text": "one of the situations we have is we have these different conditions .", "speaker": "D"}, {"text": "we have different languages ,", "speaker": "D"}, {"text": "we have different noises ,", "speaker": "D"}, {"text": "if we have some drastically different conditions and we just train up different ps with them .", "speaker": "D"}, {"text": "and put them together .", "speaker": "D"}, {"text": "what what what mike found , for the reverberation case at least ,", "speaker": "D"}, {"text": "who knows if it 'll work for these other ones .", "speaker": "D"}, {"text": "that you did have interpolative effects .", "speaker": "D"}, {"text": "that is , that yes , if you knew what the reverberation condition was gonna be and you trained for that , then you got the best results .", "speaker": "D"}, {"text": "but if you had , say , heavily - reverberation ca heavy - reverberation case and no - reverberation case , , and then you fed the thing , something that was modest amount of reverberation then you 'd get some result in between the two .", "speaker": "D"}, {"text": "so it was behaved reasonably .", "speaker": "D"}, {"text": "is tha that fair", "speaker": "D"}, {"text": "so you 's perhaps better to have several", "speaker": "A"}, {"text": "it works better if what ?", "speaker": "D"}, {"text": "you were doing some something that was", "speaker": "D"}, {"text": "so the analogy isn't quite .", "speaker": "D"}, {"text": "you were doing something that was in way little better behaved .", "speaker": "D"}, {"text": "you had reverb for single variable", "speaker": "D"}, {"text": "which was re , reverberation .", "speaker": "D"}, {"text": "here the problem seems to be is that we don't have hug really huge net with really huge amount of training data .", "speaker": "D"}, {"text": "but we have for this task , would think , modest amount .", "speaker": "D"}, {"text": "million frames actually isn't that much .", "speaker": "D"}, {"text": "we have modest amount of training data from couple different conditions ,", "speaker": "D"}, {"text": "and then in , that and the real situation is that there 's enormous variability that we anticipate in the test set in terms of language ,", "speaker": "D"}, {"text": "and , , channel characteristic ,", "speaker": "D"}, {"text": "all over the map .", "speaker": "D"}, {"text": "bunch of different dimensions .", "speaker": "D"}, {"text": "and so , 'm just concerned that we don't really have , the data to train up", "speaker": "D"}, {"text": "one of the things that we were seeing is that when we added in we still don't have good explanation for this ,", "speaker": "D"}, {"text": "but we are seeing that we 're adding in , fe few different databases", "speaker": "D"}, {"text": "and the performance is getting worse", "speaker": "D"}, {"text": "and , when we just take one of those databases that 's pretty good one , it actually is is better .", "speaker": "D"}, {"text": "and that says to me , yes , that , , there might be some problems with the pronunciation models that some of the databases we 're adding in like that .", "speaker": "D"}, {"text": "but one way or another we don't have , seemingly , the ability to represent , in the neural net of the size that we have , , all of the variability that we 're gonna be covering .", "speaker": "D"}, {"text": "so that 'm 'm hoping that", "speaker": "D"}, {"text": "this is another take on the efficiency argument you 're making ,", "speaker": "D"}, {"text": "which is 'm hoping that with moderate size neural nets , , that if we if they look at more constrained conditions they 'll have enough parameters to really represent them .", "speaker": "D"}, {"text": "so doing both is not is not , you mean ,", "speaker": "A"}, {"text": "the it 's true that the ogi folk found that using lda rasta ,", "speaker": "D"}, {"text": "which is lograsta ,", "speaker": "D"}, {"text": "it 's just that they have the", "speaker": "D"}, {"text": "it 's done in the log domain , as recall ,", "speaker": "D"}, {"text": "and it 's it it 's just that they it 's trained up ,", "speaker": "D"}, {"text": "that that benefitted from on - line normalization .", "speaker": "D"}, {"text": "so they did at least in their case , it did seem to be somewhat complimentary .", "speaker": "D"}, {"text": "so will it be in our case , where we 're using the neural net ?", "speaker": "D"}, {"text": "they were not using the neural net .", "speaker": "D"}, {"text": "so the other things you have here are , trying to improve results from single", "speaker": "D"}, {"text": "and cpu memory issues .", "speaker": "D"}, {"text": "we 've been ignoring that ,", "speaker": "D"}, {"text": "but we have to address the problem of cpu and memory we", "speaker": "A"}, {"text": "you you folks have been looking at this more than me .", "speaker": "D"}, {"text": "but my impression was that , there was strict constraint on the delay ,", "speaker": "D"}, {"text": "but beyond that it was that using less memory was better ,", "speaker": "D"}, {"text": "and using less cpu was better .", "speaker": "D"}, {"text": "something like that ,", "speaker": "D"}, {"text": "so , , but we 've", "speaker": "A"}, {"text": "we have to get some reference point to where we", "speaker": "A"}, {"text": "what 's reasonable number ?", "speaker": "A"}, {"text": "perhaps be because if it 's if it 's too large or large", "speaker": "A"}, {"text": "don't think we 're completely off the wall .", "speaker": "D"}, {"text": "that if we if we have", "speaker": "D"}, {"text": "the ultimate fall back that we could do", "speaker": "D"}, {"text": "we may find that we 're not really gonna worry about the", "speaker": "D"}, {"text": "if the mlp ultimately , after all is said and done , doesn't really help then we won't have it in .", "speaker": "D"}, {"text": "if the mlp does , we find , help us enough in some conditions , , we might even have more than one mlp .", "speaker": "D"}, {"text": "we could simply say that is , done on the , server .", "speaker": "D"}, {"text": "we do the other manipulations that we 're doing before that .", "speaker": "D"}, {"text": "so , that 's that 's .", "speaker": "D"}, {"text": "so the key thing was , this plug into ogi .", "speaker": "D"}, {"text": "what are they what are they gonna be working", "speaker": "D"}, {"text": "do we they 're gonna be working on while we take their features ,", "speaker": "D"}, {"text": "they 're they 're starting to wor work on some multi - band .", "speaker": "A"}, {"text": "this that was pratibha .", "speaker": "A"}, {"text": "what was he doing ,", "speaker": "A"}, {"text": "do you remember ?", "speaker": "A"}, {"text": "he was doing something new", "speaker": "A"}, {"text": "don't re didn't remember .", "speaker": "B"}, {"text": "he 's working with neural network .", "speaker": "B"}, {"text": "they were also mainly ,", "speaker": "A"}, {"text": "working little bit of new things , like networks and multi - band ,", "speaker": "A"}, {"text": "but mainly trying to tune their system as it is now", "speaker": "A"}, {"text": "to just trying to get the best from this architecture .", "speaker": "A"}, {"text": "so the way it would work is that you 'd get", "speaker": "D"}, {"text": "there 'd be some point where you say , \" , this is their version - one \" or whatever ,", "speaker": "D"}, {"text": "and we get these vad labels and features and for all these test sets from them ,", "speaker": "D"}, {"text": "and then , , that 's what we work with .", "speaker": "D"}, {"text": "we have certain level we try to improve it with this other path", "speaker": "D"}, {"text": "and then , , when it gets to be , january", "speaker": "D"}, {"text": "we say , \" we have shown that we can improve this , in this way .", "speaker": "D"}, {"text": "so now what 's your newest version ? \"", "speaker": "D"}, {"text": "and then they 'll have something that 's better", "speaker": "D"}, {"text": "and then we 'd combine it .", "speaker": "D"}, {"text": "this is always hard .", "speaker": "D"}, {"text": "used to work with folks who were trying to improve good , system with with neural net system", "speaker": "D"}, {"text": "and , it was common problem that you 'd", "speaker": "D"}, {"text": "and this actually , this is true not just for neural nets", "speaker": "D"}, {"text": "but just for in general if people were working with , rescoring , - best lists or lattices that come came from , mainstream recognizer .", "speaker": "D"}, {"text": "you get something from the other site at one point and you work really hard on making it better with rescoring .", "speaker": "D"}, {"text": "but they 're working really hard , too .", "speaker": "D"}, {"text": "so by the time you have , improved their score , they have also improved their score", "speaker": "D"}, {"text": "and now there isn't any difference ,", "speaker": "D"}, {"text": "so , , at some point we 'll have to", "speaker": "D"}, {"text": "we 're we 're integrated little more tightly than happens in lot of those cases .", "speaker": "D"}, {"text": "at the moment they say that they have better thing we can we", "speaker": "D"}, {"text": "what takes all the time here is that we 're trying so many things ,", "speaker": "D"}, {"text": "presumably , in in day we could turn around", "speaker": "D"}, {"text": "taking new set of things from them and rescoring it ,", "speaker": "D"}, {"text": ", perhaps we could .", "speaker": "A"}, {"text": "no , this is this is good .", "speaker": "D"}, {"text": "that the most wide open thing is the issues about the , , different trainings .", "speaker": "D"}, {"text": "da training targets and noises and .", "speaker": "D"}, {"text": "so we can for we we can forget combining multiple features and mlg perhaps ,", "speaker": "A"}, {"text": "that 's wide open .", "speaker": "D"}, {"text": "or focus more on the targets and on the training data", "speaker": "A"}, {"text": "for now , really liked msg .", "speaker": "D"}, {"text": "and that , , one of the things liked about it is has such different temporal properties .", "speaker": "D"}, {"text": "and , that there is ultimately really good , potential for , , bringing in things with different temporal properties .", "speaker": "D"}, {"text": "we only have limited time", "speaker": "D"}, {"text": "and there 's lot of other things we have to look at .", "speaker": "D"}, {"text": "and it seems like much more core questions are issues about the training set", "speaker": "D"}, {"text": "and the training targets ,", "speaker": "D"}, {"text": "and fitting in what we 're doing with what they 're doing ,", "speaker": "D"}, {"text": "and , , with limited time .", "speaker": "D"}, {"text": "we have to start cutting down .", "speaker": "D"}, {"text": "and then , , once we", "speaker": "D"}, {"text": "having gone through this process and trying many different things , would imagine that certain things , come up that you are curious about", "speaker": "D"}, {"text": "that you 'd not getting to", "speaker": "D"}, {"text": "and so when the dust settles from the evaluation , that would time to go back and take whatever intrigued you most ,", "speaker": "D"}, {"text": "got you most interested", "speaker": "D"}, {"text": "and and work with it , , for the next round .", "speaker": "D"}, {"text": "as you can tell from these numbers , nothing that any of us is gonna do is actually gonna completely solve the problem .", "speaker": "D"}, {"text": "so , there 'll still be plenty to do .", "speaker": "D"}, {"text": "barry , you 've been pretty quiet .", "speaker": "D"}, {"text": "but that what were you involved in this primarily ?", "speaker": "D"}, {"text": "helping out , preparing", "speaker": "C"}, {"text": "they 've been running all the experiments and", "speaker": "C"}, {"text": "and 've been , doing some work on the preparing all the data for them to , train and to test on .", "speaker": "C"}, {"text": "now , 'm 'm focusing mainly on this final project 'm working on in jordan 's class .", "speaker": "C"}, {"text": "what 's what 's that ?", "speaker": "D"}, {"text": "so there was paper in icslp about this multi - band , belief - net structure . this guy did", "speaker": "C"}, {"text": "it was two ms with with dependency arrow between the two", "speaker": "C"}, {"text": "and so wanna try coupling them instead of having an arrow that flows from one sub - band to another sub - band .", "speaker": "C"}, {"text": "wanna try having the arrows go both ways .", "speaker": "C"}, {"text": "and , 'm just gonna see if that better models , asynchrony in any way", "speaker": "C"}, {"text": "that sounds interesting .", "speaker": "D"}, {"text": "anything to you wanted to", "speaker": "D"}, {"text": "silent partner in the in the meeting .", "speaker": "D"}, {"text": "we got laugh out of him ,", "speaker": "D"}, {"text": "that 's good .", "speaker": "D"}, {"text": "everyone must contribute to the our sound files here .", "speaker": "D"}, {"text": "so speaking of which , if we don't have anything else that we need", "speaker": "D"}, {"text": "you happy with where we are ?", "speaker": "D"}, {"text": "know know wher know where we 're going ?", "speaker": "D"}, {"text": "you you happy ?", "speaker": "D"}, {"text": "you 're happy .", "speaker": "D"}, {"text": "everyone should be happy .", "speaker": "D"}, {"text": "you don't have to be happy .", "speaker": "D"}, {"text": "you 're almost done .", "speaker": "D"}, {"text": "al - actually should mention", "speaker": "E"}, {"text": "so if , about the linux machine", "speaker": "E"}, {"text": "\" swede . \"", "speaker": "E"}, {"text": "so it looks like the , neural net tools are installed there .", "speaker": "E"}, {"text": "and dan ellis believe knows something about using that machine", "speaker": "E"}, {"text": "if people are interested in getting jobs running on that could help with that .", "speaker": "E"}, {"text": "but if we really need now lot of machines .", "speaker": "A"}, {"text": "we could start computing another huge table", "speaker": "A"}, {"text": ", we want different table , at least", "speaker": "D"}, {"text": "there 's there 's some different things that we 're trying to get at now .", "speaker": "D"}, {"text": "so . , as far as you can tell , you 're actually on - on cpu , for training and so on ?", "speaker": "D"}, {"text": "more is always better ,", "speaker": "A"}, {"text": "don't think we have to train lot of networks , now that we know", "speaker": "A"}, {"text": "we just select what works fine", "speaker": "A"}, {"text": "and try to improve this", "speaker": "A"}, {"text": "and we 're on and we 're on disk ?", "speaker": "D"}, {"text": "it 's , .", "speaker": "A"}, {"text": "sometimes we have some problems .", "speaker": "A"}, {"text": "some problems with the", "speaker": "B"}, {"text": "but they 're correctable , problems .", "speaker": "D"}, {"text": "'m familiar with that one ,", "speaker": "D"}, {"text": "so , since , we didn't ha get channel on for you , you don't have to read any digits", "speaker": "D"}, {"text": "but the rest of us will .", "speaker": "D"}, {"text": "is it on ?", "speaker": "D"}, {"text": "cuz 'm afraid of making the driver crash", "speaker": "D"}, {"text": "which it seems to do , pretty easily .", "speaker": "D"}, {"text": "so we 'll 'll start off the", "speaker": "D"}, {"text": "my battery is low .", "speaker": "A"}, {"text": "let 's hope it works .", "speaker": "D"}, {"text": "you should go first and see", "speaker": "D"}, {"text": "so that you 're .", "speaker": "D"}, {"text": "your battery 's going down too .", "speaker": "C"}, {"text": "carmen 's battery is going down too .", "speaker": "C"}, {"text": "why don't you go next then .", "speaker": "D"}, {"text": "we 're done .", "speaker": "D"}, {"text": "just finished digits .", "speaker": "D"}, {"text": "it 's good .", "speaker": "D"}, {"text": "we can turn off our microphones now .", "speaker": "D"}, {"text": "just pull the batteries out .", "speaker": "C"}], "id": "xxxxx", "relations": [{"y": 1, "x": 0, "type": "Explanation"}]}]